[
  {
    "objectID": "logml2024/people2024/team/guada.html",
    "href": "logml2024/people2024/team/guada.html",
    "title": "Guadalupe Gonzalez",
    "section": "",
    "text": "Guadalupe is a machine learning scientist at Roche, in the Prescient Design team. She focuses on developing deep learning methods for a range of tasks within the drug discovery pipeline. She has previous experience on target discovery, and disease characterization. At Prescient, her focus is primarily on developing general deep learning methods for various tasks including molecule design, mode-of-action prediction, and structure elucidation from mass spectrometry data."
  },
  {
    "objectID": "logml2024/people2024/team/van.html",
    "href": "logml2024/people2024/team/van.html",
    "title": "Van Truong",
    "section": "",
    "text": "Home\n    People\n    Organising Committee\n    Van Truong\n  \n\nVan Truong is a PhD Candidate in Genomics and Computational Biology at the University of Pennsylvania Perelman School of Medicine and MA Candidate in Statistics and Data Science at the Wharton School of Business. Her research focuses on improving the limited understanding of autoimmune diseases using computational multi-omics approaches. She previously completed an MS in Biotechnology with concentrations in Bioinformatics and Drug Discovery jointly at Johns Hopkins University and the National Cancer Institute. Van also has an unconventional background in the arts and humanities, which shape her approach to collaborative and transparent science. She received her BA in Anthropology at the University of Florida and completed multiple post-baccalaureate research fellowships at Virginia Tech PREP, Harvard University, and Harvard T.H. Chan School of Public Health. She is passionate about making science accessible - ranging from publishing quality control procedures, mentoring underrepresented students, and exhibiting public interactive sculptures. Van believes that cross-disciplinary collaboration and public engagement are essential for a healthy civic society."
  },
  {
    "objectID": "logml2024/people2024/team/mike.html",
    "href": "logml2024/people2024/team/mike.html",
    "title": "Mike Heyns",
    "section": "",
    "text": "Mike Heyns is a Schmidt Science Fellow at Imperial College London. The goal of my research is to use in-situ satellite data to deliver real-time space weather forecasting tools to the Met Office and ultimately at-risk end users, such as power utilities and railways. Mike has organised a range of summer schools, tutorials, and outreach activities both in the UK and South Africa."
  },
  {
    "objectID": "logml2024/people2024/team/josh.html",
    "href": "logml2024/people2024/team/josh.html",
    "title": "Josh Southern",
    "section": "",
    "text": "Josh is currently a PhD student at Imperial College London, supervised by Prof. Michael Bronstein and Dr. Kirill Veselkov and part of the AI4Health CDT. He id interested in geometric deep learning, generative models and their applications (particularly in biology, healthcare and sport). He enjoys working on interdisciplinary projects and he is currently collaborating with Bruno Correia’s lab at EPFL on protein design, as well as on a metabolomics + hyperfoods project with a team at Imperial.\nPrior to the PhD, he did his Masters degree in Applied Mathematics, also at Imperial College London, and then spent two years as a Machine Learning Researcher at a Health Tech startup working with wearable device data, trying to benefit the mental health space. Outside of academia, he is obsessed with sport (particularly tennis), pubs and jazz."
  },
  {
    "objectID": "logml2024/people2024/team/kate.html",
    "href": "logml2024/people2024/team/kate.html",
    "title": "Katherine Benjamin",
    "section": "",
    "text": "Katherine Benjamin is a DPhil student in the Mathematical Institute at the University of Oxford. As a member of the Centre for Topological Data Analysis, her research focuses on topological methods for analysis of biomedical data with spatial components. In particular, she is interested in geometric invariants of biomolecular structure, and topological summaries for spatial transcriptomics data."
  },
  {
    "objectID": "logml2024/people2024/team/francesco.html",
    "href": "logml2024/people2024/team/francesco.html",
    "title": "Francesco Viganò",
    "section": "",
    "text": "Francesco Viganò is a PhD student at Imperial College (LSGNT) with a background in algebraic geometry, now working on spectral graph theory and applications of geometry to graph-based learning models. He is one of the authors of “Equivariant Mesh Attention Networks”, one of the LOGML 2021 projects. He has experience organising events and raising funding, for example as a co-organiser of the LOGML 2022 workshop."
  },
  {
    "objectID": "logml2024/people2024/advisors/marinka.html",
    "href": "logml2024/people2024/advisors/marinka.html",
    "title": "Marinka Zitnik",
    "section": "",
    "text": "website\n  \n\n\n\nMarinka Zitnik is an Assistant Professor in the Department of Biomedical Informatics at Harvard Medical School, Associate Faculty at the Kempner Institute for the Study of Natural and Artificial Intelligence at Harvard University, Associate Member at the Broad Institute of MIT and Harvard, and Affiliated Faculty at the Harvard Data Science Initiative.\nArtificial intelligence is poised to enable breakthroughs in science and reshape medicine. Dr. Zitnik investigates foundations of AI to enhance scientific discovery and to realize individualized diagnosis and treatment. The overarching goal of her lab is to lay the foundations for AI that contribute to the scientific understanding of therapeutic design and genomic medicine or acquire such understanding autonomously.\nAI for Medicine: The state of a person is described with increasing precision incorporating modalities like genetic code, cellular atlases, molecular datasets, and therapeutics—the challenge is how to reason over these data to develop powerful disease diagnostics and empower new kinds of therapies. Our research creates new avenues for fusing knowledge and patient data to give the right patient the right treatment at the right time and have medicinal effects that are consistent from person to person and with results in the laboratory.\nAI for Science: For centuries, the method of discovery—the fundamental practice of science that scientists use to explain the natural world systematically and logically—has remained largely the same. We are using AI to change that. The natural world is interconnected, from the various facets of genome regulation to the molecular and organismal levels. These interactions across different levels yield a bewildering degree of complexity. Our research seeks to disentangle this complexity, developing AI models that advance drug design and help develop new kinds of therapies. Her lab is pioneering AI systems informed by geometry, structure, and symmetry, grounded in scientific knowledge. Current research directions in the lab include:\nLarge pre-trained AI to fuse data modalities like genetic code, single-cell atlases, molecular datasets, and therapeutics via multimodal knowledge graph networks and large pretrained language models Geometric deep learning and graph neural networks to reason about network biology and medicine Multi-scale, individualized, and contextualized AI to transfer prediction prowess acquired from one data type to another and to design contextually adaptive AI that can dynamically adjust outputs to biological contexts, such as patients, diseases, and cell types, in which they operate Better methods for drug design to enhance drug development across therapeutic modalities and stages of development Foundations for scientific discovery in the age of AI. Dr. Zitnik has founded Therapeutics Data Commons and is the faculty lead of the International AI4Science initiative.\nIn 2020 she organized the National Symposium on Drug Repurposing for Future Pandemics on behalf of the National Science Foundation (NSF)"
  },
  {
    "objectID": "logml2024/people2024/advisors/thomas.html",
    "href": "logml2024/people2024/advisors/thomas.html",
    "title": "Thomas Kipf",
    "section": "",
    "text": "website\n  \n\n\n\nThomas Kipf is a Senior Research Scientist at Google DeepMind. He obtained his PhD at the University of Amsterdam working with Max Welling. For his PhD thesis on Deep Learning with Graph-Structured Representations he received the ELLIS PhD Award 2021. He am broadly interested in developing and studying machine learning models that can reason about the rich structure of both the physical and digital world and their combinatorial complexity."
  },
  {
    "objectID": "logml2024/people2024/advisors/bastian.html",
    "href": "logml2024/people2024/advisors/bastian.html",
    "title": "Bastian Rieck",
    "section": "",
    "text": "website"
  },
  {
    "objectID": "logml2024/people2024/advisors/solomon.html",
    "href": "logml2024/people2024/advisors/solomon.html",
    "title": "Justin Solomon",
    "section": "",
    "text": "website"
  },
  {
    "objectID": "logml2024/people.html",
    "href": "logml2024/people.html",
    "title": "People",
    "section": "",
    "text": "Imperial College London\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUniversity of Oxford\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHarvard University\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImperial College London\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrescient Design, Roche\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImperial College London\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHarvard University\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImperial College London\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImperial College London\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUniversity of Pennsylvania\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImperial College London\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Archives",
      "LOGML 2024",
      "People"
    ]
  },
  {
    "objectID": "logml2024/people.html#organising-committee",
    "href": "logml2024/people.html#organising-committee",
    "title": "People",
    "section": "",
    "text": "Imperial College London\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUniversity of Oxford\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHarvard University\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImperial College London\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrescient Design, Roche\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImperial College London\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHarvard University\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImperial College London\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImperial College London\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUniversity of Pennsylvania\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImperial College London\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Archives",
      "LOGML 2024",
      "People"
    ]
  },
  {
    "objectID": "logml2024/people.html#scientific-advisory-committee",
    "href": "logml2024/people.html#scientific-advisory-committee",
    "title": "People",
    "section": "Scientific Advisory Committee",
    "text": "Scientific Advisory Committee\n\n\n\n\n\n\n\n\n\n\nXavier Bresson\n\n\nNational University of Singapore\n\n\n\n\n\n\n\n\n\n\n\n\n\nMichael Bronstein\n\n\nUniversity of Oxford, Twitter\n\n\n\n\n\n\n\n\n\n\n\n\n\nHeather Harrington\n\n\nMPI-CBG Dresden\n\n\n\n\n\n\n\n\n\n\n\n\n\nThomas Kipf\n\n\nGoogle Deepmind\n\n\n\n\n\n\n\n\n\n\n\n\n\nBastian Rieck\n\n\nHelmholtz Munich\n\n\n\n\n\n\n\n\n\n\n\n\n\nJustin Solomon\n\n\nMIT\n\n\n\n\n\n\n\n\n\n\n\n\n\nUlrike Tillmann\n\n\nIsaac Newton Institute for Mathematical Sciences\n\n\n\n\n\n\n\n\n\n\n\n\n\nMarinka Zitnik\n\n\nHarvard University\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Archives",
      "LOGML 2024",
      "People"
    ]
  },
  {
    "objectID": "logml2024/projects2024/project2/index.html",
    "href": "logml2024/projects2024/project2/index.html",
    "title": "Matching graphs with spatial constrains",
    "section": "",
    "text": "Anna Calissano is a Chapman Fellow at the Department of Mathematics at Imperial College London. She conducts research on defining suitable geometrical embeddings and novel statistical tools for the analysis of set of graphs and networks. She works at the intersection of geometry, statistics, and computing. Her main applicational areas are urban planning and medical imaging. Anna received a PhD in Mathematics from Politecnico di Milano in 2021, under the supervision of Prof. Simone Vantini and Prof. Aasa Feragen (DTU). She worked as a postdoctoral researcher at INRIA (France) within the ERC Project Geometric Statistics leaded by Xavier Pennec."
  },
  {
    "objectID": "logml2024/projects2024/project2/index.html#anna-calissano",
    "href": "logml2024/projects2024/project2/index.html#anna-calissano",
    "title": "Matching graphs with spatial constrains",
    "section": "",
    "text": "Anna Calissano is a Chapman Fellow at the Department of Mathematics at Imperial College London. She conducts research on defining suitable geometrical embeddings and novel statistical tools for the analysis of set of graphs and networks. She works at the intersection of geometry, statistics, and computing. Her main applicational areas are urban planning and medical imaging. Anna received a PhD in Mathematics from Politecnico di Milano in 2021, under the supervision of Prof. Simone Vantini and Prof. Aasa Feragen (DTU). She worked as a postdoctoral researcher at INRIA (France) within the ERC Project Geometric Statistics leaded by Xavier Pennec."
  },
  {
    "objectID": "logml2024/projects2024/project2/index.html#project",
    "href": "logml2024/projects2024/project2/index.html#project",
    "title": "Matching graphs with spatial constrains",
    "section": "Project",
    "text": "Project\nProblem Statement: Given two graphs, finding a correspondence between the two sets of nodes has been broadly studied in the literature as graph matching. In many real-world applications, the matching is constrained: not every node can be assigned to every other node. Consider for example the brain structural connectivity of different subjects, a misalignment between brain parcels (i.e. nodes) can occur during the signal preprocessing. However, the only reasonable alignments are between neighboring parcels [1]. The goal of the project is to define a graph matching procedure with spatial constrains (e.g. the spatial distance on the cortex between parcels) on a set of structural connectivity matrices of healthy subjects from the Human Connectome Project.\nSolutions to explore: The problem can be addressed in different ways: (1) by adding a penalization term in the optimization procedure which depends on the spatial distance of the nodes (e.g. Rigid Graph Matching [2]); (2) by defining an optimization problem using the generators of the permutation group ([3]).\n[1] Calissano, A., Papadopoulo, T., Pennec, X., & Deslauriers-Gauthier, S. (2022). Graph Alignment Exploiting the Spatial Organisation Improves the Similarity of Brain Networks. In Human Brain Mapping (to appear)\n[2] Ravindra, V., Nassar, H., Gleich, D. F., & Grama, A. (2020). Rigid graph alignment. In Complex Networks and Their Applications VIII: Volume 1 Proceedings of the Eighth International Conference on Complex Networks and Their Applications COMPLEX NETWORKS 2019 8 (pp. 621-632). Springer International Publishing.\n[3] Coxeter, H. S., & Moser, W. O. (2013). Generators and relations for discrete groups (Vol. 14). Springer Science & Business Media."
  },
  {
    "objectID": "logml2024/projects2024/project4/index.html",
    "href": "logml2024/projects2024/project4/index.html",
    "title": "Geometric GNNs for particle level reconstruction",
    "section": "",
    "text": "Dolores Garcia is a Senior Fellow in the Experimental Physical Department at CERN. She received her MSc in Theoretical Physics from Imperial College London, and her PhD in Telecommunications Engineering from University of Carlos III supervised by Joerg Widmer. Her research interests include equivariant machine learning, graph neural networks and the application of these to high energy physics."
  },
  {
    "objectID": "logml2024/projects2024/project4/index.html#dolores-garcia",
    "href": "logml2024/projects2024/project4/index.html#dolores-garcia",
    "title": "Geometric GNNs for particle level reconstruction",
    "section": "",
    "text": "Dolores Garcia is a Senior Fellow in the Experimental Physical Department at CERN. She received her MSc in Theoretical Physics from Imperial College London, and her PhD in Telecommunications Engineering from University of Carlos III supervised by Joerg Widmer. Her research interests include equivariant machine learning, graph neural networks and the application of these to high energy physics."
  },
  {
    "objectID": "logml2024/projects2024/project4/index.html#project",
    "href": "logml2024/projects2024/project4/index.html#project",
    "title": "Geometric GNNs for particle level reconstruction",
    "section": "Project",
    "text": "Project\nThe particle flow algorithm enables the reconstruction of the particle-level view of an event by integrating information from the entire detector, spanning from the tracker to the calorimeters. Machine learning (ML) holds promise in enhancing the quality of reconstruction by leveraging raw data and acquiring the ability to untangle complex events. In this project, the students will explore a geometric graph neural network approach for particle-level reconstruction, focusing on the simplified scenario of e+e- collisions. The primary objective is to investigate architectures capable of extracting complex geometric structures, particle showers, from geometric graphs generated through simulations. Specifically, the team will delve into understanding the significance of equivariance and locality for this problem. The project will involve crafting a representation, managing large graphs in a distributed manner, and assessing the physics outputs."
  },
  {
    "objectID": "logml2024/projects2024/project16/index.html",
    "href": "logml2024/projects2024/project16/index.html",
    "title": "Geometry for Distribution Learning",
    "section": "",
    "text": "Zhengang Zhong is a third-year PhD student at Imperial College London. His research focuses on shape optimisation and data-driven stochastic optimal control. A particular emphasis has been placed on data-driven distributionally robust control, which encodes the knowledge about the uncertainty of the controlled system into safe control actions with the help of the information and optimal transport geometry. Before his PhD, Zhengang received his Diplom Ingenieur degree in Mechatronics at the Technical University of Dresden, Germany."
  },
  {
    "objectID": "logml2024/projects2024/project16/index.html#zhengang-zhong",
    "href": "logml2024/projects2024/project16/index.html#zhengang-zhong",
    "title": "Geometry for Distribution Learning",
    "section": "",
    "text": "Zhengang Zhong is a third-year PhD student at Imperial College London. His research focuses on shape optimisation and data-driven stochastic optimal control. A particular emphasis has been placed on data-driven distributionally robust control, which encodes the knowledge about the uncertainty of the controlled system into safe control actions with the help of the information and optimal transport geometry. Before his PhD, Zhengang received his Diplom Ingenieur degree in Mechatronics at the Technical University of Dresden, Germany."
  },
  {
    "objectID": "logml2024/projects2024/project16/index.html#project",
    "href": "logml2024/projects2024/project16/index.html#project",
    "title": "Geometry for Distribution Learning",
    "section": "Project",
    "text": "Project\nInformation and optimal transport geometry provide powerful tools for analyzing and understanding the characteristics of complex probability distributions, thereby fostering the development of fast and scalable methods for approximating these distributions. For example, with the help of optimal transport geometry, sampling problems can be viewed as a gradient flow with respect to the Wasserstein geometry [1].\nIn the project, we will conduct an experiment similar to section 5 in [2] and section 6 in [3]: we will solve Bayesian inference problems based on optimal transport geometry. Then we will compare the performance of Wasserstein variational inference with the methods using different metrics on the space of probability measures and classic MCMC methods based on various information geometries.\n[1] Garcia Trillos, N., B. Hosseini, and D. Sanz-Alonso. ““From Optimization to Sampling Through Gradient Flows.”” arXiv e-prints (2023): arXiv-2302.\n[2] Lambert, Marc, et al. ““Variational inference via Wasserstein gradient flows.”” Advances in Neural Information Processing Systems 35 (2022): 14434-14447.\n[3] Ambrogioni, Luca, et al. ““Wasserstein variational inference.”” Advances in Neural Information Processing Systems 31 (2018)."
  },
  {
    "objectID": "logml2024/projects2024/project10/index.html",
    "href": "logml2024/projects2024/project10/index.html",
    "title": "Multimodal Protein Representation Learning",
    "section": "",
    "text": "Michail Chatzianastasis is a PhD student at École Polytechnique in Paris, under the supervision of Prof. Michalis Vazirgiannis. He has a keen interest in machine learning on graph-structured data, focusing on developing neural network architectures for solving real-world problems in biology. His current research focuses on multimodal generative models for protein representation learning, combining both graph and text modalities using Graph Neural Networks and Large Language Models. Previously, he interned at Flatiron Institute of Simons Foundation in New York, working on graph neural networks for cancer gene prediction under the guidance of Prof. Zijun Frank Zhang."
  },
  {
    "objectID": "logml2024/projects2024/project10/index.html#michail-chatzianastasis",
    "href": "logml2024/projects2024/project10/index.html#michail-chatzianastasis",
    "title": "Multimodal Protein Representation Learning",
    "section": "",
    "text": "Michail Chatzianastasis is a PhD student at École Polytechnique in Paris, under the supervision of Prof. Michalis Vazirgiannis. He has a keen interest in machine learning on graph-structured data, focusing on developing neural network architectures for solving real-world problems in biology. His current research focuses on multimodal generative models for protein representation learning, combining both graph and text modalities using Graph Neural Networks and Large Language Models. Previously, he interned at Flatiron Institute of Simons Foundation in New York, working on graph neural networks for cancer gene prediction under the guidance of Prof. Zijun Frank Zhang."
  },
  {
    "objectID": "logml2024/projects2024/project10/index.html#project",
    "href": "logml2024/projects2024/project10/index.html#project",
    "title": "Multimodal Protein Representation Learning",
    "section": "Project",
    "text": "Project\nProteins are fundamental building blocks of life, playing crucial roles in various biological processes. Representing proteins is a multifaceted challenge due to their complex structural and functional characteristics. Traditionally, proteins have been represented in various ways, including as amino acid sequences, 2D graphs, and 3D graphs, each capturing different aspects of protein structure and function. However, these diverse representations often provide limited insight when considered in isolation. In this project, we aim to address this challenge by exploring multimodal protein representation learning methods[1,2], with the goal of discovering the most effective way to represent proteins or fusing these modalities to enhance downstream tasks. The primary goal of this project is to develop techniques that can effectively unify and harness the information contained in different modalities of protein representation. This involves understanding the complementary aspects of amino acid sequences, 2D graphs (e.g., contact maps), and 3D graphs (e.g., protein structures) and finding a way to merge and learn useful representations from them. By improving the representation learning process, we aim to boost the performance of downstream protein-related tasks such as function property prediction, and protein-protein interaction prediction. Our efforts will result in more accurate models that can have a profound impact on bioinformatics and drug discovery.\n[1] Xu, Minghao, et al. “Protst: Multi-modality learning of protein sequences and biomedical texts.”\n[2] Abdine, Hadi, et al. “Prot2Text: Multimodal Protein’s Function Generation with GNNs and Transformers.”"
  },
  {
    "objectID": "logml2024/projects2024/project1/index.html",
    "href": "logml2024/projects2024/project1/index.html",
    "title": "Predicting the pathogenicity of a (missense) mutation",
    "section": "",
    "text": "Abhishek is currently a senior research scientist in AI dept of Illumina, Cambridge. He did his PhD in computer science from Ecole polytechnique and masters in Applied Math from Ecole Centrale Paris. Before that, he also spent time as a researcher at EPFL, INRIA and MPI. In general, he is very interested in ML breakthroughs in real world problems with major societal impact."
  },
  {
    "objectID": "logml2024/projects2024/project1/index.html#abhishek-sharma",
    "href": "logml2024/projects2024/project1/index.html#abhishek-sharma",
    "title": "Predicting the pathogenicity of a (missense) mutation",
    "section": "",
    "text": "Abhishek is currently a senior research scientist in AI dept of Illumina, Cambridge. He did his PhD in computer science from Ecole polytechnique and masters in Applied Math from Ecole Centrale Paris. Before that, he also spent time as a researcher at EPFL, INRIA and MPI. In general, he is very interested in ML breakthroughs in real world problems with major societal impact."
  },
  {
    "objectID": "logml2024/projects2024/project1/index.html#project",
    "href": "logml2024/projects2024/project1/index.html#project",
    "title": "Predicting the pathogenicity of a (missense) mutation",
    "section": "Project",
    "text": "Project\nPredicting the effect of a (missense) mutation accurately is one of the central problems in biology. Currently, two paradigms dominate benchmarks for missense variant pathogenicity prediction, namely PrimateAI-3D and alpha-missense [1]. Interestingly, both follow totally different approaches in their learned model. While PrimateAI-3D is trained from scratch, alpha-missense [2] follows a pretraining-finetuning strategy. Both approaches rely on MSA, Protein 3D structures as input and attention mechanism in their model architecture. The goal of the project is to explore the alpha-missense pretraining-fine tuning strategy and find (simpler/better) alternatives E.g. [3] outlines a (simpler) alternative to the pre-training part (alphafold2.)\n1 . PrimateAI-3D outperforms AlphaMissense in real-world cohorts (Under Review) https://www.medrxiv.org/content/10.1101/2024.01.12.24301193v1\n\nAccurate proteome-wide missense variant effect prediction with AlphaMissense Cheng et al. Science, 2023\nEfficient and accurate prediction of protein structure using RoseTTAFold2. Minkyung Baek, Ivan Anishchenko, Ian Humphreys, Qian Cong, David Baker, Frank DiMaio Bio arxiv, 2023.\nhttps://structural-bioinformatics.netlify.app/index/"
  },
  {
    "objectID": "logml2024/projects2024/project9/index.html",
    "href": "logml2024/projects2024/project9/index.html",
    "title": "On the Geometry of Relative Representations",
    "section": "",
    "text": "Marco Fumero is an ELLIS Ph.D. student at Sapienza University of Rome (moving to IST Austria as PostDoc in prof. Locatello group) in the GLADIA research group led by Professor Emanuele Rodolà. His previous experiences includes a research internship at Autodesk AI LAB and Amazon AWS AI working on geometric deep earning and causal representation learning topics. His research stands at the intersection of geometry and deep learning with a focus on representation learning, disentanglement and out-of-distribution generalization. He has been recently focusing on the direction of latent space communication, and, more broadly, on the question of when how and why distinct learning processes yield similar representation, organizing also the UniReps workshop at NeurIPS 2023 on these topics."
  },
  {
    "objectID": "logml2024/projects2024/project9/index.html#marco-fumero",
    "href": "logml2024/projects2024/project9/index.html#marco-fumero",
    "title": "On the Geometry of Relative Representations",
    "section": "",
    "text": "Marco Fumero is an ELLIS Ph.D. student at Sapienza University of Rome (moving to IST Austria as PostDoc in prof. Locatello group) in the GLADIA research group led by Professor Emanuele Rodolà. His previous experiences includes a research internship at Autodesk AI LAB and Amazon AWS AI working on geometric deep earning and causal representation learning topics. His research stands at the intersection of geometry and deep learning with a focus on representation learning, disentanglement and out-of-distribution generalization. He has been recently focusing on the direction of latent space communication, and, more broadly, on the question of when how and why distinct learning processes yield similar representation, organizing also the UniReps workshop at NeurIPS 2023 on these topics."
  },
  {
    "objectID": "logml2024/projects2024/project9/index.html#project",
    "href": "logml2024/projects2024/project9/index.html#project",
    "title": "On the Geometry of Relative Representations",
    "section": "Project",
    "text": "Project\nNeural networks embed the geometric structure of a data manifold lying in a high-dimensional space into latent representations. Ideally, the distribution of the data points in the latent space should depend only on the task, the data, the loss, and other architecture-specific constraints. However, factors such as the random weights initialization, training hyperparameters, or other sources of randomness in the training phase may induce incoherent latent spaces that hinder any form of reuse. In a recent work [1] , it has been empirically observed that, under the same data and modeling choices, the angles between the encodings within distinct latent spaces do not change.To exploit this observation,we can compute a representation based on the latent similarity between each sample and a fixed set of training points, denoted as anchors. Given a specific choice of anchors and similarity function, the representation will retain different properties: choosing cosine similarity in the latent spaces enforces the desired invariance to angle preserving transformation of the latent space, without any additional training procedures. Neural architectures can leverage these relative representations to guarantee, in practice, invariance to latent isometries and rescalings, effectively enabling latent space communication: from zero-shot model stitching to latent space comparison between diverse settings, on different datasets, spanning various modalities (images, text, graphs), tasks (e.g., classification, reconstruction) and architectures (e.g., CNNs, GCNs, transformers).\nThe choice of similarity function should not be limited to capture invariances to angle preserving transformations. As shown in [2], other choices are good as well, and there’s not a clear best choice for capturing transformation across distinct latent spaces, depending on the setting and nuisance factors that affect the representations and cause the spaces to differ.\nIn this project the objective is to follow this direction by extending the notion of similarity function in two different ways aiming at enhancing the expressivity of this framework using differential geometry and topological analysis tools.\nHigher Order Relative Representations: The first direction aims to enhance the scope of the relative representation function by considering a similarity function that takes three or more arguments in input, switching from considering binary relations between data points to n-way relations. From a geometrical perspective, the standard relative representation corresponds to constructing a bipartite graph between anchors and sample: considering n-way relations is analogous to constructing a simplicial complex in the latent space. For instance, three-way relationships correspond to triangles, four-way to tetrahedra, and so on. The objective here is to assess whether this extension enriches the framework’s expressiveness from both practical and theoretical standpoints.\nRelative Geodesic Representations: Our second area of exploration involves utilizing geodesic distance as a similarity function, calculated within latent spaces, as a metric for relative representation. This approach ensures that the relative space remains invariant to the isometries of the data’s manifold, defined by geodesic distance. The primary objective of this project is to experimentally determine whether this set of transformations results in a more expressive and efficient relative space, one that more accurately encapsulates the relationships and transformations across different latent spaces. A secondary goal is to devise effective approximation techniques for computing geodesics, thereby enhancing the practical efficiency of the method.\n[1] Moschella, L., Maiorca, V., Fumero, M., Norelli, A., Locatello, F., & Rodola, E. (2022). Relative representations enable zero-shot latent space communication.ICLR 2023\n[2] Cannistraci, I., Moschella, L., Fumero, M., Maiorca, V., & Rodolà, E. (2023). From Bricks to Bridges: Product of Invariances to Enhance Latent Space Communication. arXiv\n[3] Shao, H., Kumar, A., & Thomas Fletcher, P. (2018). The Riemannian geometry of deep generative models. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops (pp. 315-323)."
  },
  {
    "objectID": "logml2024/projects2024/project12/index.html",
    "href": "logml2024/projects2024/project12/index.html",
    "title": "Mixed Curvature Graph Neural Networks",
    "section": "",
    "text": "Rishi Sonthalia is a Hedrick Assistant Adjunct Professor in the Math department at UCLA. He completed his Ph.D. from the University of Michigan where he won the Peter Smereka prize for the best Applied Math thesis. Rishi’s research interested is in mathematics for machine learning with a special focus on generalization, optimization, and the role of geometry."
  },
  {
    "objectID": "logml2024/projects2024/project12/index.html#rishi-sonthalia",
    "href": "logml2024/projects2024/project12/index.html#rishi-sonthalia",
    "title": "Mixed Curvature Graph Neural Networks",
    "section": "",
    "text": "Rishi Sonthalia is a Hedrick Assistant Adjunct Professor in the Math department at UCLA. He completed his Ph.D. from the University of Michigan where he won the Peter Smereka prize for the best Applied Math thesis. Rishi’s research interested is in mathematics for machine learning with a special focus on generalization, optimization, and the role of geometry."
  },
  {
    "objectID": "logml2024/projects2024/project12/index.html#project",
    "href": "logml2024/projects2024/project12/index.html#project",
    "title": "Mixed Curvature Graph Neural Networks",
    "section": "Project",
    "text": "Project\nRecent work has shown that hyperbolic geometry can be very useful in improving the performance of neural networks, including graph neural networks. There has also been recent work suggesting that using the correct geometry (based on curvature) can be used to alleviate oversquashing in GNNs. Hence it is currently of relevance to understand the performance of GNNs that use mix curvature geometries. For this a variety of different models have been proposed - product manifolds (Gu et al. 2019), hierarchical hyperbolic spaces (Sonthalia et al. 2022), the space of positive definite matrices (Lopez et al. 2021), as well neural networks that intertwine standard Euclidean and hyperbolic layers (Cui at al. 2022)."
  },
  {
    "objectID": "logml2024/projects2024/project14/index.html",
    "href": "logml2024/projects2024/project14/index.html",
    "title": "Invariantly learning terminal singularities",
    "section": "",
    "text": "Sara Veneziale is a final year PhD student in the Department of Mathematics at Imperial College London, as part of the London School of Geometry and Number Theory. Her research is focused on applying machine learning and data analysis methods to problems arising from pure mathematics, with the aim of helping conjecture formulation. Before starting her PhD she studied Mathematics at the University of Warwick, where she completed her integrated masters in 2020."
  },
  {
    "objectID": "logml2024/projects2024/project14/index.html#sara-veneziale",
    "href": "logml2024/projects2024/project14/index.html#sara-veneziale",
    "title": "Invariantly learning terminal singularities",
    "section": "",
    "text": "Sara Veneziale is a final year PhD student in the Department of Mathematics at Imperial College London, as part of the London School of Geometry and Number Theory. Her research is focused on applying machine learning and data analysis methods to problems arising from pure mathematics, with the aim of helping conjecture formulation. Before starting her PhD she studied Mathematics at the University of Warwick, where she completed her integrated masters in 2020."
  },
  {
    "objectID": "logml2024/projects2024/project14/index.html#project",
    "href": "logml2024/projects2024/project14/index.html#project",
    "title": "Invariantly learning terminal singularities",
    "section": "Project",
    "text": "Project\nRecent work [1] uses a neural network to predict an important, but hard to track, property of geometrical shapes: that of having at worst terminal singularities. The network is later used to generate a lot of data, by replacing a computationally expensive routine with the neural network, to start sketching the landscape of a certain class of geometrical shapes.\nThe experiment in the paper is carried out on a special class of geometrical shapes, called toric varieties. These are highly symmetrical objects and their geometric information is summarised in a weight matrix. However, the space of weight matrices is subject to two group actions: one of the symmetric group shuffling the columns and one of GL_2(Z) acting on the left by multiplication. These two actions change the weight matrix but leave the actual geometrical object unchanged. In the paper, we identify a special form of a weight matrix (which we call the standard form) which precisely identifies one specific representative of each group orbit. Before training and testing, each weight matrix is transformed into the corresponding standard form, which allows us to avoid any symmetry problem in the design of the architecture of the model.\nThe aim of this project is to survey currently available invariant machine learning methods and apply them to this problem. The aim is to compare accuracy, training time, and training sample size with the results obtained by using the standard form. This is a very important analysis to perform. In fact, the results in the paper were carried out only for dimension eight and rank two geometrical objects (toric varieties) and the training of the model required more data than one might like, since it is relatively expensive to calculate. Finding that different methods and architectures perform better with less training data would greatly aid in replicating this result in different dimensions and ranks.\n[1] T. Coates, A. M. Kasprzyk, S. Veneziale. Machine learning detects terminal singularities. Thirty-seventh Conference on Neural Information Processing Systems (2023)."
  },
  {
    "objectID": "logml2024/speakers2024/keynote/kathryn-hess.html",
    "href": "logml2024/speakers2024/keynote/kathryn-hess.html",
    "title": "Kathryn Hess",
    "section": "",
    "text": "website\n  \n\n\n\nKathryn Hess received her PhD in algebraic topology from MIT and held positions at the universities of Stockholm, Nice, and Toronto before moving to the EPFL, where she is now full professor of mathematics and life sciences. Her research focuses on algebraic topology and its applications, primarily in the life sciences, but also in materials science. In her work in applied topology, she has elaborated methods based on topological data analysis for high-throughput screening of nanoporous crystalline materials, classification and synthesis of neuron morphologies, and classification of neuronal network dynamics. She has also developed and applied innovative topological approaches to network theory."
  },
  {
    "objectID": "logml2024/speakers2024/other/joan.html",
    "href": "logml2024/speakers2024/other/joan.html",
    "title": "Joan Bruna",
    "section": "",
    "text": "website\n  \n\n\n\nI am an Associate Professor of Computer Science, Data Science and Mathematics (affiliated) at the Courant Institute and the Center for Data Science, New York University. I belong to the CILVR (Computational Intelligence, Learning, Vision and Robotics) group and I co-founded the MaD (Math and Data) group. During Spring 2020 I was also a Member of the Institute for Advanced Study, part of the Special Year on Optimization, Statistics and Theoretical Machine Learning.\nMy research interests touch several areas of Machine Learning, Signal Processing and High-Dimensional Statistics. I am particularly interested in mathematical foundations of Deep Learning, covering optimization, representation and statistical aspects and their interplay. I am also interested in developing applications to computational science, in areas such as Quantum chemistry, cosmology and climate modeling.\nAs of Fall 2020, I am also a Visiting Scholar at the Flatiron Institute (Simons Foundation), at the Center for Computational Mathematics, where I am co-organizing a working group on the Mathematics of Deep Learning."
  },
  {
    "objectID": "logml2024/speakers2024/other/anthea.html",
    "href": "logml2024/speakers2024/other/anthea.html",
    "title": "Anthea Monod",
    "section": "",
    "text": "website\n  \n\n\n\nAnthea Monod is a Lecturer (Assistant Professor with tenure) in Biomathematics at the Department of Mathematics at Imperial College London. Her research areas primarily include topological data analysis, algebraic statistics, and nonlinear algebra applied to mathematical and computational biology. She earned her PhD from the Swiss Federal Institute of Technology in Lausanne (EPFL). Prior to joining Imperial, she held postdoctoral and visiting faculty positions at the Technion–Israel Institute of Technology, Duke University, Columbia University in the City of New York, and Tel Aviv University."
  },
  {
    "objectID": "logml2022/speaker.html",
    "href": "logml2022/speaker.html",
    "title": "Speakers",
    "section": "",
    "text": "Keynote speakers\n\n\n\n\n\n\n\n\n\n\nHeather Harrington\n\n\nUniversity of Oxford\n\n\n\n\n\n\n\n\n\n\n\n\n\nJustin Solomon\n\n\nMIT\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\nSpeakers\n\n\n\n\n\n\n\n\n\n\nTaco Cohen\n\n\nQualcomm\n\n\n\n\n\n\n\n\n\n\n\n\n\nYang-Hui He\n\n\nCity, University of London\n\n\n\n\n\n\n\n\n\n\n\n\n\nSmita Krishnaswamy\n\n\nYale University\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlice Le Brigant\n\n\nUniversity Paris 1 Pantheon Sorbonne\n\n\n\n\n\n\n\n\n\n\n\n\n\nPetar Veličković\n\n\nDeepMind\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Archives",
      "LOGML 2022",
      "Speakers"
    ]
  },
  {
    "objectID": "logml2022/people2022/team/josh.html",
    "href": "logml2022/people2022/team/josh.html",
    "title": "Josh Southern",
    "section": "",
    "text": "Josh is currently a PhD student at Imperial College London, supervised by Prof. Michael Bronstein and Dr. Kirill Veselkov and part of the AI4Health CDT. He id interested in geometric deep learning, generative models and their applications (particularly in biology, healthcare and sport). He enjoys working on interdisciplinary projects and he is currently collaborating with Bruno Correia’s lab at EPFL on protein design, as well as on a metabolomics + hyperfoods project with a team at Imperial.\nPrior to the PhD, he did his Masters degree in Applied Mathematics, also at Imperial College London, and then spent two years as a Machine Learning Researcher at a Health Tech startup working with wearable device data, trying to benefit the mental health space. Outside of academia, he is obsessed with sport (particularly tennis), pubs and jazz."
  },
  {
    "objectID": "logml2022/people2022/team/francesco.html",
    "href": "logml2022/people2022/team/francesco.html",
    "title": "Francesco Viganò",
    "section": "",
    "text": "Francesco Viganò is a PhD student at Imperial College (LSGNT) with a background in algebraic geometry, now working on spectral graph theory and applications of geometry to graph-based learning models. He is one of the authors of “Equivariant Mesh Attention Networks”, one of the LOGML 2021 projects. He has experience organising events and raising funding, for example as a co-organiser of the LOGML 2022 workshop."
  },
  {
    "objectID": "logml2022/people2022/advisors/marinka.html",
    "href": "logml2022/people2022/advisors/marinka.html",
    "title": "Marinka Zitnik",
    "section": "",
    "text": "website\n  \n\n\n\nMarinka Zitnik is an Assistant Professor in the Department of Biomedical Informatics at Harvard Medical School, Associate Faculty at the Kempner Institute for the Study of Natural and Artificial Intelligence at Harvard University, Associate Member at the Broad Institute of MIT and Harvard, and Affiliated Faculty at the Harvard Data Science Initiative.\nArtificial intelligence is poised to enable breakthroughs in science and reshape medicine. Dr. Zitnik investigates foundations of AI to enhance scientific discovery and to realize individualized diagnosis and treatment. The overarching goal of her lab is to lay the foundations for AI that contribute to the scientific understanding of therapeutic design and genomic medicine or acquire such understanding autonomously.\nAI for Medicine: The state of a person is described with increasing precision incorporating modalities like genetic code, cellular atlases, molecular datasets, and therapeutics—the challenge is how to reason over these data to develop powerful disease diagnostics and empower new kinds of therapies. Our research creates new avenues for fusing knowledge and patient data to give the right patient the right treatment at the right time and have medicinal effects that are consistent from person to person and with results in the laboratory.\nAI for Science: For centuries, the method of discovery—the fundamental practice of science that scientists use to explain the natural world systematically and logically—has remained largely the same. We are using AI to change that. The natural world is interconnected, from the various facets of genome regulation to the molecular and organismal levels. These interactions across different levels yield a bewildering degree of complexity. Our research seeks to disentangle this complexity, developing AI models that advance drug design and help develop new kinds of therapies. Her lab is pioneering AI systems informed by geometry, structure, and symmetry, grounded in scientific knowledge. Current research directions in the lab include:\nLarge pre-trained AI to fuse data modalities like genetic code, single-cell atlases, molecular datasets, and therapeutics via multimodal knowledge graph networks and large pretrained language models Geometric deep learning and graph neural networks to reason about network biology and medicine Multi-scale, individualized, and contextualized AI to transfer prediction prowess acquired from one data type to another and to design contextually adaptive AI that can dynamically adjust outputs to biological contexts, such as patients, diseases, and cell types, in which they operate Better methods for drug design to enhance drug development across therapeutic modalities and stages of development Foundations for scientific discovery in the age of AI. Dr. Zitnik has founded Therapeutics Data Commons and is the faculty lead of the International AI4Science initiative.\nIn 2020 she organized the National Symposium on Drug Repurposing for Future Pandemics on behalf of the National Science Foundation (NSF)"
  },
  {
    "objectID": "logml2022/people2022/advisors/kevin.html",
    "href": "logml2022/people2022/advisors/kevin.html",
    "title": "Kevin Buzzard",
    "section": "",
    "text": "website"
  },
  {
    "objectID": "logml2022/people.html",
    "href": "logml2022/people.html",
    "title": "People",
    "section": "",
    "text": "King’s College London\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHarvard University\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImperial College London\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImperial College London\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImperial College London\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Archives",
      "LOGML 2022",
      "People"
    ]
  },
  {
    "objectID": "logml2022/people.html#organising-committee",
    "href": "logml2022/people.html#organising-committee",
    "title": "People",
    "section": "",
    "text": "King’s College London\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHarvard University\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImperial College London\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImperial College London\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImperial College London\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Archives",
      "LOGML 2022",
      "People"
    ]
  },
  {
    "objectID": "logml2022/people.html#scientific-advisory-committee",
    "href": "logml2022/people.html#scientific-advisory-committee",
    "title": "People",
    "section": "Scientific Advisory Committee",
    "text": "Scientific Advisory Committee\n\n\n\n\n\n\n\n\n\n\nMichael Bronstein\n\n\nUniversity of Oxford\n\n\n\n\n\n\n\n\n\n\n\n\n\nKevin Buzzard\n\n\nImperial College London\n\n\n\n\n\n\n\n\n\n\n\n\n\nRon Kimmel\n\n\nTechnion – Israel Institute of Technology\n\n\n\n\n\n\n\n\n\n\n\n\n\nMarinka Zitnik\n\n\nHarvard University\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Archives",
      "LOGML 2022",
      "People"
    ]
  },
  {
    "objectID": "logml2022/projects2022/project5/index.html",
    "href": "logml2022/projects2022/project5/index.html",
    "title": "Towards training GNNs using explanation feedbacks",
    "section": "",
    "text": "Chirag is a Research Scientist at Adobe Media and Data Science Research Lab and a visiting researcher at Harvard University. Chirag’s research interest includes developing trustworthy machine learning that goes beyond training models for specific downstream tasks and ensuring they satisfy other desirable properties, such as explainability, fairness, and robustness. He is one of the co-founders of the Trustworthy ML Initiative, a forum and seminar series related to Trustworthy ML, and an active member of the Machine Learning Collective research group that focuses on democratizing research by supporting open collaboration in machine learning (ML) research. His works have been published in top machine learning, artificial intelligence, and computer vision conferences, including ICML, AISTATS, UAI, and CVPR."
  },
  {
    "objectID": "logml2022/projects2022/project5/index.html#dr-chirag-agarwal",
    "href": "logml2022/projects2022/project5/index.html#dr-chirag-agarwal",
    "title": "Towards training GNNs using explanation feedbacks",
    "section": "",
    "text": "Chirag is a Research Scientist at Adobe Media and Data Science Research Lab and a visiting researcher at Harvard University. Chirag’s research interest includes developing trustworthy machine learning that goes beyond training models for specific downstream tasks and ensuring they satisfy other desirable properties, such as explainability, fairness, and robustness. He is one of the co-founders of the Trustworthy ML Initiative, a forum and seminar series related to Trustworthy ML, and an active member of the Machine Learning Collective research group that focuses on democratizing research by supporting open collaboration in machine learning (ML) research. His works have been published in top machine learning, artificial intelligence, and computer vision conferences, including ICML, AISTATS, UAI, and CVPR."
  },
  {
    "objectID": "logml2022/projects2022/project5/index.html#project",
    "href": "logml2022/projects2022/project5/index.html#project",
    "title": "Towards training GNNs using explanation feedbacks",
    "section": "Project",
    "text": "Project\nIntroduction. Graph Neural Networks (GNNs) are increasingly used as powerful tools for representing graph-structured data, such as social, information, chemical, and biological networks. As these models are deployed in critical applications (e.g., drug repurposing and crime forecasting), it becomes essential to ensure that the relevant stakeholders understand and trust their decisions. To this end, several approaches have been proposed in recent literature to explain the predictions of GNNs. Depending on the employed techniques, there are three broad categories: perturbation-based, gradient-based, and surrogate-based methods. While several classes of GNN explanation methods have been proposed, there is little to no work done on showing how to use these explanations to improve the GNN performance. In particular, there is no framework that leverages these explanations on the fly and aids the training of a GNN. This lack of understanding mainly stems from the fact that there is very little work on systematically analyzing the use of explanations generated by state-of-the-art GNN explanation methods\nProposal. Previous research in GraphXAI has focused on developing post-hoc explanation methods. In this work, we propose in-hoc GNN explanations that act as feedbacks, on the fly, during the training phase of a GNN model. In addition, we aim to use the generated explanations to improve the GNN training. Using explanations, we plan to define local neighborhoods for neural message passing, e.g., for a correctly classified node u, we can generate the most important nodes in its local neighborhood N_u and then use it as a prior or generate augmented samples for guiding the message-passing of similar nodes in the subsequent training stages. To this extent, we propose to have an explanation layer after every message-passing layer which acts as a unit buffer that passes all the information to the upper layers during the forward pass, but propagates the explanation information back to the graph representations."
  },
  {
    "objectID": "logml2022/projects2022/project3/index.html",
    "href": "logml2022/projects2022/project3/index.html",
    "title": "Distilling large GNNs for molecules",
    "section": "",
    "text": "Johannes Gasteiger is a PhD student at the Data Analytics and Machine Learning group at the TU Munich and has previously interned at FAIR and DeepMind. His main research interests are graph neural networks and any aspects that are relevant for their benefit to society – be it scalability, accuracy, fairness, applications, or robustness. His research has explored ways of combining graph structure with geometric spaces, with a special focus on applications in science, such as molecular systems. Before starting his PhD he studied Computer Science and Physics at the TU Munich and the University of Cambridge."
  },
  {
    "objectID": "logml2022/projects2022/project3/index.html#johannes-gasteiger",
    "href": "logml2022/projects2022/project3/index.html#johannes-gasteiger",
    "title": "Distilling large GNNs for molecules",
    "section": "",
    "text": "Johannes Gasteiger is a PhD student at the Data Analytics and Machine Learning group at the TU Munich and has previously interned at FAIR and DeepMind. His main research interests are graph neural networks and any aspects that are relevant for their benefit to society – be it scalability, accuracy, fairness, applications, or robustness. His research has explored ways of combining graph structure with geometric spaces, with a special focus on applications in science, such as molecular systems. Before starting his PhD he studied Computer Science and Physics at the TU Munich and the University of Cambridge."
  },
  {
    "objectID": "logml2022/projects2022/project3/index.html#project",
    "href": "logml2022/projects2022/project3/index.html#project",
    "title": "Distilling large GNNs for molecules",
    "section": "Project",
    "text": "Project\nPredicting the energy and forces of an atomic system is a central task for multiple fields of science, such as computational chemistry and material science. Large directional GNNs like GemNet [1] currently set the state of the art for this task on diverse datasets such as COLL and OC20. Unfortunately, these models are computationally expensive since they are centered around directed edge embeddings and their message passing is based on triplets or even quadruplets of nodes. This makes them prohibitively expensive for long simulations and large systems such as proteins. This project aims at making these models substantially faster, while retaining a similar level of accuracy. In order to achieve this, we will explore a combination of (i) cheap models that circumvent edge-based representations either partially or altogether such as PaiNN [2] and (ii) distilling large directional GNNs into cheaper regular GNNs.\nThere are three aspects that make distillation of atomic GNNs especially promising. First, the cheaper student GNN can actually have more learnable parameters than the teacher model. This is offset by using the parameters in a more efficient way that circumvents higher-order graph representations. Second, energy-and-force models allow us to generate an arbitrary amount of additional data without requiring a transfer set. Third, previous evidence suggests that models trained on small atomic systems generalize well to large systems. We thus do not need to train the expensive teacher GNN on large systems. Together, these properties suggest that an accurate and cheap atomic GNN might be within reach.\nReferences [1] Gasteiger, Becker, Günnemann. GemNet: Universal Directional Graph Neural Networks for Molecules. NeurIPS 2021 [2] Schütt, Unke, Gastegger. Equivariant message passing for the prediction of tensorial properties and molecular spectra. ICML 2021"
  },
  {
    "objectID": "logml2022/projects2022/project20/index.html",
    "href": "logml2022/projects2022/project20/index.html",
    "title": "Generalized Laplacian positional encoding for graph learning",
    "section": "",
    "text": "Haggai is a Research Scientist at NVIDIA Research and a member of NVIDIA’s TLV lab. His main fields of interest are machine learning, optimization, and shape analysis. In particular, he is working on applying deep learning to irregular domains (e.g., sets, graphs, point clouds, and surfaces), usually by leveraging their symmetry structure. He completed his Ph.D. in 2019 at the Weizmann Institute of Science under the supervision of Prof. Yaron Lipman. He will be joining the Faculty of Electrical and Computer Engineering at the Technion as an Assistant Professor in 2023."
  },
  {
    "objectID": "logml2022/projects2022/project20/index.html#dr-haggai-maron",
    "href": "logml2022/projects2022/project20/index.html#dr-haggai-maron",
    "title": "Generalized Laplacian positional encoding for graph learning",
    "section": "",
    "text": "Haggai is a Research Scientist at NVIDIA Research and a member of NVIDIA’s TLV lab. His main fields of interest are machine learning, optimization, and shape analysis. In particular, he is working on applying deep learning to irregular domains (e.g., sets, graphs, point clouds, and surfaces), usually by leveraging their symmetry structure. He completed his Ph.D. in 2019 at the Weizmann Institute of Science under the supervision of Prof. Yaron Lipman. He will be joining the Faculty of Electrical and Computer Engineering at the Technion as an Assistant Professor in 2023."
  },
  {
    "objectID": "logml2022/projects2022/project20/index.html#project",
    "href": "logml2022/projects2022/project20/index.html#project",
    "title": "Generalized Laplacian positional encoding for graph learning",
    "section": "Project",
    "text": "Project\nLaplacian positional encoding (LPE), i.e. using the graph laplacian eigenvectors as input node features to GNNs [1,2,3], has emerged as a successful and popular positional encoding scheme in the past few years. Nevertheless, the space of positional encodings on graphs is largely unexplored. Following the observation that laplacian eigenvectors are a solution to a specific problem: minimizing the L2 distance between node embeddings, weighted by their affinity score (see [1], section 3.1), our intention is to generalize LPEs to a family of positional encodings defined by the minimisers of the same function with general Lp norms (for p!=2). The PE formulation presented above raises several interesting challenges and questions, which we aim to explore in this project. How to calculate these positional encodings? While the L2 based functional can be easily minimized by solving a generalized eigenvalue problem, it is not clear how to approach the minimization of the generalized functionals above. Is there an efficient way to calculate/approximate these minimizers? What graph/node properties do these embeddings capture? Which graph learning tasks can benefit from them? Where do we expect to see them outperform L2 embeddings? Expressive power. What is the expressive power of GNNs that use these positional encodings? Symmetries. Is it possible to devise GNN architectures that are invariant to the natural symmetries of these embeddings? (see [4] for a solution to the L2 case) The plan is to explore these questions in the week we have and then continue the project with a focus on a specific challenge, aiming for a submission to ICLR/ICML. Candidates with some prior knowledge in optimization theory / GNN expressive power / equivariant deep learning are the most suitable for this project.\nReferences [1] Laplacian Eigenmaps for Dimensionality Reduction and Data Representation, Belkin and Niyogi, Neural Computation 2003 [2] Rethinking graph transformers with spectral attention, Kreuzer et al., NeurIPS 2021 [3] A generalization of transformer networks to graphs, Dwivedi et al., AAAI WS 2021 [4] Sign and Basis Invariant Networks for Spectral Graph Representation Learning, Lim et al., 2022"
  },
  {
    "objectID": "logml2022/projects2022/project11/index.html",
    "href": "logml2022/projects2022/project11/index.html",
    "title": "Exploiting domain structure for music ML tasks",
    "section": "",
    "text": "Cătălina is a Research Scientist at DeepMind, having previously obtained her PhD at the University of Cambridge, supervised by Pietro Liò. Her main interests are learning in challenging data scenarios and using these methods in real-world applications; besides that, she deeply cares about music and is excited about the prospect of developing ML tools that support people who wish to be (more) creative! In the past, she has done research on cross-modal architectures, V&LN and VQA, scene graphs, hierarchical & topological relational representations and Neural Process-based models."
  },
  {
    "objectID": "logml2022/projects2022/project11/index.html#dr-cătălina-cangea",
    "href": "logml2022/projects2022/project11/index.html#dr-cătălina-cangea",
    "title": "Exploiting domain structure for music ML tasks",
    "section": "",
    "text": "Cătălina is a Research Scientist at DeepMind, having previously obtained her PhD at the University of Cambridge, supervised by Pietro Liò. Her main interests are learning in challenging data scenarios and using these methods in real-world applications; besides that, she deeply cares about music and is excited about the prospect of developing ML tools that support people who wish to be (more) creative! In the past, she has done research on cross-modal architectures, V&LN and VQA, scene graphs, hierarchical & topological relational representations and Neural Process-based models."
  },
  {
    "objectID": "logml2022/projects2022/project11/index.html#project",
    "href": "logml2022/projects2022/project11/index.html#project",
    "title": "Exploiting domain structure for music ML tasks",
    "section": "Project",
    "text": "Project\nLearning to represent and generate music is a highly relevant task for the machine learning field. This data domain is ideal for density estimation tasks and exhibits many interesting properties, such as long-term dependencies and patterns residing at various scales. More crucially, music generation is a main pillar of creative AI, which complements the ML community efforts in pushing scientific progress and gives us the amazing opportunity to assist artists in their creative process [0]!\nExisting state-of-the-art approaches for symbolic music generation [1, 2] operate on and produce sequences of tokens. However, the music domain contains structure at several scales: local (e.g. chords, arpeggios, multiple voices being played together in a fugue) and long-term/global (e.g. ABA form [3], the key that a piece is written in, repeating patterns/motifs). In that sense, there have been relatively few studies (e.g. [4, 5, 6, 7, 8]) that investigate explicit representations of structure for modelling and generating music. Each of these works has certain limitations—for instance, each piece modelled in [4] (see Fig. 2) is turned into a single graph by connecting all notes with temporal links, but does not capture the fundamental music-theoretical dependencies and insights that composers most likely used when writing the piece. Alternatively, the work in [7] and a few others look at rhythm-specific encodings only. Perhaps the closest modelling strategy to the one intended for this project is [8], where the authors choose to encode various musical relationships between bars of the score; however, it would be easier to work with more general graph representations of music, building up from first principles such as the circle of fifths / the tonality of a piece (e.g. use a sliding window and compute relationships / a graph within each local window and between windows at discrete points in the score - there are endless ways to think about this!)\nThis project aims to study the effects of leveraging music-theoretical graph representations in ML models. The goal is to encode symbolic music sequences in a principled manner that reflects the composition process and underlying structure up to a greater extent that previously. This encoding would then be passed as (additional) input to a model [1, 2, 9]. Finally, the effects on model performance would be studied in classification and/or generation scenarios (TBD based on time constraints).\nSteps: 1. Download dataset(s), choose task(s) 2. Become familiar with basic music theory concepts and design 1 or more ways of encoding the structure in symbolic music (see Appendix A.2 [1] for a description of event-based MIDI representations; see Chordify in Resources; see Wiki) 3. Set up model codebase and obtain baseline performance on chosen tasks 4. Add relational structure to the model and find suitable ways to encode it 5. Interpret the new results and investigate changes in model processing (e.g. visualising attention in layers, emerging patterns) 6. Open-source code, to allow researchers to preprocess their own music data and build more graph-based ML models for music tasks!\nResources - Music Transformer GitHub codebase - MusicVAE GitHub codebase - Perceiver AR codebase (to be released soon) - MAESTRO dataset - MetaMIDIDataset - Lakh MIDI dataset - MusPy - A toolkit for symbolic music generation - Chordify - music21 Documentation - Symbolic Representations - Fundamentals of music - A structural way to encode music (https://en.wikipedia.org/wiki/Tonnetz) studied by https://ojs.aaai.org/index.php/AAAI/article/view/11880 (here, the resulting encoding was an image)\nReferences [0] Music 2020 Archives - AI Art Gallery (http://www.aiartonline.com/category/music-2020/) [1] Music Transformer [2] General-purpose, long-context autoregressive modeling with Perceiver AR [3] Ternary form [4] Graph Neural Network for Music Score Data and Modeling Expressive Piano Performance [5] StructureNet: Inducing Structure in Generated Melodies - IR Anthology [6] Neurosymbolic Deep Generative Models for Sequence Data with Relational Constraints [7] Pop Music Transformer: Beat-based Modeling and Generation of Expressive Pop Piano Compositions [8] MELONS: generating melody with long-term structure using transformers and structure graph [9] A Hierarchical Latent Vector Model for Learning Long-Term Structure in Music"
  },
  {
    "objectID": "logml2022/projects2022/project17/index.html",
    "href": "logml2022/projects2022/project17/index.html",
    "title": "Geometric tools for investigating loss landscapes of deep neural networks",
    "section": "",
    "text": "James Lucas is a Research Scientist at NVIDIA within the Toronto AI lab. He is also a PhD candidate at the University of Toronto, where he is supervised by Richard Zemel and Roger Grosse. James has a broad set of research interests within deep learning and ML more broadly. In the past, his research has focused on understanding and improving the optimization of deep neural networks. In particular, he has worked towards understanding the loss landscape geometry of these models and how we may leverage their properties in practice. He is also excited by deep generative models for 3D geometry and improving the data-efficiency of deep learning systems."
  },
  {
    "objectID": "logml2022/projects2022/project17/index.html#dr-james-lucas",
    "href": "logml2022/projects2022/project17/index.html#dr-james-lucas",
    "title": "Geometric tools for investigating loss landscapes of deep neural networks",
    "section": "",
    "text": "James Lucas is a Research Scientist at NVIDIA within the Toronto AI lab. He is also a PhD candidate at the University of Toronto, where he is supervised by Richard Zemel and Roger Grosse. James has a broad set of research interests within deep learning and ML more broadly. In the past, his research has focused on understanding and improving the optimization of deep neural networks. In particular, he has worked towards understanding the loss landscape geometry of these models and how we may leverage their properties in practice. He is also excited by deep generative models for 3D geometry and improving the data-efficiency of deep learning systems."
  },
  {
    "objectID": "logml2022/projects2022/project17/index.html#project",
    "href": "logml2022/projects2022/project17/index.html#project",
    "title": "Geometric tools for investigating loss landscapes of deep neural networks",
    "section": "Project",
    "text": "Project\nNeural networks have achieved extraordinary success, but this achievement is limited to only those networks that are trainable with stochastic gradient descent (SGD). What is special about these networks? At the very least, SGD can effectively traverse their loss landscapes. In this project, we will train neural networks and investigate their loss landscapes using tools from geometry.\nShocking phenomena have been discovered in relation to loss landscape geometry. For example, the existence of sparse subnetworks within larger networks that can be trained in isolation to achieve comparable performance [1]. Or linear paths within loss landscapes that exhibit monotonically decreasing loss [2, 3] (or that connect together several minima [4, 5]). Explaining these phenomena is challenging because loss landscapes in deep learning are extremely high dimensional and come with minimal guarantees on their geometric structure. However, in practice, we observe a surprisingly rich structure in the loss landscapes of neural networks.\nWe will review existing work that has successfully utilized geometric tools to better understand deep neural networks (for example, [3,6]). While this project will have its roots in deep learning theory, there will be a significant element of empirical analysis as we implement and evaluate standard deep learning architectures. Some general questions that we may hope to address include: how does the geometry around initialization differ from that at local minima? How does the geometry at initialization shape optimization? Can we identify global structure in the loss landscapes of neural networks?\nReferences [1] The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks, Frankle & Carbin. ICLR 2019 [2] Qualitatively characterizing neural network optimization problems, Goodfellow et al. ICLR 2015 [3] Analyzing Monotonic Linear Interpolation in Neural Network Loss Landscapes, Lucas et al. ICML 2021 [4] Topology and geometry of half-rectified network optimization, Freeman & Bruna. ICLR 2017 [5] Linear mode connectivity and the lottery ticket hypothesis, Frankle et al. ICML 2020 [6] Exponential expressivity in deep neural networks through transient chaos, Poole et al. NeurIPS 2016"
  },
  {
    "objectID": "logml2022/projects2022/project19/index.html",
    "href": "logml2022/projects2022/project19/index.html",
    "title": "Adaptive frame averaging for invariant and equivariant representations",
    "section": "",
    "text": "Bruno Ribeiro is an Assistant Professor in the Department of Computer Science at Purdue University. He obtained his Ph.D. at the University of Massachusetts Amherst and was a postdoctoral fellow at Carnegie Mellon University from 2013-2015. His research interests involve devising learning algorithms and methods that operate in nontrivially structured data (such as graphs) and in the intersection between causality and machine learning. He received an NSF CAREER award in 2020 and has several best paper awards."
  },
  {
    "objectID": "logml2022/projects2022/project19/index.html#prof-bruno-ribeiro",
    "href": "logml2022/projects2022/project19/index.html#prof-bruno-ribeiro",
    "title": "Adaptive frame averaging for invariant and equivariant representations",
    "section": "",
    "text": "Bruno Ribeiro is an Assistant Professor in the Department of Computer Science at Purdue University. He obtained his Ph.D. at the University of Massachusetts Amherst and was a postdoctoral fellow at Carnegie Mellon University from 2013-2015. His research interests involve devising learning algorithms and methods that operate in nontrivially structured data (such as graphs) and in the intersection between causality and machine learning. He received an NSF CAREER award in 2020 and has several best paper awards."
  },
  {
    "objectID": "logml2022/projects2022/project19/index.html#project",
    "href": "logml2022/projects2022/project19/index.html#project",
    "title": "Adaptive frame averaging for invariant and equivariant representations",
    "section": "Project",
    "text": "Project\nMany machine learning tasks require learning functions that are invariant or equivariant to known symmetries of the input data. Unfortunately, there is a significant challenge in the design of neural network architectures that simultaneously (a) take into account the symmetries, (b) are expressive enough to have small generalization error, and (c) are computationally efficient. Murphy et al. [1, 2] have shown that it is possible to sacrifice (c) –computational efficiency– for (a) and (b) with the use of symmetrization (Reynolds operator). Recently, Puny et al. [3] have proposed a method (Frame Averaging) to improve the efficiency of symmetrization. However, in some tasks, Frame Averaging can lead to large generalization errors. This is because of a fundamental trade-off between computationally efficient and generalization error in symmetrization. This project will study this trade-off and propose efficient solutions to achieve both computational efficiency and better generalization error in the affected tasks.\nReferences [1] Murphy, Ryan L., Balasubramaniam Srinivasan, Vinayak Rao, and Bruno Ribeiro. Janossy pooling: Learning deep permutation-invariant functions for variable-size inputs. ICLR 2019. [2] Murphy, Ryan, Balasubramaniam Srinivasan, Vinayak Rao, and Bruno Ribeiro. Relational pooling for graph representations. In International Conference on Machine Learning, pp. 4663-4673. PMLR, 2019. [3] Puny, Omri, Matan Atzmon, Heli Ben-Hamu, Edward J. Smith, Ishan Misra, Aditya Grover, and Yaron Lipman. Frame Averaging for Invariant and Equivariant Network Design. ICLR 2022."
  },
  {
    "objectID": "logml2022/projects2022/project1/index.html",
    "href": "logml2022/projects2022/project1/index.html",
    "title": "Data reductions for graph attention variants",
    "section": "",
    "text": "Kaustubh is a researcher at Emory University working in Natural Language Processing & Graph Neural Networks. He has a varied set of interests spanning efficient deep learning architectures, graph neural networks and dialog systems. He has been co-organizer of the GEM (Generation, Evaluation, Metrics) workshops for the past 2 years and the co-creator of the wisdom-of-researchers effort of NL-Augmenter last year. For 7 years, he led the R&D team of Amelia (IPsoft) designing the natural language capabilities for their conversation agent Amelia."
  },
  {
    "objectID": "logml2022/projects2022/project1/index.html#kaustubh-dholé",
    "href": "logml2022/projects2022/project1/index.html#kaustubh-dholé",
    "title": "Data reductions for graph attention variants",
    "section": "",
    "text": "Kaustubh is a researcher at Emory University working in Natural Language Processing & Graph Neural Networks. He has a varied set of interests spanning efficient deep learning architectures, graph neural networks and dialog systems. He has been co-organizer of the GEM (Generation, Evaluation, Metrics) workshops for the past 2 years and the co-creator of the wisdom-of-researchers effort of NL-Augmenter last year. For 7 years, he led the R&D team of Amelia (IPsoft) designing the natural language capabilities for their conversation agent Amelia."
  },
  {
    "objectID": "logml2022/projects2022/project1/index.html#project",
    "href": "logml2022/projects2022/project1/index.html#project",
    "title": "Data reductions for graph attention variants",
    "section": "Project",
    "text": "Project\nThere are a lot of data reduction techniques that have been applied over general transformers like Linformer (JL-Lemma), Reformer (using LSH), Nymstromformer (using Nymstrom method), etc. However, many of these approaches which have sped up attention computation have not been explored for speeding up graph attention variants. I am vouching for performing a baseline set of experiments to test some of these data reduction approaches to approximate GAT/GATv2’s attention and measure how much drop on some downstream task is seen."
  },
  {
    "objectID": "logml2022/projects2022/project9/index.html",
    "href": "logml2022/projects2022/project9/index.html",
    "title": "Line bundle cohomology formulae on Calabi-Yau threefolds",
    "section": "",
    "text": "Andrei is a Stephen Hawking Fellow in Theoretical Physics. He studied Physics in Bremen and received an MSc in Theoretical and Mathematical Physics from LMU Munich. He did his doctoral studies in Oxford, followed by two postdocs, one in Oxford and one in Uppsala. His research lies at the interface of string theory, algebraic geometry, and machine learning and focuses on developing mathematical and computational tools to investigate string theory and its implications for particle physics, cosmology and quantum gravity. One of the central drives of his research is to construct models of particle physics from string theory. His current work is related to the study of vector bundle cohomology and its applications to string model building. Computing cohomology is a crucial and time consuming step in the derivation of the spectrum of low-energy particles resulting from string compactifications. He has shown that in many cases of interest in string theory topological formulae for cohomology exist. These mathematical shortcuts can reduce the time needed for deciding the physical viability of a string compactification from several months of computer algebra to a split of a second. He is also working on adapting, refining and applying machine learning techniques to problems in string theory and mathematics. He is using these to generate new conjectures about Calabi-Yau manifolds, vector bundles and cohomology, as well as to probe the landscape of string theory solutions relevant for particle physics."
  },
  {
    "objectID": "logml2022/projects2022/project9/index.html#dr-andrei-constantin",
    "href": "logml2022/projects2022/project9/index.html#dr-andrei-constantin",
    "title": "Line bundle cohomology formulae on Calabi-Yau threefolds",
    "section": "",
    "text": "Andrei is a Stephen Hawking Fellow in Theoretical Physics. He studied Physics in Bremen and received an MSc in Theoretical and Mathematical Physics from LMU Munich. He did his doctoral studies in Oxford, followed by two postdocs, one in Oxford and one in Uppsala. His research lies at the interface of string theory, algebraic geometry, and machine learning and focuses on developing mathematical and computational tools to investigate string theory and its implications for particle physics, cosmology and quantum gravity. One of the central drives of his research is to construct models of particle physics from string theory. His current work is related to the study of vector bundle cohomology and its applications to string model building. Computing cohomology is a crucial and time consuming step in the derivation of the spectrum of low-energy particles resulting from string compactifications. He has shown that in many cases of interest in string theory topological formulae for cohomology exist. These mathematical shortcuts can reduce the time needed for deciding the physical viability of a string compactification from several months of computer algebra to a split of a second. He is also working on adapting, refining and applying machine learning techniques to problems in string theory and mathematics. He is using these to generate new conjectures about Calabi-Yau manifolds, vector bundles and cohomology, as well as to probe the landscape of string theory solutions relevant for particle physics."
  },
  {
    "objectID": "logml2022/projects2022/project9/index.html#project",
    "href": "logml2022/projects2022/project9/index.html#project",
    "title": "Line bundle cohomology formulae on Calabi-Yau threefolds",
    "section": "Project",
    "text": "Project\nCohomology is a universal tool in mathematics: from topology and geometry to algebra and number theory, cohomology is used to quantify the possible ways in which a problem fails to meet the naive expectations of the problem solver. As such, estimates of cohomology represent a vital key to problem solving. In areas of theoretical physics such as string theory, cohomology is linked to the low-energy quantum excitations of fields and strings that can be experimentally observed.\nIn practice, cohomology computations are notoriously difficult to carry out in general. However, it has been recently shown that line bundle cohomology dimensions on several classes of two-dimensional and three-dimensional complex manifolds, including compact toric surfaces, generalised del Pezzo surfaces, K3 surfaces and Calabi-Yau threefolds, are described by closed-form formulae. This new technique for the computation of cohomology uses a combination of algebro-geometric methods and exploratory machine learning methods. The goal of the project will be to go one step further and discover exact generating functions for line bundle cohomology on Calabi-Yau threefolds.\nReferences: [1] C. Brodie, A. Constantin, J. Gray, A. Lukas, F. Ruehle, Recent Developments in Line Bundle Cohomology and Applications to String Phenomenology, Nankai Symposium on Mathematical Dialogues 2021 Conference Proceedings, arXiv: 2112.12107. [2] C. Brodie, A. Constantin, R. Deen, A. Lukas, Machine Learning Line Bundle Cohomology, Fortsch.Phys. 68 (2020) 1, 1900087, arXiv: 1906.08730. [3] C. Brodie, A. Constantin, A. Lukas, Flops, Gromov-Witten invariants and symmetries of line bundle cohomology on Calabi-Yau three-folds, J.Geom.Phys. 171 (2022), arXiv: 2010.06597. [4] C. Brodie, A. Constantin, Cohomology Chambers on Complex Surfaces and Elliptically Fibered Calabi-Yau Three-folds, arXiv: 2009.01275."
  },
  {
    "objectID": "logml2022/projects2022/project12/index.html",
    "href": "logml2022/projects2022/project12/index.html",
    "title": "DImplicit neural filters for steerable CNNs",
    "section": "",
    "text": "Gabriele Cesa is a Research Associate at Qualcomm AI Research, Amsterdam and a PhD student at University of Amsterdam, under the supervision of Max Welling. Gabriele’s research focuses on augmenting machine learning methods with prior information about the geometry of a problem to achieve improved data efficiency and generalization. A particular emphasis has been given to equivariant neural networks, which can encode our knowledge about the symmetries in the data into the model’s architecture. Previously, Gabriele received a Master degree in Artificial Intelligence at the University of Amsterdam, Netherlands and a Bachelor degree in computer science at the University of Trento, Italy."
  },
  {
    "objectID": "logml2022/projects2022/project12/index.html#gabriele-cesa",
    "href": "logml2022/projects2022/project12/index.html#gabriele-cesa",
    "title": "DImplicit neural filters for steerable CNNs",
    "section": "",
    "text": "Gabriele Cesa is a Research Associate at Qualcomm AI Research, Amsterdam and a PhD student at University of Amsterdam, under the supervision of Max Welling. Gabriele’s research focuses on augmenting machine learning methods with prior information about the geometry of a problem to achieve improved data efficiency and generalization. A particular emphasis has been given to equivariant neural networks, which can encode our knowledge about the symmetries in the data into the model’s architecture. Previously, Gabriele received a Master degree in Artificial Intelligence at the University of Amsterdam, Netherlands and a Bachelor degree in computer science at the University of Trento, Italy."
  },
  {
    "objectID": "logml2022/projects2022/project12/index.html#project",
    "href": "logml2022/projects2022/project12/index.html#project",
    "title": "DImplicit neural filters for steerable CNNs",
    "section": "Project",
    "text": "Project\nSteerable CNNs [1] and, in particular, Euclidean Steerable CNNs [2, 3] provide a general framework for building neural networks which are equivariant to groups beyond translations, for example reflections and rotations. Euclidean Steerable CNNs rely on standard convolution with steerable filters, i.e. filters satisfying a steerability constraint [2, 3, 4]. Filters are typically parameterized by linear combination of a steerable filter basis [4], obtained by analytically solving the steerability constraint. However, we note that if the filter is parameterized by an implicit MLP [5] (e.g. for point cloud data), the steerability constraint just requires the MLP to be equivariant. This suggests a simpler way to build steerable CNNs, which does not rely on the polar decomposition of the filters and spherical harmonics. Moreover, this strategy enjoys the flexibility of the implicit neural function, supporting wider filters with limited parameter cost. Since no steerable basis is required, this approach can also be easily extended to different equivariance groups with minor changes, provided an equivariant MLP can be built. We will explore applications for point cloud data (with full rotational symmetries or with only azimuthal symmetries) and molecular data.\nReferences [1] A General Theory of Equivariant CNNs on Homogeneous Spaces, Taco Cohen and Mario Geiger and Maurice Weiler [2] 3D Steerable CNNs: Learning Rotationally Equivariant Features in Volumetric Data, Maurice Weiler and Mario Geiger and Max Welling and Wouter Boomsma and Taco Cohen [3] Tensor field networks: Rotation- and translation-equivariant neural networks for 3D point clouds, Nathaniel Thomas and Tess Smidt and Steven Kearnes and Lusann Yang and Li Li and Kai Kohlhoff and Patrick Riley [4] A Program to Build E(N)-Equivariant Steerable CNNs, Gabriele Cesa and Leon Lang and Maurice Weiler [5] PointConv: Deep Convolutional Networks on 3D Point Clouds, Wenxuan Wu and Zhongang Qi and Li Fuxin"
  },
  {
    "objectID": "logml2022/projects2022/project14/index.html",
    "href": "logml2022/projects2022/project14/index.html",
    "title": "Helmhotlz-Hodge Laplacians: edge flows and simplicial learning",
    "section": "",
    "text": "In the summer of 2020, Stefan joined UC Davis and the TETRAPODS Institute of Data Science, working under Prof. Naoki Saito. and the UC Davis math department as a Krener Assistant Professor. He teaches classes in applied math, specifically: numerical analysis, differential equations, optimization, and signal processing. Previously, he obtained a Ph.D. in the Applied Mathematics Department at Rensselaer Polytechnic Institute, working under Prof. Rongjie Lai. At RPI, he was secretary of our SIAM chapter, spent a semester at UCLA attending the IPAM long program “Geometry and Learning from Data in 3D and Beyond” and worked with researchers at IBM through the Rensselaer-IBM Artificial Intelligence Research Collaboration. He completed my thesis, “Computational Analysis of Deformable Manifolds: from Geometric Modeling to Deep Learning” in the spring of 2020, and won the Bill and Nancy Siegmann Applied Mathematical Modeling Prize. He studies computational differential geometry focusing on PDEs and optimization problems involving manifolds. Specifically, he is interested in applications in shape processing, signal analysis, and machine learning in non-Euclidean domains. He also has strong interests in harmonic analysis and generative processes for both Euclidean and non-Euclidean data."
  },
  {
    "objectID": "logml2022/projects2022/project14/index.html#prof-stefan-schonsheck",
    "href": "logml2022/projects2022/project14/index.html#prof-stefan-schonsheck",
    "title": "Helmhotlz-Hodge Laplacians: edge flows and simplicial learning",
    "section": "",
    "text": "In the summer of 2020, Stefan joined UC Davis and the TETRAPODS Institute of Data Science, working under Prof. Naoki Saito. and the UC Davis math department as a Krener Assistant Professor. He teaches classes in applied math, specifically: numerical analysis, differential equations, optimization, and signal processing. Previously, he obtained a Ph.D. in the Applied Mathematics Department at Rensselaer Polytechnic Institute, working under Prof. Rongjie Lai. At RPI, he was secretary of our SIAM chapter, spent a semester at UCLA attending the IPAM long program “Geometry and Learning from Data in 3D and Beyond” and worked with researchers at IBM through the Rensselaer-IBM Artificial Intelligence Research Collaboration. He completed my thesis, “Computational Analysis of Deformable Manifolds: from Geometric Modeling to Deep Learning” in the spring of 2020, and won the Bill and Nancy Siegmann Applied Mathematical Modeling Prize. He studies computational differential geometry focusing on PDEs and optimization problems involving manifolds. Specifically, he is interested in applications in shape processing, signal analysis, and machine learning in non-Euclidean domains. He also has strong interests in harmonic analysis and generative processes for both Euclidean and non-Euclidean data."
  },
  {
    "objectID": "logml2022/projects2022/project14/index.html#project",
    "href": "logml2022/projects2022/project14/index.html#project",
    "title": "Helmhotlz-Hodge Laplacians: edge flows and simplicial learning",
    "section": "Project",
    "text": "Project\nThe Laplacian operator is a ubiquitous tool in signal processing. Generalizations of this operator to manifolds (i.e., the Laplace-Beltrami operator) and graphs (i.e., the graph Laplacian) have proved to be very useful for learning on non-euclidean domains. Helmholtz-Hodge theory can further generalize these operators to higher-order differential forms. For example, the continuous 1-Laplacian can be used to analyze vector-fields on manifolds [1] and edge flows on directed or undirected graphs can be analyzed via wavelet-like bases generated by the Hodge-Laplacian [3]. Similarly, the dth-Laplacian can be used to define convolutional operations on d-degree complexes [2]. This project will use these operators to develop methods for solving signal processing problems such as denoising and in-painting on simplicial domains focussing on edge-based and face-based signals. We do so using both traditional variational methods and neural network-based approaches.\nReferences [1] Y.-C. Chen, M. Meila and I. G. Kevrekidis, Helmholtzian eigenmap: Topological feature discovery and edge flow learning from point cloud data, arXiv preprint arXiv:2103.07626, (2021). [2] S. Ebli, M. Defferrard, and G. Spreemann, Simplicial neural networks, arXiv preprint arXiv:2010.03633, (2020). [3] T. M. Roddenberry, F. Frantzen, M. T. Schaub, and S. Segarra, Hodgelets: Localized spectral representations of flows on simplicial complexes, arXiv preprint arXiv:2109.08728, (2021)."
  },
  {
    "objectID": "logml2022/projects2022/project22/index.html",
    "href": "logml2022/projects2022/project22/index.html",
    "title": "Latent graph learning for multivariate time series",
    "section": "",
    "text": "Dr. Xiang Zhang is a postdoctoral fellow at Harvard University. He received his Ph.D. degree in Computer Science at the University of New South Wales. His research interests lie in data mining, deep learning, and graph representation learning with applications in pervasive healthcare, Brain-Computer Interfaces (BCIs), and biomedical sciences. Xiang’s research outcomes have been published in prestigious conferences (such as ICLR, NeurIPS, KDD, and UbiComp) and journals (like Nature Computational Science). Moreover, he has published a book, which is the only off-the-shelf one of its kind, in deep learning for BCI."
  },
  {
    "objectID": "logml2022/projects2022/project22/index.html#dr-xiang-zhang",
    "href": "logml2022/projects2022/project22/index.html#dr-xiang-zhang",
    "title": "Latent graph learning for multivariate time series",
    "section": "",
    "text": "Dr. Xiang Zhang is a postdoctoral fellow at Harvard University. He received his Ph.D. degree in Computer Science at the University of New South Wales. His research interests lie in data mining, deep learning, and graph representation learning with applications in pervasive healthcare, Brain-Computer Interfaces (BCIs), and biomedical sciences. Xiang’s research outcomes have been published in prestigious conferences (such as ICLR, NeurIPS, KDD, and UbiComp) and journals (like Nature Computational Science). Moreover, he has published a book, which is the only off-the-shelf one of its kind, in deep learning for BCI."
  },
  {
    "objectID": "logml2022/projects2022/project22/index.html#project",
    "href": "logml2022/projects2022/project22/index.html#project",
    "title": "Latent graph learning for multivariate time series",
    "section": "Project",
    "text": "Project\nMultivariate time series are prevalent in a variety of domains, including healthcare, transportation, space science, biology, and finance. Recently, it has attracted increasing attention to leverage the structural relationships among channels to learn stronger representation. Previous studies demonstrated that inter-sensor correlations bring rich information in modelling time series. In this project, we will learn how to use a graph neural network model to learn the latent relationship between different time series sensors (such as different leads in ECG signals). The learned graph patterns will correspond to the task. The model will be evaluated by the PhysioNet benchmark."
  },
  {
    "objectID": "logml2022/speakers2022/keynote/justin-solomon.html",
    "href": "logml2022/speakers2022/keynote/justin-solomon.html",
    "title": "Justin Solomon",
    "section": "",
    "text": "website\n  \n\n\n\nJustin Solomon is an associate professor of Electrical Engineering and Computer Science at MIT. He leads the Geometric Data Processing Group in the MIT Computer Science and Artificial Intelligence Laboratory (CSAIL), which studies problems at the intersection of geometry, optimization, and applications."
  },
  {
    "objectID": "logml2022/speakers2022/other/smita-krishnaswamy.html",
    "href": "logml2022/speakers2022/other/smita-krishnaswamy.html",
    "title": "Smita Krishnaswamy",
    "section": "",
    "text": "website\n  \n\n\n\nSmita Krishnaswamy is an Associate Professor in the departments of Computer Science (SEAS) and Genetics (YSM). She is part of the programs in Applied Mathematics, Computational Biology & Bioinformatics and Interdisciplinary Neuroscience. She is also affiliated with the Yale Center for Biomedical Data Science, Yale Cancer Center, Wu-Tsai Institute. Smita’s lab works at the intersection of computer science, applied math, computational biology, and signal processing to develop representation-learning and deep learning methods that enable exploratory analysis, scientific inference and prediction from big biomedical datasets. She has applied her methods on datasets generated from single-cell sequencing, structural biology, biomedical imaging, brain activity recording, electronic health records on a wide variety of biological, cellular, and disease systems. Her techniques generally incorporate mathematical priors from graph spectral theory, manifold learning, signal processing, and topology into machine learning and deep learning frameworks, in order to denoise and model the underlying systems faithfully for predictive insight. Currently her methods are being widely used for data denoising, visualization, generative modeling, dynamics. modeling, comparative analysis and domain transfer.\nSmita teaches several courses including: Deep Learning Theory and Applications, Unsupervised learning, and Geometric and Topological Methods in Machine Learning. Prior to joining Yale, Smita completed her postdoctoral training at Columbia University in the systems biology department where she focused on learning computational models of cellular signaling from single-cell mass cytometry data. She obtained her Ph.D. from EECS department at University of Michigan where her research focused on algorithms for automated synthesis and probabilistic verification of nanoscale logic circuits. Following her time in Michigan, Smita spent 2 years at IBM’s TJ Watson Research Center as a researcher in the systems division where she worked on automated bug finding and error correction in logic. Smita’s work over the years has won several awards including the NSF CAREER Award, Sloan Faculty Fellowship, and Blavatnik fund for Innovation."
  },
  {
    "objectID": "logml2022/speakers2022/other/alice-le-brigant.html",
    "href": "logml2022/speakers2022/other/alice-le-brigant.html",
    "title": "Alice Le Brigant",
    "section": "",
    "text": "website\n  \n\n\n\nAlice Le Brigant is an Assistant Professor in the Applied Mathematics team SAMM at University Paris 1 Pantheon Sorbonne. Previously, she was a Postdoctoral Fellow at the French Civil Aviation School, in collaboration with the Toulouse Mathematics Institute. She obtained her PhD in Applied Mathematics from the University of Bordeaux in 2017. Her research interests are at the interface of statistics and applied Riemannian geometry."
  },
  {
    "objectID": "speaker.html",
    "href": "speaker.html",
    "title": "Speakers",
    "section": "",
    "text": "Home\n    People\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFilippo Maria Bianchi\n\n\nUiT the Arctic University of Norway\n\n\n\n\n\n\n\n\n\n\n\n\n\nTolga Birdal\n\n\nImperial College London\n\n\n\n\n\n\n\n\n\n\n\n\n\nCoralia Cartis\n\n\nUniversity of Oxford\n\n\n\n\n\n\n\n\n\n\n\n\n\nMarco Cuturi\n\n\nApple, CREST - ENSAE, Institut Polytechnique de Paris\n\n\n\n\n\n\n\n\n\n\n\n\n\nEstelle Massart\n\n\nUCLouvain\n\n\n\n\n\n\n\n\n\n\n\n\n\nBastian Rieck\n\n\nUniversity of Fribourg\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrancisco J. R. Ruiz\n\n\nGoogle DeepMind\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlice Barbara Tumpach\n\n\nWolfgang Pauli Institute (WPI), Vienna\n\n\n\n\n\n\n\n\n\n\n\n\n\nMarinka Zitnik\n\n\nHarvard University\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "people/team/valentina.html",
    "href": "people/team/valentina.html",
    "title": "Valentina Giunchiglia",
    "section": "",
    "text": "Valentina is a second-year PhD student jointly affiliated with Imperial College London and Harvard University funded by the Medical Research Council, under the supervision of Prof. Marinka Zitnik and Prof. Adam Hampshire. Her work lies at the intersection of artificial intelligence, biology, and health, with a focus on neuroscience. She is interested in a holistic study of diseases across multiple levels — from cellular and genetic to brain structure and clinical phenotypes — to drive advancements in precision medicine. To achieve this, she works on research spanning bioinformatics, artificial intelligence, and clinical and behavioural analysis, with the aim to gain expertise that enables her to leverage diverse data types and insights for precision medicine. Specifically, she is focused on developing multimodal foundation models that integrate diverse data types to effectively capture the complexity of diseases while embedding context-aware knowledge. Her long-term research goal is to investigate diseases using multimodal data (e.g., imaging, omics, and electronic health records) to capture diverse disease characteristics and identify precise, clinically applicable treatments."
  },
  {
    "objectID": "people/team/daniel.html",
    "href": "people/team/daniel.html",
    "title": "Daniel Platt",
    "section": "",
    "text": "Daniel is a Chapman-Schmidt fellow at Imperial College London working on differential geometry and machine learning. He obtained his PhD from Imperial College London for his thesis on gauge theory in dimension 7 on problems that are motivated by string theory. Because of this, he is interested in big datasets of Calabi-Yau manifolds. Apart from this, Lie groups and  group actions are important for the kind of geometry that he studies and he is also interested in applications of differential geometry to group invariant machine learning."
  },
  {
    "objectID": "people/team/simone.html",
    "href": "people/team/simone.html",
    "title": "Simone Foti",
    "section": "",
    "text": "Simone is a Postdoctoral Research Associate in the Department of Computing at Imperial College London. Previously, he obtained a PhD at the University College London (UCL) by researching “Latent Disentanglement for the Analysis and Generation of Digital Human Shapes”, which found applications in computer vision, graphics and plastic surgery. During the PhD, he gained industry experience as a Research Scientist Intern at Disney Research Studios (Zurich) and Adobe Research (Paris). Simone also obtained an MRes in Medical Imaging from UCL, an MSc in Biomedical Engineering at Politecnico di Milano, and a BSc in Computer Science at the University of Trieste. Currently, his research interest lies at the intersection of geometric deep learning, computer graphics, and computer vision. Through his research, Simone strives to understand how to solve problems in non-Euclidean domains such as graphs and meshes. He is currently working on multiple projects ranging from shape and texture generation, latent disentanglement, mesh sampling, and 3D reconstruction."
  },
  {
    "objectID": "people/team/arne.html",
    "href": "people/team/arne.html",
    "title": "Arne Wolf",
    "section": "",
    "text": "Arne is doing his PhD at Imperial College London (LSGNT) under the supervision of Anthea Monod. He is working on adapting ideas from geometry and topology to discrete settings. Concretely, he is interested in different tools of topological data analysis like persistent homology and Hodge Laplacians for simplicial complexes and other discrete structures."
  },
  {
    "objectID": "people/advisors/xavier.html",
    "href": "people/advisors/xavier.html",
    "title": "Xavier Bresson",
    "section": "",
    "text": "Home\n    People\n    Xavier Bresson\n\n  \n\n\n  \n \n \n  \n   \n  \n    \n     website\n  \n\n\n\nXavier Bresson (PhD 2005, EPFL, Switzerland) is Associate Professor in Computer Science at NUS, Singapore. He is a leading researcher in the field of Graph Deep Learning, a new framework that combines graph theory and deep learning techniques to tackle complex data domains in natural language processing, computer vision, combinatorial optimization, quantum chemistry, physics, neuroscience, genetics and social networks. In 2016, he received the highly competitive Singaporean NRF Fellowship of $2.5M to develop these deep learning techniques. He was also awarded several research grants in the U.S. and Hong Kong. As a leading researcher in the field, he has published more than 60 peer-reviewed papers in the leading journals and conference proceedings in machine learning, including articles in NeurIPS, ICML, ICLR, CVPR, JMLR. He has organized several international workshops and tutorials on AI and deep learning in collaboration with Facebook, NYU and Imperial such as the 2019 and 2018 UCLA workshops, the 2017 CVPR tutorial and the 2017 NeurIPS tutorial. He has been teaching undergraduate, graduate and industrial courses in AI and deep learning since 2014 at EPFL (Switzerland), NTU (Singapore) and UCLA (U.S.)."
  },
  {
    "objectID": "people/advisors/arnaud.html",
    "href": "people/advisors/arnaud.html",
    "title": "Arnaud Doucet",
    "section": "",
    "text": "Home\n    People\n    Arnaud Doucet\n\n  \n\n\n  \n \n \n  \n   \n  \n    \n     website\n  \n\n\n\nArnaud Doucet is a Professor of Statistics at University of Oxford and a Senior Staff Research Scientist at Google DeepMind. He was an Institute of Mathematical Statistics Medallion (IMS) Lecturer in 2016, was elected IMS Fellow in 2017 and awarded the Guy Silver Medal from the Royal Statistical Society in 2020. His research interests lie in the development and analysis of efficient computational methods for inference and learning, machine learning, signal processing and related areas. Recently, he has been very interested in generative modeling (in particular denoising diffusion models) and computational optimal transport."
  },
  {
    "objectID": "people/advisors/anthea.html",
    "href": "people/advisors/anthea.html",
    "title": "Anthea Monod",
    "section": "",
    "text": "Home\n    People\n    Anthea Monod\n\n  \n\n\n  \n \n \n  \n   \n  \n    \n     website\n  \n\n\n\nAnthea Monod is a Senior Lecturer (associate professor) in the Department of Mathematics at Imperial College London. Her research areas primarily include topological data analysis, algebraic statistics, and nonlinear algebra applied to mathematical and computational biology and theoretical machine learning. She is a Co-Director of the “Erlangen Programme” £10M EPSRC-funded Hub for Mathematical and Computational Foundations of Artificial Intelligence. She earned her PhD from the Swiss Federal Institute of Technology in Lausanne (EPFL). Prior to joining Imperial, she held postdoctoral and visiting faculty positions at the Technion–Israel Institute of Technology, Duke University, Columbia University in the City of New York, and Tel Aviv University."
  },
  {
    "objectID": "people.html",
    "href": "people.html",
    "title": "LOGML 2025",
    "section": "",
    "text": "Home\n    People"
  },
  {
    "objectID": "people.html#organising-committee",
    "href": "people.html#organising-committee",
    "title": "LOGML 2025",
    "section": "Organising Committee",
    "text": "Organising Committee\n\n\n\n\n\n\n\n\n\n\nVincenzo Marco De Luca\n\n\nUniversity of Trento\n\n\n\n\n\n\n\n\n\n\n\n\n\nMassimiliano Esposito\n\n\nIBM\n\n\n\n\n\n\n\n\n\n\n\n\n\nSimone Foti\n\n\nImperial College London\n\n\n\n\n\n\n\n\n\n\n\n\n\nValentina Giunchiglia\n\n\nImperial College London, Harvard University\n\n\n\n\n\n\n\n\n\n\n\n\n\nDaniel Platt\n\n\nImperial College London\n\n\n\n\n\n\n\n\n\n\n\n\n\nPragya Singh\n\n\nImperial College London\n\n\n\n\n\n\n\n\n\n\n\n\n\nRahul Singh\n\n\nYale University\n\n\n\n\n\n\n\n\n\n\n\n\n\nArne Wolf\n\n\nImperial College London\n\n\n\n\n\n\n\n\n\n\n\n\n\nZhengang Zhong\n\n\nUniversity of Warwick\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "people.html#scientific-advisory-committee",
    "href": "people.html#scientific-advisory-committee",
    "title": "LOGML 2025",
    "section": "Scientific Advisory Committee",
    "text": "Scientific Advisory Committee\n\n\n\n\n\n\n\n\n\n\nXavier Bresson\n\n\nNational University of Singapore\n\n\n\n\n\n\n\n\n\n\n\n\n\nMichael Bronstein\n\n\nUniversity of Oxford\n\n\n\n\n\n\n\n\n\n\n\n\n\nArnaud Doucet\n\n\nUniversity of Oxford, Google DeepMind\n\n\n\n\n\n\n\n\n\n\n\n\n\nHeather Harrington\n\n\nMPI-CBG, TU Dresden\n\n\n\n\n\n\n\n\n\n\n\n\n\nJure Leskovec\n\n\nStanford University\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnthea Monod\n\n\nImperial College London\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "policies.html",
    "href": "policies.html",
    "title": "Policies",
    "section": "",
    "text": "The Organisers acknowledge that the workforce in STEM fields (including mathematics and computer science) does not reflect the diversity of the general population, and the importance of role models in encouraging talents to pursue careers in STEM regardless of their background. ​ This document outlines the steps the Organising Committee is taking in order to make the event accessible and welcoming to a diverse crowd, independently of ethnicity, gender, sexual orientation, religion, or disability.\nWe encourage anyone with concerns regarding accessibility, diversity, or inclusion to contact the Organising Committee.\nNote: this document is not final and will be subject to changes by the time of the event."
  },
  {
    "objectID": "policies.html#diversity-and-inclusion-policy",
    "href": "policies.html#diversity-and-inclusion-policy",
    "title": "Policies",
    "section": "",
    "text": "The Organisers acknowledge that the workforce in STEM fields (including mathematics and computer science) does not reflect the diversity of the general population, and the importance of role models in encouraging talents to pursue careers in STEM regardless of their background. ​ This document outlines the steps the Organising Committee is taking in order to make the event accessible and welcoming to a diverse crowd, independently of ethnicity, gender, sexual orientation, religion, or disability.\nWe encourage anyone with concerns regarding accessibility, diversity, or inclusion to contact the Organising Committee.\nNote: this document is not final and will be subject to changes by the time of the event."
  },
  {
    "objectID": "policies.html#speaker-and-mentors",
    "href": "policies.html#speaker-and-mentors",
    "title": "Policies",
    "section": "Speaker and mentors",
    "text": "Speaker and mentors\n​ The Organisers are committed to inviting speakers and mentors representative of all people. This includes ensuring a diverse representation of research subjects and fields, gender, and ethnic backgrounds among our speakers and mentors.\nIn particular, we attempt to counteract the underrepresentation of women in STEM by making sure that at least 20% of speakers and mentors are non-male. We also strongly encourage applications by non-men and minorities and will reserve 20% of the participant spots for underrepresented groups.\nThe Organisers will, to the best of their abilities, ensure traditionally under-represented groups in AI and STEM are aware of the event and encouraged to apply, e.g., by reaching out to university and department diversity officers."
  },
  {
    "objectID": "policies.html#disabilities",
    "href": "policies.html#disabilities",
    "title": "Policies",
    "section": "Disabilities",
    "text": "Disabilities\n​ The Organisers are committed to making the event accessible to people with disabilities. We will strive to provide accessible versions of the content of the summer school (e.g. plain-text pages with links to the videoconference meetings). We also have funding available for individualised assistance.\nPlease reach out to the organisers if you require help with accessibility."
  },
  {
    "objectID": "policies.html#caring-responsibilities",
    "href": "policies.html#caring-responsibilities",
    "title": "Policies",
    "section": "Caring responsibilities",
    "text": "Caring responsibilities\n​ Financial support will be available to relieve participants, mentors or speakers of caring responsibilities that might interfere with their participation in the Summer School. Applicants can state in the application form that they request financial assistance to cover caring expenses in order to attend the event. Attendance will also be provided for free to legal carers of participants if requested in the application form.\nPlease reach out to the organisers if you require help with caring responsibilities."
  },
  {
    "objectID": "policies.html#financial-support",
    "href": "policies.html#financial-support",
    "title": "Policies",
    "section": "Financial support",
    "text": "Financial support\n​ Limited financial support is available. Preference will be given to applicants who cannot obtain financial assistance by other means (e.g. from their university, department, or advisors). Applicants will have the possibility to apply for support in the application form. ​"
  },
  {
    "objectID": "policies.html#code-of-conduct",
    "href": "policies.html#code-of-conduct",
    "title": "Policies",
    "section": "Code of Conduct",
    "text": "Code of Conduct\nThis document states the Code of Conduct of the LOGML 2025 Workshop (hereafter referred to as “the Event”).\nNote: This document is not final and will be subject to changes by the time of the event.\n\nIntroduction\n​ The Organisers are committed to making this conference productive and enjoyable for everyone, regardless of sex, sexual orientation, disability, age, physical appearance, body size, ethnicity, nationality, or religion. For this reason, we will not tolerate harassment of participants in any form, and will implement policies to promote an inclusive environment and support vectors for attendees who require assistance to participate in the workshop.\nAs part of the registration process, all attendees (i.e. all participants, mentors, speakers, any volunteer helpers, and any other person attending the event) are required to agree to adhere to this Code of Conduct. In particular, Sponsors are equally subject to this Code of Conduct, and may not use images, activities, or any other materials that are of sexual, racial, or otherwise offensive nature. This code applies both to official Sponsors as well as any organisation that uses the Workshop name, image, or identity as part of its activities at or around the Workshop.\n\n\nCode of Conduct\n​ Attendees are required to behave professionally. Harassment and sexist, racist, or exclusionary comments or jokes and any behaviour pertaining to bullying are not appropriate. Compliance with these principles and, in particular, with the present Code of Conduct is expected not just for virtual meetings, but also for interactions on social media.\nHarassment includes sustained disruption of talks or other events, sustained unwanted contact, sexual attention or innuendo, deliberate intimidation, pressuring, stalking, and photography or recording of an individual without consent. It also includes offensive or belittling comments related to gender, sexual orientation, disability, age, physical appearance, body size, ethnicity, or religion.\nAll communication should be appropriate for a professional audience including people of many different backgrounds. Sexual language and imagery are not appropriate."
  },
  {
    "objectID": "policies.html#incident-reporting-and-resolution",
    "href": "policies.html#incident-reporting-and-resolution",
    "title": "Policies",
    "section": "Incident reporting and resolution",
    "text": "Incident reporting and resolution\n​ If you observe someone making you or anyone else feel unsafe or unwelcome, please tell them so, and remind them of the Code of Conduct. Non-confrontational alternatives can be to create a distraction, or to refer the person to an Organiser.\nWhether or not you addressed the person yourself, report incidents as soon as possible to a member of the Organising Committee - see contact information below. The Committee is committed to addressing and resolving the matter to the best of their abilities and within the best possible delay. We are prepared to help participants in contacting relevant help services, and to escort them to a safe location.\nPlease use the following contact information, and explain what happened and who was involved so that we can investigate: logml.committee@gmail.com.\nThe phone number of the point of contact will be provided to all attendees as part of the electronic materials during the Event. ​ ## Sanctions ​ When someone is asked to stop any behaviour that violates the Code of Conduct, they are expected to comply immediately. In response to behaviour deemed inappropriate by the Organising Committee (e.g. sexual content, rudeness, unprofessional), the Organisers may take any action they deem appropriate, including warning the person in question, asking them to leave or banning them from the event, or removing them from a mailing list.\nSpecific actions may include but are not limited to:\n\nasking the person to cease the inappropriate behaviour, and warning them that any further reports will result in other sanctions\nrequiring the person makes conciliatory efforts that may include an apology, informal mediation, or other steps intended to facilitate restoration of relationships\nrequiring the person avoids any interaction with another person for the remainder of the event\nearly termination of a talk that violates the policy\nnot publishing the video or slides of a talk that violates the policy\nnot allowing a speaker who violated the policy to give (further) talks at the event\nimmediately ending any event responsibilities or privileges held\nrequiring that the person immediately leave the event and not return\nblocking the person on social media platforms\nbanning the person from future events\npublishing an anonymous account of the harassment\nreporting the incident to the person’s employer\nreporting the incident to competent authorities\n\nThe Organisers reserve the right to remove participants who fail to comply with the Code of Conduct if the situation requires it. The Organisers may decline to refund any cost incurred with attendance (including, e.g., registration fee) to participants who have been expelled for breaching the Code of Conduct.\nThank you for your participation in the LOGML community, and your efforts to keep our conference welcoming, respectful, and friendly for all participants!"
  },
  {
    "objectID": "policies.html#data-protection-statement",
    "href": "policies.html#data-protection-statement",
    "title": "Policies",
    "section": "Data Protection Statement",
    "text": "Data Protection Statement\nWho is collecting your data?\nThe London Geometry and Machine Learning Organisation Committee (LOGML) is collecting your data. The individual members of our initiative are listed on the “Organisers” page. Submitted data are controlled by LOGML. Data is shared with third parties, if necessary, for the organisation of the workshop. In particular, contact data of successful applicants is shared with project mentors, and participants’ CVs are shared with sponsors if the participant has opted-in for this during the application process.\nWhy will we be using your personal data?\nWe are using your personal data to make admissions decision for the summer school and to communicate with you about the summer school, for example to share access information or schedule changes. We share your contact information with the mentor of the project you work on during the workshop in order to enable them to contact you about matters regarding the project you will participate in.\nWhat are the categories of personal data concerned?\nWe only collect personal data when you share them directly with us through the application form, personally, via email. The categories of personal data concerned is the category of data that you are sharing directly with us.\nWhat is the legal justification for processing your data?\nWe are providing a task in the public interest linked to the core purpose “Education”.\nFor how long will we keep your data?\nWe will keep data of both successful and unsuccessful applicants for six months after the workshop.\nWho else might receive your data?\nIf you explicitly advise us to do this during the application process, we will share your CV with our sponsors who process your data according to their respective privacy policies. Our sponsors are listed on the “Sponsors” page of our website."
  },
  {
    "objectID": "policies.html#information-about-your-rights",
    "href": "policies.html#information-about-your-rights",
    "title": "Policies",
    "section": "Information about your rights",
    "text": "Information about your rights\nUnder certain circumstances, you may have the following rights in relation to your personal data:\nRight 1: A right to access personal data held by us about you.\nRight 2: A right to require us to rectify any inaccurate personal data held by us about you.\nRight 3: A right to require us to erase personal data held by us about you. This right will only apply where, for example, we no longer need to use the personal data to achieve the purpose we collected it for; or where you withdraw your consent if we are using your personal data based on your consent; or where you object to the way we process your data (in line with Right 6 below).\nRight 4: A right to restrict our processing of personal data held by us about you. This right will only apply where, for example, you dispute the accuracy of the personal data held by us; or where you would have the right to require us to erase the personal data but would prefer that our processing is restricted instead; or where we no longer need to use the personal data to achieve the purpose we collected it for, but we require the data for the purposes of dealing with legal claims.\nRight 5: A right to receive personal data, which you have provided to us, in a structured, commonly used and machine readable format. You also have the right to require us to transfer this personal data to another organisation.\nRight 6: A right to object to our processing of personal data held by us about you.\nRight 7: A right to withdraw your consent, where we are relying on it to use your personal data.\nRight 8: A right to ask us not to use information about you in a way that allows computers to make decisions about you and ask us to stop.\nIf you want to exercise these rights, please contact us at logml.committee@gmail.com.\nYou can read more about your rights in the Guidance from the UK Information Commissioner’s Office (ICO). You also have the right to make a complaint to the ICO about how we use your personal data. You can do this by contacting the ICO via their website or by calling 0303 123 1113."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Beyond Text: Exploring Adaptations of LLMs for Graph-Based Tasks\n\n\n\n\n\n\n\n\n\n\n\nFabrizio Frasca\n\n\n\n\n\n\n\n\n\n\n\n\nBeyond VC Dimension - Rademacher Complexity for GNN Generalization\n\n\n\n\n\n\n\n\n\n\n\nCaterina Graziani\n\n\n\n\n\n\n\n\n\n\n\n\nCycle Matching for High-Dimensional Neural Activation Patterns\n\n\n\n\n\n\n\n\n\n\n\nAnthea Monod, Omer Bobrowski\n\n\n\n\n\n\n\n\n\n\n\n\nFairness-Aware GraphRAG for Trustworthy and Equitable Document Retrieval\n\n\n\n\n\n\n\n\n\n\n\nGuadalupe Gonzalez, Chirag Agarwal\n\n\n\n\n\n\n\n\n\n\n\n\nFinite groups and the Cayley graph representation, such that ML can then help identify symmetry from generators\n\n\n\n\n\n\n\n\n\n\n\nEdward Hirst\n\n\n\n\n\n\n\n\n\n\n\n\nGraph Transformers for Relational Deep Learning\n\n\n\n\n\n\n\n\n\n\n\nVijay Prakash Dwivedi\n\n\n\n\n\n\n\n\n\n\n\n\nInvestigating Emergent Invariance and Sampling Thresholds in Hopfield Networks on Graph Orbit Datasets\n\n\n\n\n\n\n\n\n\n\n\nMichael Murray\n\n\n\n\n\n\n\n\n\n\n\n\nIterative Reasoning in Graph Neural Networks for Drug Repurposing\n\n\n\n\n\n\n\n\n\n\n\nYasha Ektefaie\n\n\n\n\n\n\n\n\n\n\n\n\nLooking for Einstein Metrics with Machine Learning\n\n\n\n\n\n\n\n\n\n\n\nTancredi Schettini Gherardini\n\n\n\n\n\n\n\n\n\n\n\n\nOn Depth in Geometric Deep Learning: Scaling Up Biomolecular Analysis Using Deep Neural k-Forms\n\n\n\n\n\n\n\n\n\n\n\nKelly Maggs\n\n\n\n\n\n\n\n\n\n\n\n\nPolyhedral Complex Extraction from ReLU Networks\n\n\n\n\n\n\n\n\n\n\n\nArturs Berzins\n\n\n\n\n\n\n\n\n\n\n\n\nRepresentation learning and knowledge encoding with biomedical knowledge graphs\n\n\n\n\n\n\n\n\n\n\n\nRuth Johnson\n\n\n\n\n\n\n\n\n\n\n\n\nRepresentational Alignment for Universal Spaces\n\n\n\n\n\n\n\n\n\n\n\nDonato Crisostomi\n\n\n\n\n\n\n\n\n\n\n\n\nRiemannian deep reinforcement learning for PDE-constrained shape optimisation\n\n\n\n\n\n\n\n\n\n\n\nEstefania Loayza Romero\n\n\n\n\n\n\n\n\n\n\n\n\nSymmetry, degeneracy and effective dimensions of neural networks\n\n\n\n\n\n\n\n\n\n\n\nJiayi Li\n\n\n\n\n\n\n\n\n\n\n\n\nTopological Machine Learning for Brain Dynamics\n\n\n\n\n\n\n\n\n\n\n\nDhananjay Bhaskar\n\n\n\n\n\n\n\n\n\n\n\n\nTopological data analysis (TDA) to elucidate protein functions via variant landscapes\n\n\n\n\n\n\n\n\n\n\n\nOwen Queen\n\n\n\n\n\n\n\n\n\n\n\n\nTowards a More Rigorous Evaluation of Hyperbolic Graph Representation Learning\n\n\n\n\n\n\n\n\n\n\n\nVeronica Lachi\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/Arturs-Berzins.html",
    "href": "projects/Arturs-Berzins.html",
    "title": "Polyhedral Complex Extraction from ReLU Networks",
    "section": "",
    "text": "Arturs Berzins is a postdoctoral researcher at the Institute for Machine Learning at JKU Linz. Arturs research is focused on the intersection of machine learning, geometry, and simulation. He received a PhD in mathematics from the University of Oslo for the thesis on Neural Representations in Geometry. His PhD was hosted by the Geometry group at SINTEF as a fellow of GRAPES — a Marie Sklodowska-Curie ITN on Learning, processing and optimising shapes. Previously, Arturs received his BSc and MSc in mechanical engineering from RWTH Aachen specializing in simulation technology."
  },
  {
    "objectID": "projects/Arturs-Berzins.html#arturs-berzins",
    "href": "projects/Arturs-Berzins.html#arturs-berzins",
    "title": "Polyhedral Complex Extraction from ReLU Networks",
    "section": "",
    "text": "Arturs Berzins is a postdoctoral researcher at the Institute for Machine Learning at JKU Linz. Arturs research is focused on the intersection of machine learning, geometry, and simulation. He received a PhD in mathematics from the University of Oslo for the thesis on Neural Representations in Geometry. His PhD was hosted by the Geometry group at SINTEF as a fellow of GRAPES — a Marie Sklodowska-Curie ITN on Learning, processing and optimising shapes. Previously, Arturs received his BSc and MSc in mechanical engineering from RWTH Aachen specializing in simulation technology."
  },
  {
    "objectID": "projects/Arturs-Berzins.html#project",
    "href": "projects/Arturs-Berzins.html#project",
    "title": "Polyhedral Complex Extraction from ReLU Networks",
    "section": "Project",
    "text": "Project\nA neural network consisting of piecewise affine building blocks, such as fully-connected layers and ReLU activations, is itself a piecewise affine function supported on a polyhedral complex. This complex is used to study the theoretical properties of neural networks and in applied geometry, but, in practice, extracting it has been challenging due to its high combinatorial complexity. A natural idea described in previous works is subdividing the regions via intersections with hyperplanes induced by each neuron. However, [1] argues that subdividing regions leads to computational redundancy and proposes to subdivide edges instead. The resulting “edge subdivision” method uses standard GPU accelerated tensor operations and improves the speed over previous methods by an order of magnitude.\nSeveral avenues remain to improve edge subdivision further, including the generalization to unbounded domains, non-generic arrangements, and other piecewise affine architectures, as well as improving pruning strategies for more efficient extraction of level-sets. The integration of hashing in a specific sub-routine could potentially improve the performance significantly. This improved performance and the differentiability of the method would allow us to consider new use-cases after or during the training, such as for adversarial robustness, grokking, shape representations, or geometric optimization.\n[1] Berzins, Arturs. “Polyhedral complex extraction from ReLU networks using edge subdivision.” International Conference on Machine Learning. PMLR, 2023."
  },
  {
    "objectID": "projects/Donato-Crisostomi.html",
    "href": "projects/Donato-Crisostomi.html",
    "title": "Representational Alignment for Universal Spaces",
    "section": "",
    "text": "Donato Crisostomi is a researcher at the Sapienza University of Rome, focusing on model merging and representational alignment. He currently leads the “Model Reuse” work package for the 1.5M€ project “NEXUS: Interoperable Machine Learning with Universal Representations”. He previously held roles as a visiting researcher at the University of Cambridge, a Research Scientist at Amazon Alexa, and an Applied Scientist at Amazon Search. His research has been featured in top-tier AI conferences and journals, including CVPR, NeurIPS, ACM, ACL, and LoG. In addition to his scientific contributions, he has played an active role in the research community as the organizer of the UniReps workshop at NeurIPS and as a program committee member for leading conferences such as CVPR, NeurIPS, ICLR, etc."
  },
  {
    "objectID": "projects/Donato-Crisostomi.html#donato-crisostomi",
    "href": "projects/Donato-Crisostomi.html#donato-crisostomi",
    "title": "Representational Alignment for Universal Spaces",
    "section": "",
    "text": "Donato Crisostomi is a researcher at the Sapienza University of Rome, focusing on model merging and representational alignment. He currently leads the “Model Reuse” work package for the 1.5M€ project “NEXUS: Interoperable Machine Learning with Universal Representations”. He previously held roles as a visiting researcher at the University of Cambridge, a Research Scientist at Amazon Alexa, and an Applied Scientist at Amazon Search. His research has been featured in top-tier AI conferences and journals, including CVPR, NeurIPS, ACM, ACL, and LoG. In addition to his scientific contributions, he has played an active role in the research community as the organizer of the UniReps workshop at NeurIPS and as a program committee member for leading conferences such as CVPR, NeurIPS, ICLR, etc."
  },
  {
    "objectID": "projects/Donato-Crisostomi.html#project",
    "href": "projects/Donato-Crisostomi.html#project",
    "title": "Representational Alignment for Universal Spaces",
    "section": "Project",
    "text": "Project\nMachine learning models learn internal representations that capture task-relevant features, commonly believed to be potentially very different from each other. Yet recent research suggests that, despite their apparent difference, these learned embeddings might often be transformations of one another—sometimes via simple orthogonal mappings—thereby maintaining inter-sample relationships [1, 2]. Such observations hint at the existence of universal representation spaces, large “embeddings hubs” that multiple models can map to while preserving the semantics of their original learned features.\nIn this project, we plan to investigate whether—and to what extent—universal representation spaces can be identified by leveraging cycle-consistent alignment [3]. Concretely, given several trained models, we will explore how to align their latent representations in a way that is robust to differences in architecture, training conditions and possibly even modalities. By enforcing cycle consistency, every model’s embedding can be mapped to a shared “universe” and back again without drifting away from its original representation. This property makes it possible to preserve the local geometric and semantic relationships that each model has learned.\nOur research will revolve around three high-level steps. First, we will extract intermediate-layer embeddings from each model and define a consistent strategy for sampling representative data points. Second, we will develop a cycle-consistent alignment procedure that optimizes mapping functions from one model to another by passing through a universal space. Finally, we will evaluate whether the resulting universal space meaningfully captures class separability, domain invariance, or other useful properties across multiple models.\nBy the end of the program, we aim to have a prototype that tests this idea on a modest set of models and datasets. If the results confirm that cycle-consistent transformations do indeed create a faithful universal embedding space, this research is likely to be submitted to a top-tier machine learning conference, as well as open up new avenues for model merging and multi-modal learning.\n[1] Moschella, L., Maiorca, V., Fumero, M., Norelli, A., Locatello, F., & Rodolà, E. Relative representations enable zero-shot latent space communication. In ICLR 2023\n[2] Huh, M., Cheung, B., Wang, T., & Isola, P. (2024). The platonic representation hypothesis. ICML Position Paper Track 2024.\n[3] Crisostomi, D., Fumero, M., Baieri, D., Bernard, F., & Rodolà, E. (2024). Cycle-Consistent Multi-Model Merging. In NeurIPS 2024"
  },
  {
    "objectID": "projects/Ruth-Johnson.html",
    "href": "projects/Ruth-Johnson.html",
    "title": "Representation learning and knowledge encoding with biomedical knowledge graphs",
    "section": "",
    "text": "Ruth Johnson is a Berkowitz Family Living Laboratory Postdoctoral Research Fellow at Harvard Medical School advised by Marinka Zitnik. She received her PhD from the University of California, Los Angeles (UCLA) in computer science and bachelor’s degree from UCLA in mathematics. Her current research focuses on developing patient-level foundation models from large-scale electronic health record (EHR) systems in collaboration with the Clalit Health Services in Israel. She is also interested in the intersection of genomics and EHR for studying rare genetic disorders. Previously, she was actively involved in the UCLA ATLAS Precision Health Biobank and focused on developing statistical genetics methods, specifically studying genetic disease risk in ancestrally diverse populations."
  },
  {
    "objectID": "projects/Ruth-Johnson.html#ruth-johnson",
    "href": "projects/Ruth-Johnson.html#ruth-johnson",
    "title": "Representation learning and knowledge encoding with biomedical knowledge graphs",
    "section": "",
    "text": "Ruth Johnson is a Berkowitz Family Living Laboratory Postdoctoral Research Fellow at Harvard Medical School advised by Marinka Zitnik. She received her PhD from the University of California, Los Angeles (UCLA) in computer science and bachelor’s degree from UCLA in mathematics. Her current research focuses on developing patient-level foundation models from large-scale electronic health record (EHR) systems in collaboration with the Clalit Health Services in Israel. She is also interested in the intersection of genomics and EHR for studying rare genetic disorders. Previously, she was actively involved in the UCLA ATLAS Precision Health Biobank and focused on developing statistical genetics methods, specifically studying genetic disease risk in ancestrally diverse populations."
  },
  {
    "objectID": "projects/Ruth-Johnson.html#project",
    "href": "projects/Ruth-Johnson.html#project",
    "title": "Representation learning and knowledge encoding with biomedical knowledge graphs",
    "section": "Project",
    "text": "Project\nKnowledge graphs (KGs) capture highly complex, real-world information. Integrating diverse biomedical knowledgebases (e.g. drugs, genes) into a KG provides a unified framework of biomedical knowledge that can be integrated into downstream prediction tasks. However, encoding this information into high-quality mathematical representations is non-trivial given the scale, complexity, and breadth of information. Framed as a representation learning problem, we will leverage ideas from self-supervised learning such as contrastive learning and Information Noise-Contrastive Estimation (InfoNCE) to learn embeddings of the KG. We will explore methods for performing large-scale inference with graph neural networks and discuss practical tips for real-world implementation.\nWe’ll demonstrate an application of KG embeddings through token injection into open-source large language models (LLMs). By augmenting pre-trained LLMs with the learned embeddings in the form of ‘knowledge tokens’, we can achieve superior performance in biomedical question-answering tasks. This project can be broken down into 3 main components:\n\nKnowledge graphs: Intro to KGs and constructing biomedical KGs from scratch.\nRepresentation learning on graphs: Self-supervised learning on graphs with GNNs and tips for scaling inference to very large (100K+ node) KGs.\nAugmenting LLMs with knowledge tokens: Integrating biomedical knowledge into pre-trained LLMs in the form of new tokens and parameter-efficient fine-tuning."
  },
  {
    "objectID": "projects/Guadalupe-Gonzalez-and-Chirag-Agarwal.html",
    "href": "projects/Guadalupe-Gonzalez-and-Chirag-Agarwal.html",
    "title": "Fairness-Aware GraphRAG for Trustworthy and Equitable Document Retrieval",
    "section": "",
    "text": "Guadalupe Gonzalez joined Genentech/Roche in February 2023, after completing a PhD on graph deep learning for drug discovery advised by Michael Bronstein and Kirill Veselkov at Imperial College London. She is part of the Frontier Research team at Prescient Design in Genentech Research and Early Development (gRED). Guadalupe’s expertise lies at the intersection of graph deep learning and causal inference. Her focus is on (causal) graph deep learning for drug discovery, from the small-scale (e.g., proteins) to the large-scale (e.g., patient data) systems. Guadalupe is particularly passionate about applying her knowledge to women’s health to catalyze breakthroughs in women-specific conditions such as endometriosis."
  },
  {
    "objectID": "projects/Guadalupe-Gonzalez-and-Chirag-Agarwal.html#guadalupe-gonzalez",
    "href": "projects/Guadalupe-Gonzalez-and-Chirag-Agarwal.html#guadalupe-gonzalez",
    "title": "Fairness-Aware GraphRAG for Trustworthy and Equitable Document Retrieval",
    "section": "",
    "text": "Guadalupe Gonzalez joined Genentech/Roche in February 2023, after completing a PhD on graph deep learning for drug discovery advised by Michael Bronstein and Kirill Veselkov at Imperial College London. She is part of the Frontier Research team at Prescient Design in Genentech Research and Early Development (gRED). Guadalupe’s expertise lies at the intersection of graph deep learning and causal inference. Her focus is on (causal) graph deep learning for drug discovery, from the small-scale (e.g., proteins) to the large-scale (e.g., patient data) systems. Guadalupe is particularly passionate about applying her knowledge to women’s health to catalyze breakthroughs in women-specific conditions such as endometriosis."
  },
  {
    "objectID": "projects/Guadalupe-Gonzalez-and-Chirag-Agarwal.html#chirag-agarwal",
    "href": "projects/Guadalupe-Gonzalez-and-Chirag-Agarwal.html#chirag-agarwal",
    "title": "Fairness-Aware GraphRAG for Trustworthy and Equitable Document Retrieval",
    "section": "Chirag Agarwal",
    "text": "Chirag Agarwal\nChirag Agarwal is an Assistant Professor at the University of Virginia with appointments in the Data Science School and the Department of Computer Science. Dr. Agarwal researches on developing Scalable Trustworthy Machine Learning Frameworks that go beyond training models for specific downstream tasks and satisfy trustworthy properties, such as explainability, fairness, and robustness. He has authored in top-tier machine learning and computer vision conferences and leading scientific journals. His research has received Spotlight and Oral presentations at NeurIPS, ICML, CVPR, and ICIP, and received industrial grants from Adobe, Microsoft, and Google to support his work on Trustworthy Machine Learning."
  },
  {
    "objectID": "projects/Guadalupe-Gonzalez-and-Chirag-Agarwal.html#project",
    "href": "projects/Guadalupe-Gonzalez-and-Chirag-Agarwal.html#project",
    "title": "Fairness-Aware GraphRAG for Trustworthy and Equitable Document Retrieval",
    "section": "Project",
    "text": "Project\nGraphRAG (Graph-based Retrieval-Augmented Generation) is a framework that enhances retrieval-augmented generation (RAG) by using graph structures to improve document retrieval and knowledge integration in large language models (LLMs) [1]. Unlike traditional RAG, which retrieves documents based on embedding similarity, GraphRAG organizes and retrieves information using a structured graph representation, allowing for more context-aware, interpretable, and interconnected document retrieval.\nTraditional retrieval techniques in GraphRAG often prioritize embedding-based similarity or graph connectivity, which can amplify biases, underrepresent marginalized perspectives, or reinforce existing disparities. This project aims to develop a fairness-aware ranking algorithm for GraphRAG, ensuring that the document retrieval process across various domains, including healthcare, social sciences, and public policy, is trustworthy, representative, and unbiased. By modifying graph-based ranking algorithms, we will incorporate fairness constraints to improve document selection in GraphRAG. The resulting fairness of GraphRAG will be evaluated in both the documents retrieved by the retrieval algorithms [2] and the responses generated by the LLM [3].\n[1] Zhang, Q. et al. A Survey of Graph Retrieval-Augmented Generation for Customized Large Language Models. arXiv (2025) doi:10.48550/arxiv.2501.13958.\n[2] Dong, Y., Ma, J., Wang, S., Chen, C. & Li, J. Fairness in Graph Mining: A Survey. arXiv (2022) doi:10.48550/arxiv.2204.09888.\n[3] Gallegos, I. O. et al. Bias and Fairness in Large Language Models: A Survey. arXiv (2023) doi:10.48550/arxiv.2309.00770."
  },
  {
    "objectID": "projects/Fabrizio-Frasca.html",
    "href": "projects/Fabrizio-Frasca.html",
    "title": "Beyond Text: Exploring Adaptations of LLMs for Graph-Based Tasks",
    "section": "",
    "text": "Fabrizio Frasca is a Postdoctoral Fellow at Technion, working with Prof. Haggai Maron on Geometric Deep Learning, with a focus on equivariance, expressiveness, and Graph Neural Networks. He co-organizes GLOW (Graph Learning on Wednesdays), a reading group on graph learning.\nFabrizio holds a PhD in Computing from Imperial College London, where he worked with Prof. Michael Bronstein on addressing the representational limitations of Graph Neural Networks, exploring meso-scale topology and equivariance as design principles for expressive architectures.\nPreviously, he was a Machine Learning Researcher at Twitter Cortex (2019–2023, following the acquisition of Fabula AI). His past research includes applied Machine Learning in Computational Biology, particularly drug repurposing and epigenetic gene expression regulation."
  },
  {
    "objectID": "projects/Fabrizio-Frasca.html#fabrizio-frasca",
    "href": "projects/Fabrizio-Frasca.html#fabrizio-frasca",
    "title": "Beyond Text: Exploring Adaptations of LLMs for Graph-Based Tasks",
    "section": "",
    "text": "Fabrizio Frasca is a Postdoctoral Fellow at Technion, working with Prof. Haggai Maron on Geometric Deep Learning, with a focus on equivariance, expressiveness, and Graph Neural Networks. He co-organizes GLOW (Graph Learning on Wednesdays), a reading group on graph learning.\nFabrizio holds a PhD in Computing from Imperial College London, where he worked with Prof. Michael Bronstein on addressing the representational limitations of Graph Neural Networks, exploring meso-scale topology and equivariance as design principles for expressive architectures.\nPreviously, he was a Machine Learning Researcher at Twitter Cortex (2019–2023, following the acquisition of Fabula AI). His past research includes applied Machine Learning in Computational Biology, particularly drug repurposing and epigenetic gene expression regulation."
  },
  {
    "objectID": "projects/Fabrizio-Frasca.html#project",
    "href": "projects/Fabrizio-Frasca.html#project",
    "title": "Beyond Text: Exploring Adaptations of LLMs for Graph-Based Tasks",
    "section": "Project",
    "text": "Project\nLarge Language Models (LLMs) exhibit strong reasoning and instruction-following abilities on textual data, but how well can they process structured inputs like graphs?\nThis question is both practically relevant and theoretically intriguing. Graphs flexibly model relational systems, structured objects and knowledge – LLMs that natively and effectively handle them would constitute powerful tools for modular and nuanced reasoning thereon. Via natural language, users could guide graph-based analyses by providing in-context examples or by breaking down complex tasks into substeps, while grounding Retrieval-Augmented Generation with relational context. Studying LLMs on graphs also intersects with tackling computationally hard problems, additionally shedding light on their ability to solve challenging mathematical tasks.\nEarly research has explored LLMs for graphs, but training-free prompting schemes have shown weak performance [1]. Hybrid approaches incorporating Graph Neural Networks (GNNs) [2,3] offer improvements but either process graphs in a prompt-agnostic way or propose sophisticated architectural modifications and are specific to text-attributed structures. An open question remains under-explored: “Can LLMs themselves be effectively instructed to process relational data?”. This appears as a plausible possibility given the theoretical expressiveness of their architectural backbone, the massive size of their pretraining corpora, and preliminary, encouraging results [4,5], suggesting, inter-alia, that fine-tuned LLMs can achieve competitive performance compared to Transformers trained from scratch.\nThe goal of this project is to provide a more systematic analysis on the adaptation of pretrained LLMs for graph tasks, aiming to study whether and how the pre-acquired abilities of LLMs can apply to structured objects with parameter-efficient fine-tuning and no sophisticated architectural changes. On these fine-tuned LLMs, we will investigate aspects related to expressiveness, permutation invariance, skill transfer and sample efficiency. We will explore whether the architectural biases of LLMs give them an expressiveness edge over standard message-passing GNNs. We will also analyse their sensitivity to node orderings – i.e., the extent to which they respect the main graph symmetry – and study the impact of the LLM architectural backbones. We will finally assess their robustness to input distributions and explore their ability to tackle unseen tasks with appropriate prompting.\n[1] “Talk Like a Graph: Encoding Graphs for Large Language Models”, Fatemi et al., 2023\n[2] “Let Your Graph Do the Talking: Encoding Structured Data for LLMs”, Perozzi et al., 2024\n[3] “GL-Fusion: Rethinking The Combination Of GNN and LLM”, Yan et al., 2024\n[4] “Understanding Transformer Reasoning Capabilities via Graph Algorithms”, Sanford et al., 2024\n[5] “G-Retriever: Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering”, He et al., 2024"
  },
  {
    "objectID": "projects/Vijay-Prakash-Dwivedi.html",
    "href": "projects/Vijay-Prakash-Dwivedi.html",
    "title": "Graph Transformers for Relational Deep Learning",
    "section": "",
    "text": "Vijay Prakash Dwivedi is a Postdoctoral Scholar in Computer Science at Stanford University working on graph representation learning. He holds a PhD from Nanyang Technological University (NTU), Singapore. His work has made contributions to advancing benchmarks for Graph Neural Networks (GNNs), graph positional and structural encodings, and Graph Transformers as universal deep neural networks for graph-based learning. He has also contributed to the integration of parametric knowledge in large language models (LLMs) for diverse applications, particularly in healthcare. Several of the methods he developed during his PhD are now widely adopted in state-of-the-art Graph Transformers and other leading graph learning models. For his research, he received one of the Outstanding PhD Thesis Awards from the NTU College of Computing and Data Science. Vijay has over 7 years experience in both academia and industry with institutions including NTU, Snap Inc., Sony, and ASUS."
  },
  {
    "objectID": "projects/Vijay-Prakash-Dwivedi.html#vijay-prakash-dwivedi",
    "href": "projects/Vijay-Prakash-Dwivedi.html#vijay-prakash-dwivedi",
    "title": "Graph Transformers for Relational Deep Learning",
    "section": "",
    "text": "Vijay Prakash Dwivedi is a Postdoctoral Scholar in Computer Science at Stanford University working on graph representation learning. He holds a PhD from Nanyang Technological University (NTU), Singapore. His work has made contributions to advancing benchmarks for Graph Neural Networks (GNNs), graph positional and structural encodings, and Graph Transformers as universal deep neural networks for graph-based learning. He has also contributed to the integration of parametric knowledge in large language models (LLMs) for diverse applications, particularly in healthcare. Several of the methods he developed during his PhD are now widely adopted in state-of-the-art Graph Transformers and other leading graph learning models. For his research, he received one of the Outstanding PhD Thesis Awards from the NTU College of Computing and Data Science. Vijay has over 7 years experience in both academia and industry with institutions including NTU, Snap Inc., Sony, and ASUS."
  },
  {
    "objectID": "projects/Vijay-Prakash-Dwivedi.html#project",
    "href": "projects/Vijay-Prakash-Dwivedi.html#project",
    "title": "Graph Transformers for Relational Deep Learning",
    "section": "Project",
    "text": "Project\nRecent advancements in graph learning have extended the Transformer architecture beyond natural language processing to model graph-structured data [1,2]. Broadly, Graph Transformers integrate positional and structural encodings with local and global self-attention mechanisms, yielding state-of-the-art performance on various graph learning tasks. At the same time, relational databases remain the backbone of much of the world’s structured data, where represent entities in a table, and edges represent the connections defined by primary and foreign keys, forming complex schema-dependent graph datasets. Despite the prevalence of such relational entity graphs in enterprise and scientific applications, there has not been much success in developing graph deep learning models that perform well on prediction tasks in relational databases, without relying on traditional feature engineering.\nA critical challenge arises from the fact that the structure of relational entity graphs is intrinsically tied to the database schema, introducing unique characteristics such as temporality, heterogeneity, and complex dependency patterns. General Graph Transformers, designed for arbitrary graphs, typically do not account for these schema-specific features, resulting in models that are less effective in exploiting the relational context. Recent initiatives in relational deep learning [3], including the development of new benchmarks like RelBench [4], underscore both the importance and the difficulty of applying deep learning techniques to relational data, thereby highlighting a clear gap in current methodologies.\nIn this project, we aim to develop a Relational Entity Graph Transformer that explicitly models the distinct characteristics of relational graphs. By designing novel positional and structural encodings that capture the schema-dependent properties of relational data, we aim to enhance existing Transformer architectures to better manage the scale and complexity inherent in relational databases. Our approach will involve adapting and extending current models, followed by evaluation on RelBench benchmark tasks, with the ultimate goal of bridging the gap between general graph learning methods and the specialized demands of relational entity graphs.\n[1] Dwivedi, V.P. and Bresson, X., 2021. A generalization of transformer networks to graphs.\n[2] Rampášek, L., Galkin, M., Dwivedi, V.P., Luu, A.T., Wolf, G. and Beaini, D., 2022. Recipe for a general, powerful, scalable graph transformer.\n[3] Fey, M., Hu, W., Huang, K., Lenssen, J.E., Ranjan, R., Robinson, J., Ying, R., You, J. and Leskovec, J., 2023. Relational deep learning: Graph representation learning on relational databases.\n[4] Robinson J, Ranjan R, Hu W, Huang K, Han J, Dobles A, Fey M, Lenssen JE, Yuan Y, Zhang Z, He X. Relbench: A benchmark for deep learning on relational databases."
  },
  {
    "objectID": "projects/Dhananjay-Bhaskar.html",
    "href": "projects/Dhananjay-Bhaskar.html",
    "title": "Topological Machine Learning for Brain Dynamics",
    "section": "",
    "text": "Dhananjay Bhaskar is a postdoctoral researcher at Yale University and a visiting scholar in Engineering at Brown University. His interdisciplinary research integrates agent-based modeling, geometry, topology, and machine learning to understand in complex biological systems, from molecular interactions to brain activity. He has been recognized with several prestigious fellowships, including the Boehringer Ingelheim Biomedical Data Science Fellowship, the Kavli Institute for Neuroscience Postdoctoral Fellowship, the German Academic Exchange Fellowship for Generative Models in Machine Learning, and the Eric and Wendy Schmidt AI in Human Health Fellowship. Dhananjay earned his Ph.D. in Biomedical Engineering and Master’s in Data Science from Brown University. He completed his undergraduate studies at the University of British Columbia, where he majored in Computer Science and Mathematics. Beyond research, he is an advocate of inclusivity in science, earning the Yale Postdoctoral Association’s Outstanding Contribution Award for advancing diversity and professional development initiatives."
  },
  {
    "objectID": "projects/Dhananjay-Bhaskar.html#dhananjay-bhaskar",
    "href": "projects/Dhananjay-Bhaskar.html#dhananjay-bhaskar",
    "title": "Topological Machine Learning for Brain Dynamics",
    "section": "",
    "text": "Dhananjay Bhaskar is a postdoctoral researcher at Yale University and a visiting scholar in Engineering at Brown University. His interdisciplinary research integrates agent-based modeling, geometry, topology, and machine learning to understand in complex biological systems, from molecular interactions to brain activity. He has been recognized with several prestigious fellowships, including the Boehringer Ingelheim Biomedical Data Science Fellowship, the Kavli Institute for Neuroscience Postdoctoral Fellowship, the German Academic Exchange Fellowship for Generative Models in Machine Learning, and the Eric and Wendy Schmidt AI in Human Health Fellowship. Dhananjay earned his Ph.D. in Biomedical Engineering and Master’s in Data Science from Brown University. He completed his undergraduate studies at the University of British Columbia, where he majored in Computer Science and Mathematics. Beyond research, he is an advocate of inclusivity in science, earning the Yale Postdoctoral Association’s Outstanding Contribution Award for advancing diversity and professional development initiatives."
  },
  {
    "objectID": "projects/Dhananjay-Bhaskar.html#project",
    "href": "projects/Dhananjay-Bhaskar.html#project",
    "title": "Topological Machine Learning for Brain Dynamics",
    "section": "Project",
    "text": "Project\nTransitions between brain states are central to cognition, perception, and disease pathology, yet traditional methods often struggle to capture the intrinsic geometry and topology of brain activity dynamics. This limitation hinders our ability to decode brain activity, diagnose neurological disorders, and design targeted interventions. This project explores the use of topological data analysis (TDA) and machine learning to uncover the hidden structure of brain dynamics by learning topological features that distinguish healthy from aberrant brain activity.\nWe will leverage publicly available datasets, such as those from the Human Connectome Project, CRCNS portal, and OpenNeuro, to analyze resting-state and task-evoked brain activity. Using an end-to-end differentiable framework built with pytorch-topological, we will learn topological invariants - such as persistent homology and path signatures - directly from spatiotemporal brain activity. These features will be learned by training a machine learning model to classify brain states, enabling the discovery of interpretable, topology-driven biomarkers for neurological disorders.\nBy integrating TDA with machine learning, this project aims to bridge the gap between the geometric structure of brain dynamics and their functional implications. The learned topological features are expected to provide novel insights into the organization of brain activity, offering a new lens for understanding brain state transitions in health and disease. This work is anticipated to result in a submission to a machine learning conference workshop, such as TAG-ML or COSYNE, and eventually lead to the publication of a machine learning paper, advancing the field of topological machine learning in neuroscience."
  },
  {
    "objectID": "projects/Yasha-Ektefaie.html",
    "href": "projects/Yasha-Ektefaie.html",
    "title": "Iterative Reasoning in Graph Neural Networks for Drug Repurposing",
    "section": "",
    "text": "Yasha Ektefaie is a PhD student at Harvard Medical School, co-advised by Marinka Zitnik and Maha Farhat. His research focuses on developing and applying machine learning to problems in biology and medicine. His past projects include evaluating model generalizability, designing an AI agent for antibiotic discovery in Tuberculosis, and developing a foundation model for phylogenetic inference using a hybrid state-space transformer architecture and conditional graph diffusion."
  },
  {
    "objectID": "projects/Yasha-Ektefaie.html#yasha-ektefaie",
    "href": "projects/Yasha-Ektefaie.html#yasha-ektefaie",
    "title": "Iterative Reasoning in Graph Neural Networks for Drug Repurposing",
    "section": "",
    "text": "Yasha Ektefaie is a PhD student at Harvard Medical School, co-advised by Marinka Zitnik and Maha Farhat. His research focuses on developing and applying machine learning to problems in biology and medicine. His past projects include evaluating model generalizability, designing an AI agent for antibiotic discovery in Tuberculosis, and developing a foundation model for phylogenetic inference using a hybrid state-space transformer architecture and conditional graph diffusion."
  },
  {
    "objectID": "projects/Yasha-Ektefaie.html#project",
    "href": "projects/Yasha-Ektefaie.html#project",
    "title": "Iterative Reasoning in Graph Neural Networks for Drug Repurposing",
    "section": "Project",
    "text": "Project\nGraph Neural Networks (GNNs) are widely used for relational learning tasks such as link prediction, molecular property prediction, and biomedical knowledge discovery. However, standard GNNs operate in a one-shot inference paradigm, where predictions are made based on a static graph and a given query, without the ability to incorporate new evidence dynamically.\nIn real-world applications, especially in biomedical research, users often have additional context (e.g., experimental data, transcriptomic profiles) that could refine a model’s predictions. However, existing GNN architectures lack a built-in mechanism to revise their outputs based on newly injected information at inference time, without retraining.\nWe propose developing a reasoning GNN that can iteratively refine its predictions by incorporating previous outputs and newly injected data, enabling a more adaptive and context-aware decision-making process. The model will be deployed in the medical domain, specifically for drug repurposing [1], where transcriptomic data [2] will be used to guide and revise predictions of drug-disease associations.\nKey Contributions:\n\nInference-Time Adaptation: Unlike standard GNNs that generate static predictions, our model will dynamically adjust its outputs given new information.\nMemory-Augmented or Iterative Reasoning Framework: The reasoning mechanism could be implemented using recurrent message passing, GNN-based memory, or attention over prior predictions.\nApplication to Drug Repurposing: The framework will be tested on a biomedical knowledge graph where a GNN predicts drug-disease relationships.\nTranscriptomic data will be injected to simulate real-world perturbation-driven hypothesis testing.\n\nTechnical Approach:\n\nTransformer trained on tokenized graph [3] with injected transcriptomic tokens from scGPT [4] then using reinforcement learning, in a similar vein as Deep-Seek R1 [5], to explore if we can induce reasoning capabilities in this model to utilize transcriptomic data and previous predictions to revise its predictions.\nDataset: Drug-disease interaction graphs (e.g., PrimeKG) combined with transcriptomic perturbation data (e.g., LINCS L1000 or more recently the Tahoe-100M [6]).\nEvaluation Metrics: Improvement in link prediction accuracy and robustness to new data injections compared to static GNN baselines\n\nExpected Outcome: A proof-of-concept reasoning GNN that can revise its predictions dynamically at inference time, demonstrated in a drug repurposing setting.\n[1] Huang, K., Chandak, P., Wang, Q. et al. A foundation model for clinician-centered drug repurposing. Nat Med 30, 3601–3613 (2024). https://doi.org/10.1038/s41591-024-03233-x\n[2] Wu, P., Feng, Q., Kerchberger, V.E. et al. Integrating gene expression and clinical data to identify drug repurposing candidates for hyperlipidemia and hypertension. Nat Commun 13, 46 (2022). https://doi.org/10.1038/s41467-021-27751-1\n[3] Jinwoo Kim, Tien Dat Nguyen, Seonwoo Min, Sungjun Cho, Moontae Lee, Honglak Lee, and Seunghoon Hong. “Pure Transformers are Powerful Graph Learners.” In Advances in Neural Information Processing Systems 35 (NeurIPS 2022). 2022.\n[4] Cui, H., Wang, C., Maan, H. et al. scGPT: toward building a foundation model for single-cell multi-omics using generative AI. Nat Methods 21, 1470–1480 (2024). https://doi.org/10.1038/s41592-024-02201-0\n[5] Guo, D., Yang, D., Zhang, H., & Song, J. (2025). DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. arXiv preprint arXiv:2501.12948.\n[6] Zhang, J. et al. (2025). Tahoe-100M: A Giga-Scale Single-Cell Perturbation Atlas for Context-Dependent Gene Function and Cellular Modeling. bioRxiv. https://doi.org/10.1101/2025.02.20.639398"
  },
  {
    "objectID": "projects/Owen-Queen.html",
    "href": "projects/Owen-Queen.html",
    "title": "Topological data analysis (TDA) to elucidate protein functions via variant landscapes",
    "section": "",
    "text": "Owen Queen is a computer science Ph.D. student at Stanford University advised by James Zou. He previously obtained his B.S. in computer science from the University of Tennessee and worked as a research associate at Harvard Medical School under the supervision of Marinka Zitnik. His work focuses on building powerful and practical AI systems for biomedical discovery. In particular, he’s interested the combination of distributed lines of evidence to provide practitioners with actionable insights into biomedical data, facilitated by multimodal learning and agentic LLM systems. Previously, he’s worked on a variety of AI methods and applications in science, including building large multimodal models for protein phenotypes, evaluating explainability of graph neural networks, and designing geometric deep learning methods for polymer property prediction."
  },
  {
    "objectID": "projects/Owen-Queen.html#owen-queen",
    "href": "projects/Owen-Queen.html#owen-queen",
    "title": "Topological data analysis (TDA) to elucidate protein functions via variant landscapes",
    "section": "",
    "text": "Owen Queen is a computer science Ph.D. student at Stanford University advised by James Zou. He previously obtained his B.S. in computer science from the University of Tennessee and worked as a research associate at Harvard Medical School under the supervision of Marinka Zitnik. His work focuses on building powerful and practical AI systems for biomedical discovery. In particular, he’s interested the combination of distributed lines of evidence to provide practitioners with actionable insights into biomedical data, facilitated by multimodal learning and agentic LLM systems. Previously, he’s worked on a variety of AI methods and applications in science, including building large multimodal models for protein phenotypes, evaluating explainability of graph neural networks, and designing geometric deep learning methods for polymer property prediction."
  },
  {
    "objectID": "projects/Owen-Queen.html#project",
    "href": "projects/Owen-Queen.html#project",
    "title": "Topological data analysis (TDA) to elucidate protein functions via variant landscapes",
    "section": "Project",
    "text": "Project\nProtein functional annotation is a critical task in computational biology; it is concerned with deducing the function of a protein within a biological system, such as the pathways for which that protein plays a part. Determining the function of a protein is often the first step in developing drugs to target that protein or determining the cause of diseases. However, this task is extremely complex and traditionally involved years of work in a wet lab to determine a protein’s role in a specific biological system. Unsupervised or zero-shot protein functional annotation, with little-to-no wet lab data required, is a holy grail for computational biology.\nThe development of protein language models (PLMs), self-supervised language models trained on large protein databases, has supercharged research in functional proteomics. Despite no explicit training on functional data, PLMs have been shown to encode functional properties of proteins [1]. These functional properties are often encoded within the activations of PLMs, and elucidating them requires advanced machine learning approaches, such as mechanistic interpretability [1], multimodal learning [2], or fine-tuning on large, annotated datasets [3]. However, many gaps still exist in understanding protein functions from protein language models.\nBiological background: Proteins are made up of amino acid sequences, or sequences of a 20-character language. The function of a protein is determined by its chemical properties, and changes to the amino acid sequence may result in a change to the chemical properties (and therefore the function). A change to a protein’s sequence that does not change its function is a benign mutation; a change that affects the function is a pathogenic mutation. Pathogenic mutations often occur on chemically relevant parts of the protein, such as binding sites for other proteins or small molecules. Biologists have realized the importance of variants to understanding the properties of proteins. Deep Mutational Scanning (DMS) [4] is a functional assay (experiment in the wet lab) to understand protein fitness given mutations, or changes in the sequence of a protein. Prediction of variant effects in proteins has exploded in recent years, with the release of AlphaMissense [5], which was built solely to predict variant effects, and Evo-2 [6], whose major focus was on variant effect prediction. These methods give us predictive proxies for variant pathogenicity on proteins that have not been examined in DMS assays.\nMachine learning background: Topological data analysis (TDA) is a burgeoning field that is concerned with understanding the topology of data, i.e., what the shape and general construction of that data can tell us about its real-world characteristics. TDA has been applied in many areas of science, including molecular property prediction [7] and analysis of neuronal activation patterns [8]. In parallel, mechanistic interpretability is a new branch of explainability research that seeks to understand what a large neural network has learned. It has seen adoption in LLM research, mainly through the efforts of Anthropic [9], but it also has made its way into PLMs [1]. It has been theorized that TDA and mechanistic interpretability might have some overlap [10], either through the analysis of internal activations directly or of sparse autoencoder (SAE) features.\nThis project: I propose to tackle the problem of protein function prediction through a geometric and topological approach. DMS assays give us data on how a protein changes given some perturbation on the sequence of the protein. We can use these perturbations as a “landscape” to understand the distribution of valid changes that can be made on such a protein. Given this landscape, I propose to examine the topology of the internal activations of PLMs. Rather than training sparse autoencoders (SAEs) in a mechanistic interpretability fashion as [1], I believe the internal topology of the activations may hold valuable information about the functions of a given protein. TDA and other geometric deep learning tools might provide a valuable window into the organization of internal latent spaces in the model, allowing us to probe the model for understanding of the function of proteins. We can use the mutational landscapes of well-studied proteins to derive distributions of certain classes of proteins, such as those involved in transport pathways, and extend that to poorly studied proteins for which functions have not yet been derived.\nThere are many methodological leaps to be made here:\n\nWhat kinds of geometric and topological approaches are best for richly capturing PLM activations?\nWhich parts of the PLM do we examine? All layer activations? Final layer activations? Do we learn how to select out different activations?\nWhat is the minimal data we can use to learn this task? Can we transfer from well-studied proteins to poorly studied ones?\nWhat is the limit of biological functions that we can learn with this method? Will we stick to molecular properties such as enzyme properties or more complex phenotypes such as metabolic pathway involvement?\nWhat are the tradeoffs between SAEs and topological approaches for mechanistic interpretability?\nWhat is role of experimental (e.g., DMS) vs. predicted (e.g., AlphaMissense) variant effects in this task? Can we bootstrap on top of predicted variant effects to increase our data?\n\nIf solved, this work could have broad impact in both the machine learning and biology communities. This project will give you a chance to work directly on (1) state-of-the-art machine learning methods and (2) relevant biological problems that could have real-world impact on drug discovery and disease biology.\n[1] Simon, and Zou, “InterPLM: Discovering Interpretable Features in Protein Language Models via Sparse Autoencoders”, biorXiv 2024\n[2] Queen et al., “ProCyon: A multimodal foundation model for protein phenotypes”, biorXiv 2024\n[3] Notin et al., “ProteinGym: Large-Scale Benchmarks for Protein Design and Fitness Prediction”, NeurIPS 2023\n[4] Fowler and Fields, “Deep mutational scanning: a new style of protein science”, Nature Methods, 2014\n[5] Cheng et al., “Accurate proteome-wide missense variant effect prediction with AlphaMissense”, Science 2023\n[6] Brixi et al., “Genome modeling and design across all domains of life with Evo 2”, biorXiv 2025\n[7] Townsend et al., “Representation of molecular structures with persistent homology for machine learning applications in chemistry”, Nature Communications 2020\n[8] Saggar et al., “Precision dynamical mapping using topological data analysis reveals a hug-like transition state at rest”, Nature Communications 2022\n[9] Templeton et al., “Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet”, Anthropic Technical Report, 2024\n[10] Carlsson, “Topological Data Analysis and Mechanistic Intepretability”, LessWrong blog post, 2025"
  },
  {
    "objectID": "logml2021/projects2021/project2/index.html",
    "href": "logml2021/projects2021/project2/index.html",
    "title": "Characterising Universes in String Theory using Geometric Learning",
    "section": "",
    "text": "Challenger Mishra is a Departmental Eary Career Academic Fellow at the Computer Lab, University of Cambridge, and a Stipendiary Lecturer of Applied Mathematics at Lady Margaret Hall, University of Oxford. His research interests include Machine Learning, Calabi-Yau manifolds in String theory, String compactifications, and their interplay. Prior to this, he did a doctorate as a Rhodes Scholar in String theory at the Rudolf Peierls Center for Theoretical Physics, University of Oxford. Challenger completed his undergraduate work in Physics from the Indian Institute of Science Education and Research, Kolkata."
  },
  {
    "objectID": "logml2021/projects2021/project2/index.html#challenger-mishra",
    "href": "logml2021/projects2021/project2/index.html#challenger-mishra",
    "title": "Characterising Universes in String Theory using Geometric Learning",
    "section": "",
    "text": "Challenger Mishra is a Departmental Eary Career Academic Fellow at the Computer Lab, University of Cambridge, and a Stipendiary Lecturer of Applied Mathematics at Lady Margaret Hall, University of Oxford. His research interests include Machine Learning, Calabi-Yau manifolds in String theory, String compactifications, and their interplay. Prior to this, he did a doctorate as a Rhodes Scholar in String theory at the Rudolf Peierls Center for Theoretical Physics, University of Oxford. Challenger completed his undergraduate work in Physics from the Indian Institute of Science Education and Research, Kolkata."
  },
  {
    "objectID": "logml2021/projects2021/project2/index.html#project",
    "href": "logml2021/projects2021/project2/index.html#project",
    "title": "Characterising Universes in String Theory using Geometric Learning",
    "section": "Project",
    "text": "Project\nOne of the holy grails of modern theoretical physics is the unification of Quantum Mechanics with Einstein’s relativity. String theory is the only known consistent theory of quantum gravity, and arguably the most promising candidate for a unified theory of physics. Since its inception in the late 1960s, it has provided tremendous insights into our understanding of the physical world, and has overseen many interesting developments in various branches of pure mathematics and theoretical physics. Despite string theory’s many successes, a string model that explains all observed data from cosmology and particle physics experiments, has eluded discovery. This is owing to the particularly large landscape of valid string theory solutions, estimated to be of the size 10^{270,000}. Most of these solutions are thought to lead to descriptions of universes that do not resemble ours in detail.\nString theory posits extra-dimensions of space. These are often described by complex geometries called Calabi—Yau manifolds. A class of string theory solutions (or vacua) is characterised by Complete Intersection Calabi–Yau manifolds, and bundles over them. The data corresponding to these are encoded as bipartite graphs and integer matrices, whose size is governed by the (topological) properties of the bundles and Calabi–Yau geometries. The resulting dataset is of size tens of thousands. The objective of this project is to characterise these different solutions (Universes) using Machine Learning. More concretely, the aim is to obtain a suitable metric on this space of solutions that ‘scores’ stringy solutions based on their closeness to reality, i.e., observations from particle accelerators like the LHC. Such a metric could be approximated by a sufficiently deep neural network. Insights from such a metric will allow the construction of even more realistic string solutions, ESP on geometries that have been out of current computational reach.\nThe project will allow the participant(s) to delve into fundamentals of complex geometry, learn about effective representations of geometric data in Machine Learning, and develop an empirical understanding of the ML tools that are effective in such geometric problems.\nReferences: [1] Calabi-Yau Spaces in the String Landscape – Yang-Hui He, https://arxiv.org/abs/2006.16623\n[2] Calabi-Yau manifolds, Discrete Symmetries, and String theory – Challenger Misha, https://ora.ox.ac.uk/objects/uuid:4a174981-085e-4e81-8f27-b48533f08315\n[3] Heterotic Line Bundle Standard Models – Lara B. Anderson, James Gray, Andre Lukas, Eran Palti\nhttps://arxiv.org/abs/1202.1757"
  },
  {
    "objectID": "logml2021/projects2021/project4/index.html",
    "href": "logml2021/projects2021/project4/index.html",
    "title": "Surface reconstruction from point clouds",
    "section": "",
    "text": "Rana Hanocka is a Ph.D. candidate at Tel Aviv University under the supervision of Daniel Cohen-Or and Raja Giryes. Rana obtained an M.Sc. in Electrical Engineering from Tel Aviv University and a B.Sc. in Electrical Engineering from Rensselaer Polytechnic Institute. Rana is the recipient of the Dan David Prize’s 2020 Scholarship in Artificial Intelligence, and was awarded the Outstanding Data Science Fellowship by Israel’s Council for Higher Education."
  },
  {
    "objectID": "logml2021/projects2021/project4/index.html#rana-hanocka",
    "href": "logml2021/projects2021/project4/index.html#rana-hanocka",
    "title": "Surface reconstruction from point clouds",
    "section": "",
    "text": "Rana Hanocka is a Ph.D. candidate at Tel Aviv University under the supervision of Daniel Cohen-Or and Raja Giryes. Rana obtained an M.Sc. in Electrical Engineering from Tel Aviv University and a B.Sc. in Electrical Engineering from Rensselaer Polytechnic Institute. Rana is the recipient of the Dan David Prize’s 2020 Scholarship in Artificial Intelligence, and was awarded the Outstanding Data Science Fellowship by Israel’s Council for Higher Education."
  },
  {
    "objectID": "logml2021/projects2021/project4/index.html#project",
    "href": "logml2021/projects2021/project4/index.html#project",
    "title": "Surface reconstruction from point clouds",
    "section": "Project",
    "text": "Project\nIn this project, we will take a closer look into surface reconstruction from point clouds. This project is a hands-on project in PyTorch. You will gain familiarity with 3D surface reconstruction and applying deep neural networks to meshes. The project is mainly focused on the Point2Mesh [1] technique, which optimizes the weights of a CNN to deform some initial mesh to shrink-wrap the input point cloud. You will apply this technique to scans and a wider variety of data in order to explore the current strengths and limitations of such a technique, and how it can be further improved. One interesting direction is to learn which edge to split, thereby defining where to add additional mesh connectivity. By the end of the project, you should better understand these topics from a theoretical and practical perspective, and gain insights with respect to how to incorporate similar concepts into your own research.\n[1] Point2Mesh: A Self-Prior for Deformable Meshes. Rana Hanocka, Gal Metzer, Raja Giryes, and Daniel Cohen-Or. SIGGRAPH 2020."
  },
  {
    "objectID": "logml2021/projects2021/project18/index.html",
    "href": "logml2021/projects2021/project18/index.html",
    "title": "Uncovering and correcting biases in neuroimaging studies",
    "section": "",
    "text": "Ira is currently a Research Engineer at DeepMind working on Incubation of cutting edge research. Previously, she was a Senior Machine Learning Researcher at Twitter, focusing on real-time personalisation and causal analysis for recommendations. She completed a Doctoral degree in Medical Image Computing at Imperial College London. Her research focused on developing methods for modelling and analysing graph-structured neuroimaging data at an individual or population level using traditional graph theoretical approaches and geometric deep learning. During her PhD, she spent some time at the Massachusetts General Hospital, Harvard Medical School, where she worked on outcome prediction for ischemic stroke patients."
  },
  {
    "objectID": "logml2021/projects2021/project18/index.html#ira-ktena",
    "href": "logml2021/projects2021/project18/index.html#ira-ktena",
    "title": "Uncovering and correcting biases in neuroimaging studies",
    "section": "",
    "text": "Ira is currently a Research Engineer at DeepMind working on Incubation of cutting edge research. Previously, she was a Senior Machine Learning Researcher at Twitter, focusing on real-time personalisation and causal analysis for recommendations. She completed a Doctoral degree in Medical Image Computing at Imperial College London. Her research focused on developing methods for modelling and analysing graph-structured neuroimaging data at an individual or population level using traditional graph theoretical approaches and geometric deep learning. During her PhD, she spent some time at the Massachusetts General Hospital, Harvard Medical School, where she worked on outcome prediction for ischemic stroke patients."
  },
  {
    "objectID": "logml2021/projects2021/project18/index.html#project",
    "href": "logml2021/projects2021/project18/index.html#project",
    "title": "Uncovering and correcting biases in neuroimaging studies",
    "section": "Project",
    "text": "Project\nRecent work [1] on neuroimaging has demonstrated significant benefits of using population graphs to capture non-imaging information in the prediction of neurodegenerative and neurodevelopmental disorders. These non-imaging attributes may contain demographic information about the individuals, e.g. age or sex, but also the acquisition site, as imaging protocols and hardware might significantly differ across sites in large-scale studies. The effect of the latter is particularly prevalent in functional connectomics studies, where it’s unclear how to sufficiently homogenise fMRI signals across the different sites. A recent study [2] has highlighted the need to investigate potential biases in the classifiers devised using large-scale datasets, which might be imbalanced in terms of one or more sensitive attributes (like gender and race). This can be exacerbated when employing these attributes in a population graph and lead to disparate predictive performance across sub-populations. This project aims to uncover any potential biases of a semi-supervised classifier that relies on a population graph and explores methods to mitigate such biases to produce fairer predictions across the population.\n[1] Parisot, S., Ktena, S. I., Ferrante, E., Lee, M., Moreno, R. G., Glocker, B., & Rueckert, D. Disease Prediction using Graph Convolutional Networks: Application to Autism Spectrum Disorder and Alzheimer’s Disease. Medical Image Analysis, 2018\n[2] Larrazabal, Agostina J., et al. “Gender imbalance in medical imaging datasets produces biased classifiers for computer-aided diagnosis.” Proceedings of the National Academy of Sciences, 2020"
  },
  {
    "objectID": "logml2021/projects2021/project16/index.html",
    "href": "logml2021/projects2021/project16/index.html",
    "title": "Manifold optimization and recent applications",
    "section": "",
    "text": "Bamdev Mishra is Senior Applied Scientist at Microsoft India in the Office India Intelligence team. He develops machine learning (ML) solutions for Kaizala, Office Lens, Operations, and Information Protection for Office365, to name a few. Prior to this, he worked on various ML problems in the retail domain including competitive price prediction, review abuse detection, and style recommendations.\nBamdev looks into problem domains that allow to use and build ML solutions for industrial applications. On the research side, his primary research interests include nonlinear optimization, stochastic learning, and matrix and tensor learning methods. He has published many technical papers in ML, NLP, and numerical optimization.\nBamdev received the BTech and MTech degrees in Electrical Engineering from the Indian Institute of Technology Bombay, India, in 2010, and the Ph.D. degree from the University of Liège, Belgium, in 2014. He was a Postdoctoral Researcher at the University of Liège and a Visiting Research Associate at the University of Cambridge from 2014 to 2015."
  },
  {
    "objectID": "logml2021/projects2021/project16/index.html#bamdev-mishra",
    "href": "logml2021/projects2021/project16/index.html#bamdev-mishra",
    "title": "Manifold optimization and recent applications",
    "section": "",
    "text": "Bamdev Mishra is Senior Applied Scientist at Microsoft India in the Office India Intelligence team. He develops machine learning (ML) solutions for Kaizala, Office Lens, Operations, and Information Protection for Office365, to name a few. Prior to this, he worked on various ML problems in the retail domain including competitive price prediction, review abuse detection, and style recommendations.\nBamdev looks into problem domains that allow to use and build ML solutions for industrial applications. On the research side, his primary research interests include nonlinear optimization, stochastic learning, and matrix and tensor learning methods. He has published many technical papers in ML, NLP, and numerical optimization.\nBamdev received the BTech and MTech degrees in Electrical Engineering from the Indian Institute of Technology Bombay, India, in 2010, and the Ph.D. degree from the University of Liège, Belgium, in 2014. He was a Postdoctoral Researcher at the University of Liège and a Visiting Research Associate at the University of Cambridge from 2014 to 2015."
  },
  {
    "objectID": "logml2021/projects2021/project16/index.html#project",
    "href": "logml2021/projects2021/project16/index.html#project",
    "title": "Manifold optimization and recent applications",
    "section": "Project",
    "text": "Project\nOptimization over smooth manifolds or manifold optimization involves minimizing an objective function over a smooth constrained set. Many such sets have usually a manifold structure. Some particularly useful manifolds include the set of orthogonal matrices, the set of symmetric positive definite matrices, the set of subspaces, the set of fixed-rank matrices/tensors, and the set of doubly stochastic matrices (optimal transport plans), to name a few [1]. Consequently, there has been a development of a number of manifold optimization toolboxes [2].\nIn this project, we make use of these wonderful tools to solve a few machine learning problems with manifold optimization. The aim would be to get a hands-on experience of manifold optimization.\n[1] Boumal, N., 2020. An introduction to optimization on smooth manifolds. Web: http://sma.epfl.ch/~nboumal/book/index.html.\n[2] Manopt, pymanopt, Manopt.jl, McTorch, Geomstats, ROPTLIB, and so on. The links to many of these toolboxes are available on https://www.manopt.org/about.html."
  },
  {
    "objectID": "logml2021/projects2021/project10/index.html",
    "href": "logml2021/projects2021/project10/index.html",
    "title": "Geometric Learning on Shapes and Distributions with Optimal Transport",
    "section": "",
    "text": "Jean Feydy graduated from the ENS Paris in 2016, writing his master’s thesis on the denoising of CT images at Siemens Healthcare (Princeton, NJ). He then pursued a PhD under the supervision of Alain Trouvé (ENS Paris-Saclay) at the intersection between computational anatomy, optimal transport theory and geometric machine learning. He is currently doing a post-doc with Michael Bronstein (Imperial College London) and will soon take up a faculty position at INRIA Paris, working on building new bridges between medical imaging, computer vision and geometric statistics.\nHis research focuses on flexible geometric methods for scalable data analysis. His most significant works are distributed through: - the KeOps library for fast geometric computations (www.kernel-operations.io), - the GeomLoss library for scalable optimal transport (www.kernel-operations.io/geomloss/), - his accessible textbook “Geometric data analysis, beyond convolutions” (www.jeanfeydy.com/geometric_data_analysis.pdf)."
  },
  {
    "objectID": "logml2021/projects2021/project10/index.html#jean-feydy",
    "href": "logml2021/projects2021/project10/index.html#jean-feydy",
    "title": "Geometric Learning on Shapes and Distributions with Optimal Transport",
    "section": "",
    "text": "Jean Feydy graduated from the ENS Paris in 2016, writing his master’s thesis on the denoising of CT images at Siemens Healthcare (Princeton, NJ). He then pursued a PhD under the supervision of Alain Trouvé (ENS Paris-Saclay) at the intersection between computational anatomy, optimal transport theory and geometric machine learning. He is currently doing a post-doc with Michael Bronstein (Imperial College London) and will soon take up a faculty position at INRIA Paris, working on building new bridges between medical imaging, computer vision and geometric statistics.\nHis research focuses on flexible geometric methods for scalable data analysis. His most significant works are distributed through: - the KeOps library for fast geometric computations (www.kernel-operations.io), - the GeomLoss library for scalable optimal transport (www.kernel-operations.io/geomloss/), - his accessible textbook “Geometric data analysis, beyond convolutions” (www.jeanfeydy.com/geometric_data_analysis.pdf)."
  },
  {
    "objectID": "logml2021/projects2021/project10/index.html#project",
    "href": "logml2021/projects2021/project10/index.html#project",
    "title": "Geometric Learning on Shapes and Distributions with Optimal Transport",
    "section": "Project",
    "text": "Project\nOptimal transport generalizes sorting to spaces of dimension D&gt;1. It induces the Wasserstein metric (aka. Earth Mover’s Distance) between probability distributions, which allows us to work with unlabelled point clouds using a simple and intuitive particle-based model.\nIn this project, we will build upon the fast numerical routines of the GeomLoss library (https://www.kernel-operations.io/geomloss/) to explore the use of the Wasserstein metric in geometric data analysis. We will first start with a short lecture on the definition and main properties of optimal transport. Then, we will rely on simple experiments with Wasserstein barycenters and gradient flows to get an intuitive understanding of the optimal transport distance. Finally, we will study the impact of this metric on several standard tasks, from 3D shape registration to the UMAP visualization of a dataset of histograms.\nThis project will allow you to get a hands-on experience of optimal transport tools in realistic application scenarios. Notably, we will highlight both the strengths and the limitations of this theory in data sciences: by the end of the week, you should have a clear picture of what optimal transport can (and cannot) bring to your own research work."
  },
  {
    "objectID": "logml2021/projects2021/project19/index.html",
    "href": "logml2021/projects2021/project19/index.html",
    "title": "Geometry of HMC and Geometric Integration for Sampling and Optimization",
    "section": "",
    "text": "Alessandro is a Research Associate in the Engineering Department at the University of Cambridge and a visiting researcher at the Alan Turing Institute. His research focuses on the empowerment of measures via the geometrisation of statistical methods, such as Hamiltonian Monte Carlo and kernel algorithms built with Stein operators. After having completed the MMathPhys of the University of Warwick and Part III of the Mathematical Tripos at Cambridge, he received his PhD from Imperial College London."
  },
  {
    "objectID": "logml2021/projects2021/project19/index.html#alessandro-barp",
    "href": "logml2021/projects2021/project19/index.html#alessandro-barp",
    "title": "Geometry of HMC and Geometric Integration for Sampling and Optimization",
    "section": "",
    "text": "Alessandro is a Research Associate in the Engineering Department at the University of Cambridge and a visiting researcher at the Alan Turing Institute. His research focuses on the empowerment of measures via the geometrisation of statistical methods, such as Hamiltonian Monte Carlo and kernel algorithms built with Stein operators. After having completed the MMathPhys of the University of Warwick and Part III of the Mathematical Tripos at Cambridge, he received his PhD from Imperial College London."
  },
  {
    "objectID": "logml2021/projects2021/project19/index.html#project",
    "href": "logml2021/projects2021/project19/index.html#project",
    "title": "Geometry of HMC and Geometric Integration for Sampling and Optimization",
    "section": "Project",
    "text": "Project\nGeometric integration plays a central role in many applications. In this project, we will discuss its applications to sampling and optimisation. For sampling, we will uncover the canonical geometry of measures and apply it construct diffusions and dynamics preserving measures, symmetries and constraints. We will then discuss general strategies to construct Hamiltonian-based geometric integrators maintaining some critical properties, in particular volume preservation and conservation of a shadow energy, and hence obtain the family of Hamiltonian Monte Carlo samplers on vector spaces and manifolds.\nWe will then apply similar techniques to optimization in order to obtain rate-matching integrators that preserve the decay rate of dissipative dynamics."
  },
  {
    "objectID": "logml2021/projects2021/project1/index.html",
    "href": "logml2021/projects2021/project1/index.html",
    "title": "Stability or Collapse: Topological Properties of Deep Autoencoders",
    "section": "",
    "text": "Kelly Spendlove is a postdoctoral researcher at the University of Oxford in the Centre for Topological Data Analysis. His research primarily concerns applied algebraic topological approaches to dynamical systems. He completed his PhD at Rutgers University under Konstantin Mischaikow, with the support of a NSF Graduate Fellowship, while spending time as a long term visitor at VU Amsterdam, Ohio State University, and Kyoto University."
  },
  {
    "objectID": "logml2021/projects2021/project1/index.html#kelly-spendlove",
    "href": "logml2021/projects2021/project1/index.html#kelly-spendlove",
    "title": "Stability or Collapse: Topological Properties of Deep Autoencoders",
    "section": "",
    "text": "Kelly Spendlove is a postdoctoral researcher at the University of Oxford in the Centre for Topological Data Analysis. His research primarily concerns applied algebraic topological approaches to dynamical systems. He completed his PhD at Rutgers University under Konstantin Mischaikow, with the support of a NSF Graduate Fellowship, while spending time as a long term visitor at VU Amsterdam, Ohio State University, and Kyoto University."
  },
  {
    "objectID": "logml2021/projects2021/project1/index.html#project",
    "href": "logml2021/projects2021/project1/index.html#project",
    "title": "Stability or Collapse: Topological Properties of Deep Autoencoders",
    "section": "Project",
    "text": "Project\nRecent ideas [1,2] have considered how the behavior of different activation functions may depend upon their ‘topological’ properties, e.g., homeomorphism vs continuity. The work of Naitzat et al [1] seems to suggest that ReLU activations exhibit a stronger effect upon the topology, collapsing it in earlier layers and more quickly than homeomorphic activations such as Tanh or Leaky ReLU.\nOn the other hand, auto-encoders are neural networks which minimize the distance between a reconstruction and the original data, producing both an ‘encoder’ and ‘decoder’. The stability theorem of persistent homology suggests that if an autoencoder is trained to reconstruct the data within epsilon, the resulting persistence diagrams (a proxy for the topology) are also within distance epsilon. This would seem to suggest that the topology cannot be changed by much, even with the use of ReLU and a deep autoencoder. The goal of this project is to investigate and understand how these two observations can be reconciled; and once properly understood, to determine whether these observations can be used to design topologically-faithful dimensionality reduction techniques.\n[1] Naitzat, G., Zhitnikov, A., & Lim, L. H. (2020). Topology of deep neural networks. Journal of Machine Learning Research, 21(184), 1-40. [2] C. Olah. Neural networks, manifolds, and topology.http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/, 2014."
  },
  {
    "objectID": "logml2021/projects2021/project9/index.html",
    "href": "logml2021/projects2021/project9/index.html",
    "title": "Self-supervised non-rigid correspondence by geodesic distortion minimization using the deformation field",
    "section": "",
    "text": "Søren Hauberg is a professor in the Section for Cognitive Systems at the Technical University of Denmark. He received his PhD in computer science from the University of Copenhagen in 2011. Prior to pursuing a PhD he worked as a “digital lumberjack” in the startup Dralle A/S. He was a post doc for two years at Perceiving Systems at the Max Planck Institute for Intelligent Systems working with Michael Black. In 2013, he was the sole computer science recipient of the Sapere Aude Research Talent award from the Danish Council for Independent Research, and in 2016 he was the sole computer science Villum Young Investigator. In 2017 he was further awarded a starting grant from the European Research Council. In 2018, he joined the Young Scientists community under the World Economic Forum, and was in the process named one of “10 of the most exciting young scientists working in the world today.”\nHis research interest lie in the span of geometry and statistics. He develops machine learning techniques using geometric constructions, and works on the related numerical challenges. He is particularly interested in random geometries as they naturally appear in learning."
  },
  {
    "objectID": "logml2021/projects2021/project9/index.html#søren-hauberg",
    "href": "logml2021/projects2021/project9/index.html#søren-hauberg",
    "title": "Self-supervised non-rigid correspondence by geodesic distortion minimization using the deformation field",
    "section": "",
    "text": "Søren Hauberg is a professor in the Section for Cognitive Systems at the Technical University of Denmark. He received his PhD in computer science from the University of Copenhagen in 2011. Prior to pursuing a PhD he worked as a “digital lumberjack” in the startup Dralle A/S. He was a post doc for two years at Perceiving Systems at the Max Planck Institute for Intelligent Systems working with Michael Black. In 2013, he was the sole computer science recipient of the Sapere Aude Research Talent award from the Danish Council for Independent Research, and in 2016 he was the sole computer science Villum Young Investigator. In 2017 he was further awarded a starting grant from the European Research Council. In 2018, he joined the Young Scientists community under the World Economic Forum, and was in the process named one of “10 of the most exciting young scientists working in the world today.”\nHis research interest lie in the span of geometry and statistics. He develops machine learning techniques using geometric constructions, and works on the related numerical challenges. He is particularly interested in random geometries as they naturally appear in learning."
  },
  {
    "objectID": "logml2021/projects2021/project9/index.html#project",
    "href": "logml2021/projects2021/project9/index.html#project",
    "title": "Self-supervised non-rigid correspondence by geodesic distortion minimization using the deformation field",
    "section": "Project",
    "text": "Project\nLatent variable models, such as the variational autoencoder, suffer from the identifiability problem: there is no unique configuration of the latent variables. This is problematic as latent variables are often inspected, e.g. through visualization, to gain insights into the data generating process. The lack of identifiability then raise the risk of misinterpreting the data as conclusions may be drawn from arbitrary latent instantiations.\nIn this project you will investigate a geometric solution to the identifiability problem that amounts to endowing the latent space with a particular Riemannian metric. You will learn latent representations and compute geodesics accordingly.\nReferences: () Latent Space Oddity: on the Curvature of Deep Generative Models Georgios Arvanitidis, Lars Kai Hansen and Søren Hauberg. In International Conference on Learning Representations (ICLR), 2018. () Only Bayes should learn a manifold (on the estimation of differential geometric structure from data) Søren Hauberg."
  },
  {
    "objectID": "logml2021/projects2021/project12/index.html",
    "href": "logml2021/projects2021/project12/index.html",
    "title": "Implicit Node and Edge Features for More Expressive Graph Neural Networks",
    "section": "",
    "text": "Octabian is broadly interested in representation learning for unstructured data (graphs), 3D objects (e.g. molecules), text or images through statistical or geometric models that could be devised and understood in a mathematically principled and elegant manner. In particular, he explored non-Euclidean geometries in Machine Learning to overcome some of the current difficulties in graph representation learning and generation, e.g. finding and learning latent hierarchical structures in data via hyperbolic geometry, as well as combining optimal transport and graph neural networks for better models that deal with graphs. He is currently applying my models to problems related to computational chemistry such as drug discovery"
  },
  {
    "objectID": "logml2021/projects2021/project12/index.html#octavian-eugen-ganea",
    "href": "logml2021/projects2021/project12/index.html#octavian-eugen-ganea",
    "title": "Implicit Node and Edge Features for More Expressive Graph Neural Networks",
    "section": "",
    "text": "Octabian is broadly interested in representation learning for unstructured data (graphs), 3D objects (e.g. molecules), text or images through statistical or geometric models that could be devised and understood in a mathematically principled and elegant manner. In particular, he explored non-Euclidean geometries in Machine Learning to overcome some of the current difficulties in graph representation learning and generation, e.g. finding and learning latent hierarchical structures in data via hyperbolic geometry, as well as combining optimal transport and graph neural networks for better models that deal with graphs. He is currently applying my models to problems related to computational chemistry such as drug discovery"
  },
  {
    "objectID": "logml2021/projects2021/project12/index.html#project",
    "href": "logml2021/projects2021/project12/index.html#project",
    "title": "Implicit Node and Edge Features for More Expressive Graph Neural Networks",
    "section": "Project",
    "text": "Project\nGraph Neural Networks (GNN) have been achieving state-of-the-art performance in various graph related tasks, being the first adopted solution especially when graphs have node and edge features. However, GNNs have several difficulties [4], such as capturing long-range graph interactions (due to the oversquashing effect) or differentiating locally isomorphic nodes [5] (e.g. based on the WL test). Moreover, GNNs haven’t yet been reconciled or combined with positional independent node embedding (PINE) approaches such as Node2Vec [1] or distortion based embeddings (e.g. Poincare embeddings [2]). The latter are known to capture well long-range graph interactions, can be trained fully unsupervised, but are not uniquely defined as they can be arbitrarily transformed with a shared invertible matrix while keeping the loss value unchanged. In this project, we propose to explore combining GNNs and PINEs in a joint end-to-end supervised trainable method by leveraging the power of implicit differentiation (ID) [3] traditionally used in meta-learning approaches. Given an input graph, we will create an implicit layer that learns PINEs based on an unsupervised objective (e.g. distortion loss), and these will in turn become node features that will be the input of a GNN. Importantly, using ID, we can backpropagate through the PINE training procedure and, thus, obtain meaningful PINE features for the downstream task at hand. This would allow us to obtain globally (at graph level) correlated node features for GNNs, to differentiate non-isomorphic graphs otherwise indistinguishable by the WL test, and to reflect on other (unsupervised) inductive biases useful for specific downstream graph problems.\n[1] Node2Vec: https://snap.stanford.edu/node2vec/ [2] Poincare Embeddings: https://arxiv.org/pdf/1705.08039.pdf [3] ID Neurips 2020 tutorial: https://www.youtube.com/watch?v=MX1RJELWONc [4] GNN difficulties: https://arxiv.org/pdf/2006.05205.pdf , https://arxiv.org/abs/2006.13318, rb.gy/quo3n6 [5] https://www.mit.edu/~vgarg/GNNs_FinalVersion.pdf"
  },
  {
    "objectID": "logml2021/projects2021/project14/index.html",
    "href": "logml2021/projects2021/project14/index.html",
    "title": "Morphing of manifold-valued images",
    "section": "",
    "text": "Marie-Julie Rakotosaona is a PhD candidate at Ecole Polytechnique in the GeoVic team where she is advised by Maks Ovsjanikov. Her research focuses on 3D shape analysis and processing. She is working on finding well suited deep learning methods and representations for processing, understanding, and creating 3D shapes."
  },
  {
    "objectID": "logml2021/projects2021/project14/index.html#marie-julie-rakotosaona",
    "href": "logml2021/projects2021/project14/index.html#marie-julie-rakotosaona",
    "title": "Morphing of manifold-valued images",
    "section": "",
    "text": "Marie-Julie Rakotosaona is a PhD candidate at Ecole Polytechnique in the GeoVic team where she is advised by Maks Ovsjanikov. Her research focuses on 3D shape analysis and processing. She is working on finding well suited deep learning methods and representations for processing, understanding, and creating 3D shapes."
  },
  {
    "objectID": "logml2021/projects2021/project14/index.html#project",
    "href": "logml2021/projects2021/project14/index.html#project",
    "title": "Morphing of manifold-valued images",
    "section": "Project",
    "text": "Project\nExisting learning-based methods for mesh reconstruction from fixed point clouds mostly generate triangles individually, making it hard to create consistent manifold meshes. In this project, we will build a novel method aimed at estimating local surfaces from point clouds and constructing high quality meshes. We are interested in leveraging the properties of 2D Delaunay triangulations to construct a mesh from small manifold surface elements. The method first learns local logarithmic maps around estimated geodesic neighborhoods centered at each point, from which we can compute manifold Delaunay triangulation. The local 2D projections are then synchronized to maximize the manifoldness of the global reconstructed mesh.\nDuring this week, we will first build a robust learning-based pipeline to mesh point clouds. Throughout this process, you will gain significant familiarity with or get a deeper understanding of basic concepts in geometry for representing 3D shapes as well as existing tools for machine learning on point clouds such as PointNet or FoldingNet. In the second stage of this project, we will explore novel directions to improve the proposed method and build tools for both meshing and analysis of 3D surfaces."
  },
  {
    "objectID": "logml2021/speaker.html",
    "href": "logml2021/speaker.html",
    "title": "Speakers",
    "section": "",
    "text": "Keynote speakers\n\n\n\n\n\n\n\n\n\n\nMichael Bronstein\n\n\nImperial College London, Twitter\n\n\n\n\n\n\n\n\n\n\n\n\n\nUlrike Tillmann FRS\n\n\nUniversity of Oxford\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\nSpeakers\n\n\n\n\n\n\n\n\n\n\nMichael Betancourt\n\n\nImperial College London, Twitter\n\n\n\n\n\n\n\n\n\n\n\n\n\nSanja Fidler\n\n\nUniversity of Toronto, Vector Institute, Nvidia\n\n\n\n\n\n\n\n\n\n\n\n\n\nNiloy J. Mitra\n\n\nUniversity College London\n\n\n\n\n\n\n\n\n\n\n\n\n\nThomas Kipf\n\n\nGoogle Brain\n\n\n\n\n\n\n\n\n\n\n\n\n\nMaximilian Nickel\n\n\nFacebook AI Research\n\n\n\n\n\n\n\n\n\n\n\n\n\nMaks Ovsjanikov\n\n\nÉcole Polytechnique\n\n\n\n\n\n\n\n\n\n\n\n\n\nMartin Rumpf\n\n\nUniversität Bonn\n\n\n\n\n\n\n\n\n\n\n\n\n\nGabriele Steidl\n\n\nTechnische Universität Berlin\n\n\n\n\n\n\n\n\n\n\n\n\n\nRuriko Yoshida\n\n\nNaval Postgraduate School, California\n\n\n\n\n\n\n\n\n\n\n\n\n\nMarinka Zitnik\n\n\nHarvard University\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Archives",
      "LOGML 2021",
      "Speakers"
    ]
  },
  {
    "objectID": "logml2021/speakers2021/keynote/ulrike-tillmann.html",
    "href": "logml2021/speakers2021/keynote/ulrike-tillmann.html",
    "title": "Ulrike Tillmann FRS",
    "section": "",
    "text": "website\n  \n\n\n\nUlrike Tillmann FRS is a mathematician working on topology and geometry. Much of her recent work in topology is motivated by questions from topological data analysis and data science.\nShe obtained her PhD at Stanford University, Habilitation at Universität Bonn, and now works as a professor of mathematics at the university of Oxford. She is on the editorial boards for a range of mathematical journals, including being the founding and managing editor for the Journal of Topology. She was awarded the Whitehead Prize by the London Mathematical Society (LMS) in 2004, a Bessel Forschungspreis in 2008, was elected an inaugural fellow of the American Mathematical Society in 2012, and has been a Fellow of the Royal Society since 2008.\nApart from that, she is active in outreach and for women in mathematics, for example acting as EMS–EWM Steering Committee Chair for summer schools at the Mittag-Leffler Institute 2015–20 and leading groups at Women in Topology and Women in Mathematical Physics."
  },
  {
    "objectID": "logml2021/speakers2021/other/ruriko-yoshida.html",
    "href": "logml2021/speakers2021/other/ruriko-yoshida.html",
    "title": "Ruriko Yoshida",
    "section": "",
    "text": "Home\n    Speakers\n    Ruriko Yoshida\n\n  \n\n\n  \n \n \n  \n   \n  \n    \n     website\n  \n\n\n\nDr. Ruriko Yoshida is an Associate Professor of Operations Research at the Naval Postgraduate School. Her research topics cover a wide variety of areas: applications of algebraic combinatorics to statistical problems, such as goodness of fit tests, optimized camera placement in sensor networks, phylogenetics, and phylogenomics. Dr. Yoshida received her Ph.D. (2004) in Mathematics from the University of California, Davis. She then went to the University of California, Berkeley as a postdoctoral researcher, and then Duke University for her postdoctoral research from 2004 to 2006. She was at the University of Kentucky from 2006 to 2016 as an assistant and then as associate professor. In 2016, she joined the operations research department at the Naval Postgraduate School."
  },
  {
    "objectID": "logml2021/speakers2021/other/maximilian-nickel.html",
    "href": "logml2021/speakers2021/other/maximilian-nickel.html",
    "title": "Maximilian Nickel",
    "section": "",
    "text": "Home\n    Speakers\n    Maximilian Nickel\n\n  \n\n\n  \n \n \n  \n   \n  \n    \n     website\n  \n\n\n\nMax is a Research Scientist at Facebook AI Research in New York, working on foundational methods for learning from structured and temporal information and their applications in artificial intelligence, network science, and computational social science. Before joining FAIR in 2016, he was a postdoctoral fellow at MIT where he was with the Laboratory for Computational and Statistical Learning and the Center for Brains, Minds and Machines. In 2013, Max earned his PhD with summa cum laude from the Ludwig Maximilian University in Munich. From 2010 to 2013 he worked as a research assistant at Siemens Corporate Technology."
  },
  {
    "objectID": "logml2021/speakers2021/other/sanja-fidler.html",
    "href": "logml2021/speakers2021/other/sanja-fidler.html",
    "title": "Sanja Fidler",
    "section": "",
    "text": "Home\n    Speakers\n    Sanja Fidler\n\n  \n\n\n  \n \n \n  \n   \n  \n    \n     website\n  \n\n\n\nSanja is an Associate Professor at University of Toronto, and a Director of AI at NVIDIA, leading a research lab in Toronto. Prior coming to Toronto, in 2012/2013, she was a Research Assistant Professor at Toyota Technological Institute at Chicago, an academic institute located in the campus of University of Chicago. She did my postdoc with Prof. Sven Dickinson at University of Toronto in 2011/2012. She finished her PhD in 2010 at University of Ljubljana in Slovenia in the group of Prof. Ales Leonardis. In 2010, she was visiting Prof. Trevor Darrell’s group at UC Berkeley and ICSI. She got my BSc degree in Applied Math at University of Ljubljana."
  },
  {
    "objectID": "logml2021/speakers2021/other/martin-rumpf.html",
    "href": "logml2021/speakers2021/other/martin-rumpf.html",
    "title": "Martin Rumpf",
    "section": "",
    "text": "Home\n    Speakers\n    Martin Rumpf\n\n  \n\n\n  \n \n \n  \n   \n  \n    \n     website\n  \n\n\n\nMartin Rumpf studied mathematics at Bonn University. He did his PhD in 1992 and was scientific assistant from 1993 to 1996 at the University of Freiburg. In 1996 he became associate professor at Bonn University and in 2001 full professor at Duisburg university. Since 2004 he is full professor for mathematics at Bonn University. He is studying Riemannian calculus on shape spaces with applications in imaging and in computer graphics. Furthermore, he is interested in microstructures in material science, in shape optimization and in adaptive finite element methods."
  },
  {
    "objectID": "logml2021/speakers2021/other/gabriele-steidl.html",
    "href": "logml2021/speakers2021/other/gabriele-steidl.html",
    "title": "Gabriele Steidl",
    "section": "",
    "text": "Home\n    Speakers\n    Gabriele Steidl\n\n  \n\n\n  \n \n \n  \n   \n  \n    \n     website\n  \n\n\n\nGabriele Steidl received her PhD and Habilitation in Mathematics from the University of Rostock (Germany), in 1988 and 1991, respectively. From 1992 to 1993 she worked as a consultant at the Verband Deutscher Rentenversicherungsträger in Frankfurt am Main. Between 1993 and 2020 she held professorships at the Departments of Mathematics at the TU Darmstadt, University of Mannheim, and TU Kaiserslautern, and was Consultant of the Fraunhofer Institute for Industrial Mathematics. Since 2020, she is Professor at the Department of Mathematics at the TU Berlin. She worked as a Postdoc at the University of Debrecen (Hungary), the Banach Center Warsaw and the University of Zürich and was a Visiting Professor at the ENS Paris/Cachan and the Université Paris East Marne-la-Vallée and the Sorbonne. Since 2020 she is a member of the DFG Fachkollegium Mathematik and the Program Director of SIAG-IS (SIAM). She is a member of the Editorial board of Journal of Mathematical Imaging and Vision, SIAM Journal on Imaging Sciences, The Journal of Fourier Analysis, Inverse Problems and Imaging, Journal of Optimization Theory and Applications, Transactions in Mathematics and its Applications, Acta Applicandae Mathematicae (ACAP), and Sampling Theory, Signal Processing and Data Analysis."
  },
  {
    "objectID": "logml2021/projects.html",
    "href": "logml2021/projects.html",
    "title": "Projects",
    "section": "",
    "text": "Characterising Universes in String Theory using Geometric Learning\n\n\n\n\n\n\nGraphs\n\n\nML\n\n\nGDL\n\n\n\n\n\n\nChallenger Mishra\n\n\n\n\n\n\n\n\n\n\n\n\nCoarsening disassortative graphs\n\n\n\n\n\n\nGraphs\n\n\nML\n\n\nGDL\n\n\n\n\n\n\nDaniele Grattarola\n\n\n\n\n\n\n\n\n\n\n\n\nEfficient Fully Fourier Spherical Convolutional Networks\n\n\n\n\n\n\nGraphs\n\n\nML\n\n\nGDL\n\n\n\n\n\n\nShubhendu Trivedi\n\n\n\n\n\n\n\n\n\n\n\n\nGeometric Learning on Shapes and Distributions with Optimal Transport\n\n\n\n\n\n\nGraphs\n\n\nML\n\n\nGDL\n\n\n\n\n\n\nJean Feydy\n\n\n\n\n\n\n\n\n\n\n\n\nGeometry of HMC and Geometric Integration for Sampling and Optimization\n\n\n\n\n\n\nGraphs\n\n\nML\n\n\nGDL\n\n\n\n\n\n\nAlessandro Barp\n\n\n\n\n\n\n\n\n\n\n\n\nImplicit Node and Edge Features for More Expressive Graph Neural Networks\n\n\n\n\n\n\nGraphs\n\n\nML\n\n\nGDL\n\n\n\n\n\n\nOctavian-Eugen Ganea\n\n\n\n\n\n\n\n\n\n\n\n\nImplicit planner GNNs for continuous control\n\n\n\n\n\n\nGraphs\n\n\nML\n\n\nGDL\n\n\n\n\n\n\nAndreea Deac\n\n\n\n\n\n\n\n\n\n\n\n\nImproved expressive power for message-passing networks via subgraph aggregation\n\n\n\n\n\n\nGraphs\n\n\nML\n\n\nGDL\n\n\n\n\n\n\nHaggai Maron\n\n\n\n\n\n\n\n\n\n\n\n\nInvestigating Differentiable Graph Module\n\n\n\n\n\n\nGraphs\n\n\nML\n\n\nGDL\n\n\n\n\n\n\nAnees Kazi\n\n\n\n\n\n\n\n\n\n\n\n\nManifold optimization and recent applications\n\n\n\n\n\n\nGraphs\n\n\nML\n\n\nGDL\n\n\n\n\n\n\nBamdev Mishra\n\n\n\n\n\n\n\n\n\n\n\n\nMorphing of manifold-valued images\n\n\n\n\n\n\nGraphs\n\n\nML\n\n\nGDL\n\n\n\n\n\n\nSebastian Neumayer\n\n\n\n\n\n\n\n\n\n\n\n\nMorphing of manifold-valued images\n\n\n\n\n\n\nGraphs\n\n\nML\n\n\nGDL\n\n\n\n\n\n\nMarie-Julie Rakotosaona\n\n\n\n\n\n\n\n\n\n\n\n\nNavigating text adventures with algorithmic reasoners\n\n\n\n\n\n\nGraphs\n\n\nML\n\n\nGDL\n\n\n\n\n\n\nPetar Veličković\n\n\n\n\n\n\n\n\n\n\n\n\nPlatonic CNNs\n\n\n\n\n\n\nGraphs\n\n\nML\n\n\nGDL\n\n\n\n\n\n\nTaco Cohen\n\n\n\n\n\n\n\n\n\n\n\n\nPretraining graph neural networks with ELECTRA\n\n\n\n\n\n\nGraphs\n\n\nML\n\n\nGDL\n\n\n\n\n\n\nWengong Jin\n\n\n\n\n\n\n\n\n\n\n\n\nSelf-supervised non-rigid correspondence by geodesic distortion minimization using the deformation field\n\n\n\n\n\n\nGraphs\n\n\nML\n\n\nGDL\n\n\n\n\n\n\nSøren Hauberg\n\n\n\n\n\n\n\n\n\n\n\n\nSelf-supervised non-rigid correspondence by geodesic distortion minimization using the deformation field\n\n\n\n\n\n\nGraphs\n\n\nML\n\n\nGDL\n\n\n\n\n\n\nOshri Halimi\n\n\n\n\n\n\n\n\n\n\n\n\nStability or Collapse: Topological Properties of Deep Autoencoders\n\n\n\n\n\n\nGraphs\n\n\nML\n\n\nGDL\n\n\n\n\n\n\nKelly Spendlove\n\n\n\n\n\n\n\n\n\n\n\n\nSurface reconstruction from point clouds\n\n\n\n\n\n\nGraphs\n\n\nML\n\n\nGDL\n\n\n\n\n\n\nRana Hanocka\n\n\n\n\n\n\n\n\n\n\n\n\nUncovering and correcting biases in neuroimaging studies\n\n\n\n\n\n\nGraphs\n\n\nML\n\n\nGDL\n\n\n\n\n\n\nIra Ktena\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Archives",
      "LOGML 2021",
      "Projects"
    ]
  },
  {
    "objectID": "logml2021/people2021/team/guada.html",
    "href": "logml2021/people2021/team/guada.html",
    "title": "Guadalupe Gonzalez",
    "section": "",
    "text": "Guadalupe is a machine learning scientist at Roche, in the Prescient Design team. She focuses on developing deep learning methods for a range of tasks within the drug discovery pipeline. She has previous experience on target discovery, and disease characterization. At Prescient, her focus is primarily on developing general deep learning methods for various tasks including molecule design, mode-of-action prediction, and structure elucidation from mass spectrometry data."
  },
  {
    "objectID": "logml2021/people2021/advisors/kevin.html",
    "href": "logml2021/people2021/advisors/kevin.html",
    "title": "Kevin Buzzard",
    "section": "",
    "text": "website"
  },
  {
    "objectID": "speakers/marinka.html",
    "href": "speakers/marinka.html",
    "title": "Marinka Zitnik",
    "section": "",
    "text": "Home\n    People\n    Marinka Zitnik\n\n  \n\n\n  \n \n \n  \n   \n  \n    \n     website\n  \n\n\n\nMarinka Zitnik is an Assistant Professor in the Department of Biomedical Informatics at Harvard Medical School, Associate Faculty at the Kempner Institute for the Study of Natural and Artificial Intelligence at Harvard University, Associate Member at the Broad Institute of MIT and Harvard, and Affiliated Faculty at the Harvard Data Science Initiative.\nArtificial intelligence is poised to enable breakthroughs in science and reshape medicine. Dr. Zitnik investigates foundations of AI to enhance scientific discovery and to realize individualized diagnosis and treatment. The overarching goal of her lab is to lay the foundations for AI that contribute to the scientific understanding of therapeutic design and genomic medicine or acquire such understanding autonomously.\nAI for Medicine: The state of a person is described with increasing precision incorporating modalities like genetic code, cellular atlases, molecular datasets, and therapeutics—the challenge is how to reason over these data to develop powerful disease diagnostics and empower new kinds of therapies. Our research creates new avenues for fusing knowledge and patient data to give the right patient the right treatment at the right time and have medicinal effects that are consistent from person to person and with results in the laboratory.\nAI for Science: For centuries, the method of discovery—the fundamental practice of science that scientists use to explain the natural world systematically and logically—has remained largely the same. We are using AI to change that. The natural world is interconnected, from the various facets of genome regulation to the molecular and organismal levels. These interactions across different levels yield a bewildering degree of complexity. Our research seeks to disentangle this complexity, developing AI models that advance drug design and help develop new kinds of therapies. Her lab is pioneering AI systems informed by geometry, structure, and symmetry, grounded in scientific knowledge. Current research directions in the lab include:\nLarge pre-trained AI to fuse data modalities like genetic code, single-cell atlases, molecular datasets, and therapeutics via multimodal knowledge graph networks and large pretrained language models Geometric deep learning and graph neural networks to reason about network biology and medicine Multi-scale, individualized, and contextualized AI to transfer prediction prowess acquired from one data type to another and to design contextually adaptive AI that can dynamically adjust outputs to biological contexts, such as patients, diseases, and cell types, in which they operate Better methods for drug design to enhance drug development across therapeutic modalities and stages of development Foundations for scientific discovery in the age of AI. Dr. Zitnik has founded Therapeutics Data Commons and is the faculty lead of the International AI4Science initiative.\nIn 2020 she organized the National Symposium on Drug Repurposing for Future Pandemics on behalf of the National Science Foundation (NSF)"
  },
  {
    "objectID": "speakers/estelle.html",
    "href": "speakers/estelle.html",
    "title": "Estelle Massart",
    "section": "",
    "text": "Home\n    Speakers\n    Estelle Massart\n\n  \n\n\n  \n \n \n  \n   \n  \n    \n     website\n  \n\n\n\nEstelle Massart received a PhD in mathematical engineering from UCLouvain in 2019. Her PhD thesis, entitled Data fitting on positive semidefinite matrix manifolds, was awarded the XXI Alston Householder prize in 2022. From 2020 to 2022, she was a postdoctoral researcher with the Mathematical Institute, at the University of Oxford, where she was funded by the National Physical Laboratory. She is currently an academic faculty member at UCLouvain, conducting her research in various topics in optimization and machine learning."
  },
  {
    "objectID": "speakers/coralia.html",
    "href": "speakers/coralia.html",
    "title": "Coralia Cartis",
    "section": "",
    "text": "Home\n    Speakers\n    Coralia Cartis\n\n  \n\n\n  \n \n \n  \n   \n  \n    \n     website\n  \n\n\n\nCoralia Cartis is Professor of Numerical Optimization at the Mathematical Institute, University of Oxford and a Fellow of Balliol College. She has been a Turing Fellow of the Alan Turing Institute for Data Science (2016–2024). She received a BSc degree in mathematics from Babesh-Bolyai University, Cluj-Napoca, Romania, and a PhD degree in mathematics from the University of Cambridge, under the supervision of Prof Michael J.D. Powell. Prior to her current post, she worked as a research scientist at Rutherford Appleton Laboratory and was a tenured assistant professor in the School of Mathematics, University of Edinburgh. Her research interests include the development and analysis of nonlinear optimization algorithms and diverse applications of optimization from climate modeling to signal processing and machine learning. She serves on the editorial boards of leading optimization and numerical analysis journals and was awarded some prizes for her research, such as the 2023 SIAM and EUROPT fellowships."
  },
  {
    "objectID": "speakers/francisco.html",
    "href": "speakers/francisco.html",
    "title": "Francisco J. R. Ruiz",
    "section": "",
    "text": "Home\n    Speakers\n    Francisco J. R. Ruiz\n\n  \n\n\n  \n \n \n  \n   \n  \n    \n     website\n  \n\n\n\nFrancisco J. R. Ruiz is a Research Scientist working at Google DeepMind in the Science Team. Before joining Google DeepMind, he was a Postdoctoral Research Scientist in the Department of Computer Science at Columbia University and in the Engineering Department at the University of Cambridge, where he held a Marie-Sklodowska Curie fellowship in the context of the E.U. Horizon 2020 program. He completed his Ph.D. and M.Sc. from the University Carlos III in Madrid. His research is focused on statistical machine learning and its applications to algorithmic discovery in Mathematics and Computer Science."
  },
  {
    "objectID": "speakers/barbara.html",
    "href": "speakers/barbara.html",
    "title": "Alice Barbara Tumpach",
    "section": "",
    "text": "Home\n    Speakers\n    Alice Barbara Tumpach\n\n  \n\n\n  \n \n \n  \n   \n  \n    \n     website\n  \n\n\n\nAlice Barbara Tumpach is Research Director at Wolfgang Pauli Institute (WPI), Vienna, and deputy head of the Mathematical Artificial Intelligence and Machine Learning Research Division of WPI, leading the Geometry-Informed Machine Learning Group. She is an expert on infinite-dimensional Differential Geometry, a research topic at the intersection of Geometry, Functional Analysis and Algebra, and a fundamental cornerstone for finite dimensional Geometry, Optimization, Probability and Mathematical Physics. Together with her collaborators, she develops news theoretical approaches to deal with group actions and their invariants, as well as new solutions for machine learning tasks in the presence of an infinite-dimensional group of symmetries, typically a diffeomorphism group or a gauge group. She is P.I. of several reseach projects including FWF-projects entitled “Geometric green Learning on groups and quotient spaces” , and “Banach Poisson-Lie Groups and integrable systems”. She is a reviewer for several international research agencies and for international mathematics and computer science journals. During her studies, she trained at the Ecole Normale Supérieure d’Ulm and the Ecole Polytechnique, France, where she obtained her doctorate. More recently, she has also acquired solid training in computer vision, gender studies and diversity management."
  },
  {
    "objectID": "speakers/tolga.html",
    "href": "speakers/tolga.html",
    "title": "Tolga Birdal",
    "section": "",
    "text": "Home\n    Speakers\n    Tolga Birdal\n\n  \n\n\n  \n \n \n  \n   \n  \n    \n     website\n  \n\n\n\nDr. Tolga Birdal is an assistant professor (Lecturer) and a UKRI Future Leaders Fellow in the Department of Computing of Imperial College London. Previously, he was a senior Postdoctoral Research Fellow at Stanford University within the Geometric Computing Group of Prof. Leonidas Guibas. Tolga has defended his masters and Ph.D. theses at the Computer Vision Group under Chair for Computer Aided Medical Procedures, Technical University of Munich led by Prof. Nassir Navab. He was also a Doktorand at Siemens AG under the supervision of Dr. Slobodan Ilic working on “Geometric Methods for 3D Reconstruction from Large Point Clouds”. His thesis was awarded the prestigious EMVA Young Professional Award. His current foci of interest involve topological / geometric machine learning and 3D computer vision. His more theoretical work is aimed at investigating and interrogating limits in geometric computing and non-Euclidean inference as well as principles of deep learning. Tolga has several publications at well-respected venues such as NeurIPS, CVPR, ICCV, ECCV, ICLR, T-PAMI, ICRA, IROS, ICASSP and 3DV. He is AC for CVPR, ICCV, ECCV and is currently program-chairing 3DV 2025. Aside from his academic life, Tolga has co-founded multiple companies including Befunky, a widely used web-based image editing platform."
  },
  {
    "objectID": "speakers/filippo.html",
    "href": "speakers/filippo.html",
    "title": "Filippo Maria Bianchi",
    "section": "",
    "text": "Home\n    Speakers\n    Filippo Maria Bianchi\n\n  \n\n\n  \n \n \n  \n   \n  \n    \n     website\n  \n\n\n\nFilippo Maria Bianchi is an Associate Professor in the Department of Mathematics and Statistics at UiT the Arctic University of Norway and a Senior Researcher at NORCE The Norwegian Research Centre. He serves as Vice-Chair of the IEEE Task Force on Learning for Structured Data, is a member of the ELLIS Society, the Graph Machine Learning Group in Lugano, and the IEEE Task Force on Reservoir Computing, and is a co-founder of the Northernmost Graph Machine Learning group.\nHis research lies at the intersection of machine learning, dynamical systems, and complex networks. Together with his team, he has developed state-of-the-art Graph Neural Network architectures and advanced deep learning models for time series and relational data, enabling the processing of complex spatio-temporal information. In applied settings, he primarily works on energy analytics and remote sensing."
  },
  {
    "objectID": "speakers/bastian.html",
    "href": "speakers/bastian.html",
    "title": "Bastian Rieck",
    "section": "",
    "text": "Home\n    Speakers\n    Bastian Rieck\n\n  \n\n\n  \n \n \n  \n   \n  \n    \n     website\n  \n\n\n\nBastian Rieck, M.Sc., Ph.D., is a Full Professor of Machine Learning at the University of Fribourg in Switzerland and the Principal Investigator of the AIDOS Lab, which focuses on developing novel machine learning methods driven by geometry and topology. His research has received multiple awards, including an ERC Starting Grant. Dr. Rieck is also a member of ELLIS, the European Laboratory for Learning and Intelligent Systems, and AI-LIFE, the International Network of Artificial Intelligence for the Life Sciences. Moreover, Dr. Rieck serves as the director of AATRN, the Applied Algebraic Topology Research Network. He received his M.Sc. degree in mathematics, as well as his Ph.D. in computer science, from Heidelberg University in Germany, graduating both times with distinction."
  },
  {
    "objectID": "speakers/marco.html",
    "href": "speakers/marco.html",
    "title": "Marco Cuturi",
    "section": "",
    "text": "Home\n    Speakers\n    Marco Cuturi\n\n  \n\n\n  \n \n \n  \n   \n  \n    \n     website\n  \n\n\n\nMarco Cuturi is a Research Scientist at Apple Machine Learning Research in Paris and a Professor of Statistics at CREST - ENSAE, Institut Polytechnique de Paris. His research interests include machine learning, optimal transport, optimization, time-series analysis, and kernel methods.\nDr. Cuturi earned his Ph.D. from École des Mines de Paris in November 2005. Following his doctoral studies, he held postdoctoral positions at the Institute of Statistical Mathematics in Tokyo and at Princeton University. He also worked in the financial industry for two years. From 2010 to 2016, he was an Associate Professor at the Graduate School of Informatics, Kyoto University. In 2016, he joined ENSAE, and between 2018 and 2022, he was part of the Google Brain team. In January 2022, he joined Apple, contributing to the Machine Learning Research team led by Samy Bengio.\nThroughout his career, Dr. Cuturi has made significant contributions to the field of optimal transport and its applications in machine learning. He has served as an associate editor for the Journal of Machine Learning Research (JMLR) and as a senior area chair for conferences such as NeurIPS and ICML. He has also been an area chair for AISTATS and ICLR."
  },
  {
    "objectID": "logml2021/people2021/advisors/ron.html",
    "href": "logml2021/people2021/advisors/ron.html",
    "title": "Ron Kimmel",
    "section": "",
    "text": "website\n  \n\n\n\nRon Kimmel is a Professor of Computer Science (and Electrical and Computer Engineering by courtesy) at the Technion where he holds the Montreal Chair in Sciences. He held a post-doctoral position at UC Berkeley and a visiting professorship at Stanford University. He has worked in various areas of shape reconstruction and analysis in computer vision, image processing, deep learning of big geometric data, and computer graphics. Kimmel’s interest in recent years has been understanding of machine learning, medical imaging and specifically computational oncology/pathology, precision medicine, optimization of solvers to problems with a geometric flavor, and applications of metric, spectral, Riemannian, and differential geometries. Kimmel is an IEEE Fellow and SIAM Fellow for his contributions to image processing, shape reconstruction and geometric analysis. He is the founder of the Geometric Image Processing Lab. and a founder and advisor of several successful image processing and analysis companies."
  },
  {
    "objectID": "logml2021/people2021/advisors/welling.html",
    "href": "logml2021/people2021/advisors/welling.html",
    "title": "Max Welling",
    "section": "",
    "text": "website\n  \n\n\n\nProf. Dr. Max Welling is a research chair in Machine Learning at the University of Amsterdam and a Distinguished Scientist at MSR. He is a fellow at the Canadian Institute for Advanced Research (CIFAR) and the European Lab for Learning and Intelligent Systems (ELLIS) where he also serves on the founding board. His previous appointments include VP at Qualcomm Technologies, professor at UC Irvine, postdoc at U. Toronto and UCL under supervision of prof. Geoffrey Hinton, and postdoc at Caltech under supervision of prof. Pietro Perona. He finished his PhD in theoretical high energy physics under supervision of Nobel laureate prof. Gerard ’t Hooft.\nMax Welling has served as associate editor in chief of IEEE TPAMI from 2011-2015, he serves on the advisory board of the Neurips foundation since 2015 and has been program chair and general chair of Neurips in 2013 and 2014 respectively. He was also program chair of AISTATS in 2009 and ECCV in 2016 and general chair of MIDL 2018. Max Welling is recipient of the ECCV Koenderink Prize in 2010 and the ICML Test of Time award in 2021. He directs the Amsterdam Machine Learning Lab (AMLAB) and co-directs the Qualcomm-UvA deep learning lab (QUVA) and the Bosch-UvA Deep Learning lab (DELTA)."
  },
  {
    "objectID": "logml2021/people2021/team/daniel.html",
    "href": "logml2021/people2021/team/daniel.html",
    "title": "Daniel Platt",
    "section": "",
    "text": "Daniel is a Chapman-Schmidt fellow at Imperial College London working on differential geometry and machine learning. He obtained his PhD from Imperial College London for his thesis on gauge theory in dimension 7 on problems that are motivated by string theory. Because of this, he is interested in big datasets of Calabi-Yau manifolds. Apart from this, Lie groups and  group actions are important for the kind of geometry that he studies and he is also interested in applications of differential geometry to group invariant machine learning."
  },
  {
    "objectID": "logml2021/people.html",
    "href": "logml2021/people.html",
    "title": "People",
    "section": "",
    "text": "Imperial College London\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrescient Design, Roche\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKing’s College London\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEigen Technologies\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImperial College London\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Archives",
      "LOGML 2021",
      "People"
    ]
  },
  {
    "objectID": "logml2021/people.html#organising-committee",
    "href": "logml2021/people.html#organising-committee",
    "title": "People",
    "section": "",
    "text": "Imperial College London\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrescient Design, Roche\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKing’s College London\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEigen Technologies\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImperial College London\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Archives",
      "LOGML 2021",
      "People"
    ]
  },
  {
    "objectID": "logml2021/people.html#scientific-advisory-committee",
    "href": "logml2021/people.html#scientific-advisory-committee",
    "title": "People",
    "section": "Scientific Advisory Committee",
    "text": "Scientific Advisory Committee\n\n\n\n\n\n\n\n\n\n\nKevin Buzzard\n\n\nImperial College London\n\n\n\n\n\n\n\n\n\n\n\n\n\nRon Kimmel\n\n\nTechnion – Israel Institute of Technology\n\n\n\n\n\n\n\n\n\n\n\n\n\nMax Welling\n\n\nUniversity of Amsterdam\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Archives",
      "LOGML 2021",
      "People"
    ]
  },
  {
    "objectID": "logml2021/speakers2021/other/niloy-mitra.html",
    "href": "logml2021/speakers2021/other/niloy-mitra.html",
    "title": "Niloy J. Mitra",
    "section": "",
    "text": "Home\n    Speakers\n    Niloy J. Mitra\n\n  \n\n\n  \n \n \n  \n   \n  \n    \n     website\n  \n\n\n\nNiloy Mitra is a Professor of Geometry Processing in the Department of Computer Science, University College London (UCL). He received his MS and PhD in Electrical Engineering from Stanford University under the guidance of Leonidas Guibas and Marc Levoy, and was a postdoctoral scholar with Helmut Pottmann at Technical University Vienna. His research interests include shape analysis, computational design and fabrication, and geometry processing. For details, please visit the SmartGeometryProcessing page. Niloy received the 2013 ACM Siggraph Significant New Researcher Award for “his outstanding work in discovery and use of structure and function in 3D objects” (UCL press release), the BCS Roger Needham award (BCS press release) in 2015, and the Eurographics Outstanding Technical Contributions Award in 2019. He received the ERC Starting Grant on SmartGeometry in 2013. His work has twice been featured as research highlights in the Communications of the ACM, twice been selected by ACM Siggraph/Siggraph Asia (both in 2017) for press release as research highlight. Besides research, Niloy is an active DIYer and loves reading, bouldering, and cooking."
  },
  {
    "objectID": "logml2021/speakers2021/other/maks-ovsjanikov.html",
    "href": "logml2021/speakers2021/other/maks-ovsjanikov.html",
    "title": "Maks Ovsjanikov",
    "section": "",
    "text": "Home\n    Speakers\n    Maks Ovsjanikov\n\n  \n\n\n  \n \n \n  \n   \n  \n    \n     website\n  \n\n\n\nMaks Ovsjanikov is a professor of computer science at Ecole Polytechnique. Prior to this, he worked for Google in their Image Search team in Mountain View. In 2011, he graduated from Stanford University with a Ph.D. in Computational Mathematics (from the ICME department), having done work on shape analysis in the geometric computing lab headed by Prof. Leonidas Guibas. His research interests are in geometric data analysis, and especially in the analysis and processing of deformable 3D shapes, with an emphasis on Deep Learning for non-rigid shape comparison and processing. Over the course of his career, Prof. Ovjsanikov introduced several key concepts in shape analysis, including the Heat Kernel Signature (cited over 1400 times, led to a Wikipedia article), algorithms for isometric shape matching, and Functional Maps. His research is the object of several international patents widely adopted in the industry. He received the Eurographics Young Researcher Award in 2014, and a Bronze medal from the CNRS in 2018."
  },
  {
    "objectID": "logml2021/speakers2021/other/thomas-kipf.html",
    "href": "logml2021/speakers2021/other/thomas-kipf.html",
    "title": "Thomas Kipf",
    "section": "",
    "text": "Home\n    Speakers\n    Thomas Kipf\n\n  \n\n\n  \n \n \n  \n   \n  \n    \n     website\n  \n\n\n\nThomas Kipf is a Research Scientist at Google Research in the Brain Team in Amsterdam. Prior to joining Google, he completed his PhD at University of Amsterdam under Prof. Max Welling on the topic “Deep Learning with Graph-Structured Representations”. His research interests lie in the area of relational learning and in developing models that can reason about the world in terms of structured abstractions such as objects or events."
  },
  {
    "objectID": "logml2021/speakers2021/other/michael-betancourt.html",
    "href": "logml2021/speakers2021/other/michael-betancourt.html",
    "title": "Michael Betancourt",
    "section": "",
    "text": "Home\n    Speakers\n    Michael Betancourt\n\n  \n\n\n  \n \n \n  \n   \n  \n    \n     website\n  \n\n\n\nMichael Betancourt is the principal research scientist with Symplectomorphic, LLC where he develops theoretical and methodological tools to support practical Bayesian inference. He is also a core developer of Stan, where he implements and tests these tools. In addition to hosting tutorials and workshops on Bayesian inference with Stan he also collaborates on analyses in epidemiology, pharmacology, and physics, amongst others. Before moving into statistics, Michael earned a B.S. from the California Institute of Technology and a Ph.D. from the Massachusetts Institute of Technology, both in physics."
  },
  {
    "objectID": "logml2021/speakers2021/other/marinka-zitnik.html",
    "href": "logml2021/speakers2021/other/marinka-zitnik.html",
    "title": "Marinka Zitnik",
    "section": "",
    "text": "Home\n    Speakers\n    Marinka Zitnik\n\n  \n\n\n  \n \n \n  \n   \n  \n    \n     website\n  \n\n\n\nMarinka Zitnik is an Assistant Professor at Harvard with appointments in the Department of Biomedical Informatics, Blavatnik Institute, Broad Institute of MIT and Harvard, and Harvard Data Science Initiative. Dr. Zitnik is a computer scientist studying applied machine learning with a focus on challenges brought forward by data in science, medicine, and health. She has published extensively on the topics of representation learning, knowledge graphs, network science, and graph ML in top-tier AI venues (NeurIPS, JMLR, IEEE TPAMI, KDD, ICLR), top-tier bioinformatics venues (Bioinformatics, ISMB, RECOMB), and journals (Nature Methods, Nature Communications, PNAS). Some of her methods are used by major institutions, including Baylor College of Medicine, Karolinska Institute, Stanford Medical School, and Massachusetts General Hospital. Her work received several best paper, poster, and research awards from the International Society for Computational Biology. She has recently been named a Rising Star in EECS by MIT and also a Next Generation in Biomedicine by the Broad Institute, being the only young scientist who received such recognition in both EECS and Biomedicine."
  },
  {
    "objectID": "logml2021/speakers2021/keynote/michael-bronstein.html",
    "href": "logml2021/speakers2021/keynote/michael-bronstein.html",
    "title": "Michael Bronstein",
    "section": "",
    "text": "website\n  \n\n\n\nScientist to the World Economic Forum, an honor bestowed on forty world’s leading scientists under the age of forty. Michael is a Fellow of IEEE and IAPR, alumnus of the Technion Excellence Program and the Academy of Achievement, ACM Distinguished Speaker, and a member of the Young Academy of Europe. In addition to academic work, Michael’s industrial experience includes technological leadership in multiple startup companies, including Novafora, Videocites, and Invision (acquired by Intel in 2012), and Fabula AI (acquired by Twitter in 2019). Following the acquisition of Fabula, he joined Twitter as Head of Graph Learning Research. He previously served as Principal Engineer at Intel Perceptual Computing (2012-2019) and was one of the key developers of the Intel RealSense 3D camera technology."
  },
  {
    "objectID": "logml2021/projects2021/project13/index.html",
    "href": "logml2021/projects2021/project13/index.html",
    "title": "Morphing of manifold-valued images",
    "section": "",
    "text": "Sebastian Neumayer is a researcher at TU Berlin in the final phase of his PhD under the supervision of Gabriele Steidl. Before coming to Berlin, he worked as a researcher at TU Kaiserslautern, where he also did his Bachelor and Master studies. His research interests are focused around image analysis with a special focus on motion models, manifold valued images and also inverse problems. Recently, he began to investigate the relation of such classical tasks with machine learning, which seems like a very promising direction of future research."
  },
  {
    "objectID": "logml2021/projects2021/project13/index.html#sebastian-neumayer",
    "href": "logml2021/projects2021/project13/index.html#sebastian-neumayer",
    "title": "Morphing of manifold-valued images",
    "section": "",
    "text": "Sebastian Neumayer is a researcher at TU Berlin in the final phase of his PhD under the supervision of Gabriele Steidl. Before coming to Berlin, he worked as a researcher at TU Kaiserslautern, where he also did his Bachelor and Master studies. His research interests are focused around image analysis with a special focus on motion models, manifold valued images and also inverse problems. Recently, he began to investigate the relation of such classical tasks with machine learning, which seems like a very promising direction of future research."
  },
  {
    "objectID": "logml2021/projects2021/project13/index.html#project",
    "href": "logml2021/projects2021/project13/index.html#project",
    "title": "Morphing of manifold-valued images",
    "section": "Project",
    "text": "Project\nIn this project, we deal with manifold-valued images, which occur, e.g., in DT-MRI (values are symmetric positive definite matrices) or material science (EBSD data describing grain orientation in materials). As the potential applications are becoming more and more, many classical imaging problems have also been investigated for the manifold setting. While some problems are easy to generalize, others require more attention. One specific issue popping up is that the values at each pixel are not rotation invariant anymore (rotating a point by 90 degrees changes nothing, rotating an arrow pointing in some direction by 90 degrees also changes its direction). Hence, some applications can not be tackled with the straight-forward extension of Euclidean transformation models. Originally, we have proposed a metamorphosis (or morphing) model for manifold-valued images which does note take this observation into account. Extending the model and deriving similar theoretical results as for the original one incorporating this observation will be the main goal of this project. Part of the project is devoted to the numerical implementation of the model. There is some Matlab Code available for the original version, but we can clearly also start from scratch in Python if you prefer. Being already a bit familiar with one of the available manifold toolboxes is definitely beneficial for the project."
  },
  {
    "objectID": "logml2021/projects2021/project15/index.html",
    "href": "logml2021/projects2021/project15/index.html",
    "title": "Improved expressive power for message-passing networks via subgraph aggregation",
    "section": "",
    "text": "Haggai is a Research Scientist at NVIDIA Research. His main fields of interest are machine learning, optimization and shape analysis. More specifically, he is working on applying deep learning to irregular domains (e.g., graphs, point clouds, and surfaces) and graph/shape matching problems. He completed his Ph.D. in 2019 at the Weizmann Institute of Science under the supervision of Prof. Yaron Lipman."
  },
  {
    "objectID": "logml2021/projects2021/project15/index.html#haggai-maron",
    "href": "logml2021/projects2021/project15/index.html#haggai-maron",
    "title": "Improved expressive power for message-passing networks via subgraph aggregation",
    "section": "",
    "text": "Haggai is a Research Scientist at NVIDIA Research. His main fields of interest are machine learning, optimization and shape analysis. More specifically, he is working on applying deep learning to irregular domains (e.g., graphs, point clouds, and surfaces) and graph/shape matching problems. He completed his Ph.D. in 2019 at the Weizmann Institute of Science under the supervision of Prof. Yaron Lipman."
  },
  {
    "objectID": "logml2021/projects2021/project15/index.html#project",
    "href": "logml2021/projects2021/project15/index.html#project",
    "title": "Improved expressive power for message-passing networks via subgraph aggregation",
    "section": "Project",
    "text": "Project\nOwing to their scalability and simplicity, message-passing neural networks (MPNNS) are currently the leading architecture for deep learning on graph-structured data. However, [1,2] recently showed that these architectures have limited expressive power. The most famous example is that MPNNs cannot distinguish a graph that consists of two triangles and a graph that consists of a single cycle of length 6 (see attached figure). This limitation raises a fundamental question: is it possible to make MPNNs more expressive? Several recent works [3,4,5] suggested architectures that aim to address this problem.\nIn this project, we will develop a novel approach that tackles this problem. The key to our solution is the observation that while two graphs may not be distinguishable by an MPNN, it might be easy to find distinguishable subgraphs. Following that observation, we suggest treating each graph as a set of subgraphs generated according to some predefined rule, e.g., all graphs that can be obtained by removing one edge from the original graph. We propose to process this set using the DSS framework [6], which allows processing sets of symmetric elements. To deal with the possible computational burden, we will also consider efficient random sampling schemes to improve time complexity. We will study this new model’s theoretical properties, e.g., is it provably more expressive than other MPNNS? and evaluate its practical performance.\n[1] Morris, Christopher, et al. “Weisfeiler and leman go neural: Higher-order graph neural networks.” Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 33. No. 01. 2019.\n[2] Xu, Keyulu, et al. “How powerful are graph neural networks?.” arXiv preprint arXiv:1810.00826 (2018).\n[3] Abboud, Ralph, et al. “The Surprising Power of Graph Neural Networks with Random Node Initialization.” arXiv preprint arXiv:2010.01179 (2020).\n[4] Vignac, Clément, Andreas Loukas, and Pascal Frossard. “Building powerful and equivariant graph neural networks with structural message-passing.” arXiv e-prints (2020): arXiv-2006.\n[5] Bouritsas, Giorgos, et al. “Improving graph neural network expressivity via subgraph isomorphism counting.” arXiv preprint arXiv:2006.09252 (2020).\n[6] Maron, Haggai, et al. “On learning sets of symmetric elements.” International Conference on Machine Learning. PMLR, 2020."
  },
  {
    "objectID": "logml2021/projects2021/project7/index.html",
    "href": "logml2021/projects2021/project7/index.html",
    "title": "Efficient Fully Fourier Spherical Convolutional Networks",
    "section": "",
    "text": "I am a machine learning researcher at the MIT Computer Science and Artificial Intelligence Laboratory (CSAIL), where I work with Professors Regina Barzilay and Tommi Jaakkola as part of the MIT Machine Learning for Pharmaceutical Discovery and Synthesis Consortium (MLPDS). Before this, between 2018-19, I was the NSF sponsored Institute Fellow at Brown University’s mathematics institute. I received my PhD in August 2018 for work on group-covariant neural networks, similarity learning and metric estimation."
  },
  {
    "objectID": "logml2021/projects2021/project7/index.html#shubhendu-trivedi",
    "href": "logml2021/projects2021/project7/index.html#shubhendu-trivedi",
    "title": "Efficient Fully Fourier Spherical Convolutional Networks",
    "section": "",
    "text": "I am a machine learning researcher at the MIT Computer Science and Artificial Intelligence Laboratory (CSAIL), where I work with Professors Regina Barzilay and Tommi Jaakkola as part of the MIT Machine Learning for Pharmaceutical Discovery and Synthesis Consortium (MLPDS). Before this, between 2018-19, I was the NSF sponsored Institute Fellow at Brown University’s mathematics institute. I received my PhD in August 2018 for work on group-covariant neural networks, similarity learning and metric estimation."
  },
  {
    "objectID": "logml2021/projects2021/project7/index.html#project",
    "href": "logml2021/projects2021/project7/index.html#project",
    "title": "Efficient Fully Fourier Spherical Convolutional Networks",
    "section": "Project",
    "text": "Project\nA particularly successful and instructive special case of group equivariant neural networks is when the input consists of images painted on a sphere and the transformation to which we desire invariance are 3D rotations, leading to work on spherical CNNs. Approaches to the design of spherical CNNs include combined real and harmonic space methods (such as [1] and [2]) and fully Fourier space approaches (such as [3]). The former exploit efficient sampling theorems which ensure that the underlying 3D rotational symmetry is captured properly. However, they also lead to a somewhat unconventional architecture composed of repeated forward and backward Fourier transforms since the non-linearity is still applied pointwise in “real space”. However applications of the non-linearity in this manner also lead to errors which break strict rotational equivariance. On the other hand, fully Fourier approaches, which maintain a harmonic space representation of the input to the end, afford strict rotational equivariance, but are prohibitively expensive. In this project we will explore methods for speeding up such a harmonic space approach while still maintaining exact rotational equivariance. In particular, we will examine the channel structure of the approach of [3] building further off the approach of [4] to produce more efficient implementations.\n[1] Taco Cohen, Mario Geiger, Jonas Kohler, and Max Welling. Spherical CNNs. International Conference on Learning Representations, 2018.\n[2] Carlos Esteves, Christine Allen-Blanchette, Ameesh Makadia, and Kostas Daniilidis. Learning SO(3) Equivariant Representations with Spherical CNNs. European Conference on Computer Vision, 2018.\n[3] Risi Kondor, Zhen Lin and Shubhendu Trivedi, Clebsch-Gordan Nets: A Fully Fourier Space Spherical Convolutional Neural Network. IAdvances in Neural Information Processing Systems, 2018.\n[4] Oliver J. Cobb, Christopher G. R. Wallis, Augustine N. Mavor-Parker, Augustin Marignier, Matthew A. Price, Mayeul d’Avezac, Jason D. McEwen, Efficient Generalized Spherical CNNs, International Conference on Learning Representations, 2021."
  },
  {
    "objectID": "logml2021/projects2021/project8/index.html",
    "href": "logml2021/projects2021/project8/index.html",
    "title": "Self-supervised non-rigid correspondence by geodesic distortion minimization using the deformation field",
    "section": "",
    "text": "Oshri Halimi is a Ph.D. student in the electrical engineering faculty at Technion - Israel Institute of Technology, supervised by Prof. Ron Kimmel.\nHer research investigates geometric invariants and their application in computer vision and shapes analysis. In particular, she is interested in the interface between geometry and deep learning.\nShe published in top-tier conferences for computer vision (CVPR, ECCV) and organized workshops in the field: “iGDL 2020: Israeli Geometric Deep Learning Workshop” and “Learning and Processing of Geometric Visual Structures,” SIAM Conference on Imaging Science (SIAM-IS20). She was awarded the Israel Ministry of Science Jabotinsky Fellowship for Doctoral Students.\nShe holds B.Sc in physics and electrical engineering from Technion, which she graduated cum laude. She is an alumna of the Technion Excellence Program, the Archimedes Program in Technion Faculty of Chemistry, and a bronze medalist in the IChO. She served in Unit 8200."
  },
  {
    "objectID": "logml2021/projects2021/project8/index.html#oshri-halimi",
    "href": "logml2021/projects2021/project8/index.html#oshri-halimi",
    "title": "Self-supervised non-rigid correspondence by geodesic distortion minimization using the deformation field",
    "section": "",
    "text": "Oshri Halimi is a Ph.D. student in the electrical engineering faculty at Technion - Israel Institute of Technology, supervised by Prof. Ron Kimmel.\nHer research investigates geometric invariants and their application in computer vision and shapes analysis. In particular, she is interested in the interface between geometry and deep learning.\nShe published in top-tier conferences for computer vision (CVPR, ECCV) and organized workshops in the field: “iGDL 2020: Israeli Geometric Deep Learning Workshop” and “Learning and Processing of Geometric Visual Structures,” SIAM Conference on Imaging Science (SIAM-IS20). She was awarded the Israel Ministry of Science Jabotinsky Fellowship for Doctoral Students.\nShe holds B.Sc in physics and electrical engineering from Technion, which she graduated cum laude. She is an alumna of the Technion Excellence Program, the Archimedes Program in Technion Faculty of Chemistry, and a bronze medalist in the IChO. She served in Unit 8200."
  },
  {
    "objectID": "logml2021/projects2021/project8/index.html#project",
    "href": "logml2021/projects2021/project8/index.html#project",
    "title": "Self-supervised non-rigid correspondence by geodesic distortion minimization using the deformation field",
    "section": "Project",
    "text": "Project\nGeodesic distortion minimization has been demonstrated as an effective approach for self-supervised non-rigid correspondence; see “Unsupervised learning of dense shape correspondence‏” (CVPR 2019).\nThere, shape descriptors were represented by a deep network, and the network was optimized to minimize the geodesic distortion criteria of the correspondence, resulting through the Functional Maps pipeline.\nA novel method to differentiate the geodesic distortion with respect to the deformation field was introduced later in the paper “Limp: Learning latent shape representations with metric preservation priors” (ECCV 2020).\nThe proposed project aims at combining the observations in both mentioned publications. The goal is to represent the deformation field using a deep network and to optimize it to admit the following requirements:\n\nGeodesic distance preservation of the deformation field\nOverlapping between the deformed source shape and the target shape."
  },
  {
    "objectID": "logml2021/projects2021/project6/index.html",
    "href": "logml2021/projects2021/project6/index.html",
    "title": "Coarsening disassortative graphs",
    "section": "",
    "text": "Daniele is a Ph.D. student at the University of Lugano (CH) in the Graph Machine Learning Group. He holds a MSc in Computer Science and Engineering from Politecnico di Milano. His research is on graph neural networks and their applications to systems that change over time. He is also the main developer of Spektral, a library for graph deep learning in TensorFlow/Keras.\nDaniele is the co-host and manager of Smarter Podcast, a live streaming podcast in Italian that invites AI researchers from academia and industry to talk about their work and experience."
  },
  {
    "objectID": "logml2021/projects2021/project6/index.html#daniele-grattarola",
    "href": "logml2021/projects2021/project6/index.html#daniele-grattarola",
    "title": "Coarsening disassortative graphs",
    "section": "",
    "text": "Daniele is a Ph.D. student at the University of Lugano (CH) in the Graph Machine Learning Group. He holds a MSc in Computer Science and Engineering from Politecnico di Milano. His research is on graph neural networks and their applications to systems that change over time. He is also the main developer of Spektral, a library for graph deep learning in TensorFlow/Keras.\nDaniele is the co-host and manager of Smarter Podcast, a live streaming podcast in Italian that invites AI researchers from academia and industry to talk about their work and experience."
  },
  {
    "objectID": "logml2021/projects2021/project6/index.html#project",
    "href": "logml2021/projects2021/project6/index.html#project",
    "title": "Coarsening disassortative graphs",
    "section": "Project",
    "text": "Project\nMany real-world applications require us to study large-scale graphs which are computationally expensive to process and difficult to understand intuitively.\nTo solve this issue, graph coarsening (or, sometimes, “pooling”) techniques allow us to reduce the size of a graph while preserving its overall characteristics. Many works have been proposed recently to tackle this problem, especially in the field of graph neural networks, but it remains a challenging and open research direction.\nMost techniques for graph coarsening assume that the input graph is assortative, that is, a graph in which neighboring nodes are likely to be similar. Conversely, many real-world graphs are disassortative graphs in which connections are heterophilous. For example, scale-free graphs (in which nodes with small degree are likely to be connected to nodes with a high degree) are very frequent in nature and technology.\nIn this project, we will attempt to develop a coarsening algorithm for strongly disassortative graphs. This will require us to discard many assumptions that are usually exploited in the design of coarsening algorithms, and chiefly the assumption that clusters of nodes can be identified in densely connected communities.\nFirst, we will study the effect of typical coarsening algorithms when applied to disassortative graphs. Then, we will formulate our coarsening problem as an optimization to obtain a desired degree of disassortativity in the coarse graph. Finally, we will identify one or more solutions to the problem, either through procedural techniques or end-to-end learning"
  },
  {
    "objectID": "logml2021/projects2021/project17/index.html",
    "href": "logml2021/projects2021/project17/index.html",
    "title": "Pretraining graph neural networks with ELECTRA",
    "section": "",
    "text": "Wengong Jin is a Ph.D. student in MIT CSAIL advised by Regina Barzilay and Tommi Jaakkola. His research seeks to develop novel machine learning algorithms for structured data and utilize them to automate molecular science such as drug discovery, material design, and chemistry. He is particularly interested in deep generative models and graph neural networks. His work has been published in top ML venues like ICML, NeurIPS, ICLR, as well as top biology journals like Cell."
  },
  {
    "objectID": "logml2021/projects2021/project17/index.html#wengong-jin",
    "href": "logml2021/projects2021/project17/index.html#wengong-jin",
    "title": "Pretraining graph neural networks with ELECTRA",
    "section": "",
    "text": "Wengong Jin is a Ph.D. student in MIT CSAIL advised by Regina Barzilay and Tommi Jaakkola. His research seeks to develop novel machine learning algorithms for structured data and utilize them to automate molecular science such as drug discovery, material design, and chemistry. He is particularly interested in deep generative models and graph neural networks. His work has been published in top ML venues like ICML, NeurIPS, ICLR, as well as top biology journals like Cell."
  },
  {
    "objectID": "logml2021/projects2021/project17/index.html#project",
    "href": "logml2021/projects2021/project17/index.html#project",
    "title": "Pretraining graph neural networks with ELECTRA",
    "section": "Project",
    "text": "Project\nMolecular property prediction is an important task in cheminformatics. Current property prediction methods are based on graph neural networks and require a large amount of training data to achieve state-of-the-art performance. Unfortunately, most datasets in cheminformatic domains are small (e.g., less than 1000). On the other hand, pretraining methods have achieved great success in computer vision and natural language processing. In this project, we seek to investigate how to pretrain graph neural networks on a large collection of unlabeled molecules using ELECTRA (Clark et al., 2020). The goal is to learn a masked language model to generate corrupted molecules and train a discriminator to distinguish the real molecules from the fake molecules. The method will be evaluated on MoleculeNet benchmark (Wu et al., 2017) to test its empirical performance."
  },
  {
    "objectID": "logml2021/projects2021/project11/index.html",
    "href": "logml2021/projects2021/project11/index.html",
    "title": "Investigating Differentiable Graph Module",
    "section": "",
    "text": "Anees is currently a PhD student at the chair of Computer Aided Medical Procedure and Augmented Reality @ the Technical University of Munich working with Prof. Nassir Navab. Anees has worked on deep learning for medical applications with the main focus is on Geometric Deep Learning for Healthcare by providing solutions to brain-related disease diagnosis problems. She has worked towards solving technical challenges such a dealing with multiple graph scenario, graph structure heterogeneity. In 2019, Anees was awarded TUM-ICL incentive funding to collaborate with Prof. Michael Bronstein at Imperial College London. In this project, the team focused on the challenging problem of graph structure learning.\nHis research interest lie in the span of geometry and statistics. He develops machine learning techniques using geometric constructions, and works on the related numerical challenges. He is particularly interested in random geometries as they naturally appear in learning."
  },
  {
    "objectID": "logml2021/projects2021/project11/index.html#anees-kazi",
    "href": "logml2021/projects2021/project11/index.html#anees-kazi",
    "title": "Investigating Differentiable Graph Module",
    "section": "",
    "text": "Anees is currently a PhD student at the chair of Computer Aided Medical Procedure and Augmented Reality @ the Technical University of Munich working with Prof. Nassir Navab. Anees has worked on deep learning for medical applications with the main focus is on Geometric Deep Learning for Healthcare by providing solutions to brain-related disease diagnosis problems. She has worked towards solving technical challenges such a dealing with multiple graph scenario, graph structure heterogeneity. In 2019, Anees was awarded TUM-ICL incentive funding to collaborate with Prof. Michael Bronstein at Imperial College London. In this project, the team focused on the challenging problem of graph structure learning.\nHis research interest lie in the span of geometry and statistics. He develops machine learning techniques using geometric constructions, and works on the related numerical challenges. He is particularly interested in random geometries as they naturally appear in learning."
  },
  {
    "objectID": "logml2021/projects2021/project11/index.html#project",
    "href": "logml2021/projects2021/project11/index.html#project",
    "title": "Investigating Differentiable Graph Module",
    "section": "Project",
    "text": "Project\nGraph deep learning has recently emerged as a powerful technique especially in the medical domain. Graph-based methods have shown promising results on a broad spectrum of applications ranging from social science, biomedicine, and particle physics to computer vision, graphics, and chemistry. One of the current problems with most of the GCN based methods is the requirement of a pre-computed graph. However, in many scenarios, especially in the medical and healthcare domain, the graph may not be given.\nIn this project, we will explore how such a graph can be learned mainly for medical applications. Given the population of individuals, we focus on predicting the age and gender of each based on brain MRI features. The method is inspired by our recent work on the differentiable graph module [1, 2] which can be further extended to any problem where the graph is unknown and can be inferred from the input features. With this project, you get to learn how to define a graph learning problem in the non-/ medical domain and further collaboration with a brain mesh analysis project.\n[1] Kazi, A., Cosmo, L., Navab, N. and Bronstein, M., 2020. Differentiable graph module (dgm) graph convolutional networks. arXiv preprint arXiv:2002.04999.\n[2] Cosmo, L., Kazi, A., Ahmadi, S.A., Navab, N. and Bronstein, M., 2020, October. Latent-Graph Learning for Disease Prediction. In International Conference on Medical Image Computing and Computer-Assisted Intervention (pp. 643-653). Springer, Cham."
  },
  {
    "objectID": "logml2021/projects2021/project20/index.html",
    "href": "logml2021/projects2021/project20/index.html",
    "title": "Platonic CNNs",
    "section": "",
    "text": "Taco Cohen is a machine learning research scientist at Qualcomm AI Research in Amsterdam and a PhD student at the University of Amsterdam, supervised by prof. Max Welling. He was a co-founder of Scyfer, a company focussed on active deep learning, acquired by Qualcomm in 2017. He holds a BSc in theoretical computer science from Utrecht University and a MSc in artificial intelligence from the University of Amsterdam (both cum laude). His research is focussed on understanding and improving deep representation learning, in particular learning of equivariant and disentangled representations, data-efficient deep learning, learning on non-Euclidean domains, and applications of group representation theory and non-commutative harmonic analysis, as well as deep learning based source compression. He has done internships at Google Deepmind (working with Geoff Hinton) and OpenAI. He received the 2014 University of Amsterdam thesis prize, a Google PhD Fellowship, ICLR 2018 best paper award for “Spherical CNNs”, and was named one of 35 innovators under 35 in Europe by MIT in 2018."
  },
  {
    "objectID": "logml2021/projects2021/project20/index.html#taco-cohen",
    "href": "logml2021/projects2021/project20/index.html#taco-cohen",
    "title": "Platonic CNNs",
    "section": "",
    "text": "Taco Cohen is a machine learning research scientist at Qualcomm AI Research in Amsterdam and a PhD student at the University of Amsterdam, supervised by prof. Max Welling. He was a co-founder of Scyfer, a company focussed on active deep learning, acquired by Qualcomm in 2017. He holds a BSc in theoretical computer science from Utrecht University and a MSc in artificial intelligence from the University of Amsterdam (both cum laude). His research is focussed on understanding and improving deep representation learning, in particular learning of equivariant and disentangled representations, data-efficient deep learning, learning on non-Euclidean domains, and applications of group representation theory and non-commutative harmonic analysis, as well as deep learning based source compression. He has done internships at Google Deepmind (working with Geoff Hinton) and OpenAI. He received the 2014 University of Amsterdam thesis prize, a Google PhD Fellowship, ICLR 2018 best paper award for “Spherical CNNs”, and was named one of 35 innovators under 35 in Europe by MIT in 2018."
  },
  {
    "objectID": "logml2021/projects2021/project20/index.html#project",
    "href": "logml2021/projects2021/project20/index.html#project",
    "title": "Platonic CNNs",
    "section": "Project",
    "text": "Project\nOmnidirectional signals occur in a wide variety of domains, from climate and weather science to astronomy and omnidirectional computer vision. Neural networks for omnidirectional data should respect the topological and geometric structure and symmetries of the signal domains (typically a sphere-like manifold). Many kinds of spherical CNNs have been developed, but typically these involve non-standard and costly operations that may be hard to implement. By contrast, the gauge equivariant Icosahedral CNN (1) is implemented by performing a standard conv2d over a collection of local charts concatenated into a feature map. One downside of the method is that it requires projecting the spherical signal to the icosahedron. On the other hand, the method is very efficient, and exactly equivariant to the rotational symmetries of the icosahedron."
  },
  {
    "objectID": "logml2021/projects2021/project3/index.html",
    "href": "logml2021/projects2021/project3/index.html",
    "title": "Navigating text adventures with algorithmic reasoners",
    "section": "",
    "text": "Petar Veličković is a Senior Research Scientist at DeepMind. He holds a PhD in Computer Science from the University of Cambridge (Trinity College), obtained under the supervision of Pietro Liò. His research interests involve devising neural network architectures that operate on nontrivially structured data (such as graphs), and their applications in algorithmic reasoning and computational biology. He has published relevant research in these areas at both machine learning venues (NeurIPS, ICLR, ICML-W) and biomedical venues and journals (Bioinformatics, PLOS One, JCB, PervasiveHealth). In particular, he is the first author of Graph Attention Networks—a popular convolutional layer for graphs—and Deep Graph Infomax—a scalable local/global unsupervised learning pipeline for graphs (featured in ZDNet). Further, his research has been used in substantially improving the travel-time predictions in Google Maps (covered by outlets including the CNBC, Endgadget, VentureBeat, CNET, the Verge and ZDNet)."
  },
  {
    "objectID": "logml2021/projects2021/project3/index.html#petar-veličković",
    "href": "logml2021/projects2021/project3/index.html#petar-veličković",
    "title": "Navigating text adventures with algorithmic reasoners",
    "section": "",
    "text": "Petar Veličković is a Senior Research Scientist at DeepMind. He holds a PhD in Computer Science from the University of Cambridge (Trinity College), obtained under the supervision of Pietro Liò. His research interests involve devising neural network architectures that operate on nontrivially structured data (such as graphs), and their applications in algorithmic reasoning and computational biology. He has published relevant research in these areas at both machine learning venues (NeurIPS, ICLR, ICML-W) and biomedical venues and journals (Bioinformatics, PLOS One, JCB, PervasiveHealth). In particular, he is the first author of Graph Attention Networks—a popular convolutional layer for graphs—and Deep Graph Infomax—a scalable local/global unsupervised learning pipeline for graphs (featured in ZDNet). Further, his research has been used in substantially improving the travel-time predictions in Google Maps (covered by outlets including the CNBC, Endgadget, VentureBeat, CNET, the Verge and ZDNet)."
  },
  {
    "objectID": "logml2021/projects2021/project3/index.html#project",
    "href": "logml2021/projects2021/project3/index.html#project",
    "title": "Navigating text adventures with algorithmic reasoners",
    "section": "Project",
    "text": "Project\nText-based adventure games, such as Zork, use natural language to describe their setup, receive actions from the player, and describe the effects of such actions. However, they typically also have a clear underlying geometric structure, which links different states (“rooms”) with each other as a result of taking actions (e.g. “moving”). This also endows them with a set of symmetries (e.g. it is often the case that going west, then going east, has the outcome of not changing the state). Hence, text adventures represent a fantastic test-bed for benchmarking agents on real-world noisy language data, while still grounding that data in a simplified underlying “world model”.\nNeural algorithmic reasoning (see Cappart et al., IJCAI’21; Section 3.3.) represents an emerging area of (graph) representation learning that seeks to emulate operations of classical algorithms and data structures within a high-dimensional latent space—allowing us to more directly execute classical algorithms over noisy data. A classical collection of algorithms shown to be efficiently learnable in this way are path-finding routines (such as the Bellman-Ford algorithm). Quickly solving text adventures typically involves (implicitly) constructing a map of the environment, determining the current and goal states in this map, then searching for a (shortest) path from the current state to the goal.\nAll of these components are tricky to do in a generic text adventure setting. To simplify, we will assume that the map is given to us. It is still non-trivial to map current and previous state descriptions to a particular current and goal state, and through the use of neural algorithmic reasoning, we will attempt to directly feed such natural language descriptions to an algorithm, and then use the output representations of that algorithm to find shortest paths to the required goal state.\nThe work will be based on the TextWorld environment, specifically the subset of coin-collecting environments proposed by Yuan, Côté et al. (EWRL’18), for which code and example agents are already available. While our focus will not necessarily be reinforcement learning, I anticipate that this will provide an excellent starting point. Neural path-finders will be trained by using the methodology proposed by Veličković et al. (ICLR’20) and deployed in the style of XLVIN (Deac et al., NeurIPS DeepRL’20)."
  },
  {
    "objectID": "logml2021/projects2021/project5/index.html",
    "href": "logml2021/projects2021/project5/index.html",
    "title": "Implicit planner GNNs for continuous control",
    "section": "",
    "text": "Andreea is a second year PhD student at Mila/University of Montreal with Prof Jian Tang. She is broadly interested in how learning can be improved through the use of graph representations, in particular in the context of reinforcement learning. She has previously worked on algorithmic alignment for implicit planning, as well as applications of graph representation learning to biotechnology, such as drug discovery and drug combinations."
  },
  {
    "objectID": "logml2021/projects2021/project5/index.html#andreea-deac",
    "href": "logml2021/projects2021/project5/index.html#andreea-deac",
    "title": "Implicit planner GNNs for continuous control",
    "section": "",
    "text": "Andreea is a second year PhD student at Mila/University of Montreal with Prof Jian Tang. She is broadly interested in how learning can be improved through the use of graph representations, in particular in the context of reinforcement learning. She has previously worked on algorithmic alignment for implicit planning, as well as applications of graph representation learning to biotechnology, such as drug discovery and drug combinations."
  },
  {
    "objectID": "logml2021/projects2021/project5/index.html#project",
    "href": "logml2021/projects2021/project5/index.html#project",
    "title": "Implicit planner GNNs for continuous control",
    "section": "Project",
    "text": "Project\nReal-world applications often require sequential decision making, and, in order to reason over longer time horizons, planning. One way of doing planning is by estimating the effects of actions and the values of the states. Then a plan can be constructed consisting of the actions which lead to overall highest possible value.\nFortunately, there is a known dynamic programming algorithm, Value Iteration (VI), which can find the optimal value function of a known Markov Decision Process (MDP). Even more, we know that the VI update is taking an expectation over the neighbouring states’ values, which is something a CNN can do in a grid, or, more generally, a graph neural network (GNN) in a graph. The GNN can then be used as part of the policy network. This has been directly validated in recent work, where GNNs have been successfully used to execute dynamic programming algorithms.\nIn this project, we will focus on the recently proposed eXecuted Latent Value Iteration Network (XLVIN) model [1], which provides us with one way to apply VI-style reasoning even if the underlying state graph is not known. While the XLVIN agent holds promise, it leaves several design decisions which have not been fully explored—perhaps the most interesting of which is generalising beyond discrete action spaces. By using a simple continuous control environment (such as LunarLander), we will first construct an XLVIN agent relying on simple MLP-based encoders. Then, we will take it one step further and bin the environment’s continuous action space, allowing for more fine-grained control.\nEventually, the number of bins may expand the computational budget of our planner (which normally applies an exhaustive breadth-first search strategy), so we will attempt various forms of exploratory planning. Ultimately, this can lead us to fully continuous action spaces, specified using, for example, a Gaussian distribution over various torque actions.\n[1] Deac A., Velickovic P., Milinkovic O., Bacon P., Tang J. et. al. 2020. “XLVIN: EXecuted Latent Value Iteration Nets.” http://arxiv.org/abs/2010.13146"
  },
  {
    "objectID": "sponsors.html",
    "href": "sponsors.html",
    "title": "Sponsors",
    "section": "",
    "text": "We gratefully acknowledge sponsorship from:"
  },
  {
    "objectID": "projects/Tancredi-Schettini-Gherardini.html",
    "href": "projects/Tancredi-Schettini-Gherardini.html",
    "title": "Looking for Einstein Metrics with Machine Learning",
    "section": "",
    "text": "Tancredi Schettini Gherardini is a final-year PhD in the Centre for Theoretical Physics at Queen Mary University of London, supervised by David Berman. His research is at the intersection of geometry (differential and algebraic) and machine learning, to tackle problems that arise in theoretical physics or pure mathematics."
  },
  {
    "objectID": "projects/Tancredi-Schettini-Gherardini.html#tancredi-schettini-gherardini",
    "href": "projects/Tancredi-Schettini-Gherardini.html#tancredi-schettini-gherardini",
    "title": "Looking for Einstein Metrics with Machine Learning",
    "section": "",
    "text": "Tancredi Schettini Gherardini is a final-year PhD in the Centre for Theoretical Physics at Queen Mary University of London, supervised by David Berman. His research is at the intersection of geometry (differential and algebraic) and machine learning, to tackle problems that arise in theoretical physics or pure mathematics."
  },
  {
    "objectID": "projects/Tancredi-Schettini-Gherardini.html#project",
    "href": "projects/Tancredi-Schettini-Gherardini.html#project",
    "title": "Looking for Einstein Metrics with Machine Learning",
    "section": "Project",
    "text": "Project\nEinstein metrics are ubiquitous in differential geometry, because of their unique properties, as well as in theoretical physics, since they correspond to solutions of Einstein’s equations in vacuum (with a cosmological constant). Although many examples are already known, finding new Einstein metrics constitutes a remarkably difficult problem and an active area of research.\nMachine learning has been used to find Einstein metrics numerically in very specific contexts in the last few years, but they all relied on some analytic ansatz which simplified the problem. A new semi-supervised machine learning package, called “AInstein”, was recently presented in [1], with the scope of finding solutions of Einstein’s equations in full generality, i.e. on any manifold. The generality of this code makes it applicable to a very large number of interesting cases. Namely, to all manifolds for which the existence of Einstein metrics is proven or conjectured without any clue on the analytic formula, as well as to all manifolds for which the existence of Einstein metrics has not been settled yet.\nIn this project, we will discuss all possible options and pick one (or more) specific differentiable manifold(s), hosting some open questions regarding Einstein metrics, with a suitable atlas that is appropriate for the semi-supervised machine learning approach; then, by leveraging AInstein, we will seek to obtain numerical results that contribute towards resolving these questions.\n[1] Hirst, E., Gherardini, T.S. and Stapleton, A.G., 2025. AInstein: Numerical Einstein Metrics via Machine Learning. arXiv preprint arXiv:2502.13043."
  },
  {
    "objectID": "projects/Caterina-Graziani.html",
    "href": "projects/Caterina-Graziani.html",
    "title": "Beyond VC Dimension - Rademacher Complexity for GNN Generalization",
    "section": "",
    "text": "Caterina Graziani is a Research Associate in Artificial Intelligence at the University of Siena, Italy. She earned a Master’s degree in Mathematics and then obtained her PhD in Artificial Intelligence at the University of Siena in 2024, under the supervision of Prof. Franco Scarselli, Prof. Monica Bianchini and Prof. Moreno Falaschi. In her PhD thesis, she studied the expressive power of GNNs beyond the Weisfeiler-Lehman test. During her PhD she spent four months at the Machine Learning Research Unit of the Technical University of Vienna under the supervision of Prof. Thomas Gartner.\nHer research interests lie in the Mathematics of Deep Learning, with a focus on expressivity, approximation, and generalization in Graph Neural Networks, as well as their bioinformatics applications. She received the Best Master’s Student Award in 2020 and has actively contributed to organizing scientific events such as the Learning on Graphs (LOG) Italian Meetup, the Young Applied Mathematicians Conference, and the AI4BA Summer School on Biomedical Applications of AI, among others. She is also involved in several outreach activities, organizing events like Pint of Science or Graph Learning on Wednesdays (GLOW) reading group on the latest advancements in Graph Learning."
  },
  {
    "objectID": "projects/Caterina-Graziani.html#caterina-graziani",
    "href": "projects/Caterina-Graziani.html#caterina-graziani",
    "title": "Beyond VC Dimension - Rademacher Complexity for GNN Generalization",
    "section": "",
    "text": "Caterina Graziani is a Research Associate in Artificial Intelligence at the University of Siena, Italy. She earned a Master’s degree in Mathematics and then obtained her PhD in Artificial Intelligence at the University of Siena in 2024, under the supervision of Prof. Franco Scarselli, Prof. Monica Bianchini and Prof. Moreno Falaschi. In her PhD thesis, she studied the expressive power of GNNs beyond the Weisfeiler-Lehman test. During her PhD she spent four months at the Machine Learning Research Unit of the Technical University of Vienna under the supervision of Prof. Thomas Gartner.\nHer research interests lie in the Mathematics of Deep Learning, with a focus on expressivity, approximation, and generalization in Graph Neural Networks, as well as their bioinformatics applications. She received the Best Master’s Student Award in 2020 and has actively contributed to organizing scientific events such as the Learning on Graphs (LOG) Italian Meetup, the Young Applied Mathematicians Conference, and the AI4BA Summer School on Biomedical Applications of AI, among others. She is also involved in several outreach activities, organizing events like Pint of Science or Graph Learning on Wednesdays (GLOW) reading group on the latest advancements in Graph Learning."
  },
  {
    "objectID": "projects/Caterina-Graziani.html#project",
    "href": "projects/Caterina-Graziani.html#project",
    "title": "Beyond VC Dimension - Rademacher Complexity for GNN Generalization",
    "section": "Project",
    "text": "Project\nWhile Graph Neural Networks (GNNs) have achieved significant success across various applications, several questions about their generalization capabilities remain open. Much of the theoretical analysis has relied on the Vapnik-Chervonenkis (VC) dimension, which, while offering valuable insights, often provides overly loose and pessimistic upper bounds. These limitations highlight the need for alternative measures that better capture the real-world generalization of GNNs. A promising alternative is Rademacher complexity, as explored by Garg et al. [3], which offers a more refined, data-dependent characterization of a model’s capacity to generalize. Unlike the VC dimension, which focuses on worst-case scenarios, Rademacher complexity provides a more practical and precise assessment.\nRecent works by Morris et al.[1] and D’Inverno et al.[2] have linked expressivity and generalization, deriving VC dimension bounds in terms of the color classes assigned by the 1-Weisfeiler-Lehman (1-WL) test. However, these results do not extend to Rademacher complexity, leaving an important gap in understanding how structural properties of graphs influence generalization.\nThis project seeks to bridge that gap by extending the generalization bounds established by [1] and [2] to Rademacher complexity in message-passing GNNs. By leveraging the known relationship between VC dimension and Rademacher complexity, we aim to develop tighter and more realistic bounds.\nA key aspect of this study is examining how the number of colors produced by the 1-WL test affects Rademacher complexity, thus connecting expressivity with generalization in a more refined manner. Additionally, generalization in GNNs is strongly influenced by graph topology. We will analyze the role of spectral properties, connectivity, and clustering coefficients, among others, in shaping Rademacher complexity and its implications for learning performance.\nThis research may lead to a categorization of graphs based on their Rademacher complexity, providing a systematic framework for understanding how graph structure impacts GNN generalization.\nAn experimental analysis will be conducted to validate the derived bounds ensuring their effectiveness in real-world scenarios.\n[1] Morris, C., Geerts, F., Tönshoff, J., & Grohe, M. (2023, July). Wl meet vc. In International Conference on Machine Learning (pp. 25275-25302). PMLR.\n[2] D’Inverno, G. A., Bianchini, M., & Scarselli, F. (2025). VC dimension of Graph Neural Networks with Pfaffian activation functions. Neural Networks, 182, 106924.\n[3] Garg, V., Jegelka, S., & Jaakkola, T. (2020, November). Generalization and representational limits of graph neural networks. In International Conference on Machine Learning (pp. 3419-3430). PMLR."
  },
  {
    "objectID": "projects/Edward-Hirst.html",
    "href": "projects/Edward-Hirst.html",
    "title": "Finite groups and the Cayley graph representation, such that ML can then help identify symmetry from generators",
    "section": "",
    "text": "Dr Hirst is a postdoctoral researcher at Queen Mary, University of London, specialising in machine learning methods for geometric datasets in theoretical physics. His interests include Calabi-Yau and G2-manifolds, cluster algebras and their computational generation, and more recently physical and information-geometric approaches to explain neural network architectures."
  },
  {
    "objectID": "projects/Edward-Hirst.html#edward-hirst",
    "href": "projects/Edward-Hirst.html#edward-hirst",
    "title": "Finite groups and the Cayley graph representation, such that ML can then help identify symmetry from generators",
    "section": "",
    "text": "Dr Hirst is a postdoctoral researcher at Queen Mary, University of London, specialising in machine learning methods for geometric datasets in theoretical physics. His interests include Calabi-Yau and G2-manifolds, cluster algebras and their computational generation, and more recently physical and information-geometric approaches to explain neural network architectures."
  },
  {
    "objectID": "projects/Edward-Hirst.html#project",
    "href": "projects/Edward-Hirst.html#project",
    "title": "Finite groups and the Cayley graph representation, such that ML can then help identify symmetry from generators",
    "section": "Project",
    "text": "Project\nCayley graphs provide a diagrammatic way of representing groups in terms of their generators. Despite being such an intuitive method to represent symmetry groups, their network properties are relatively unstudied. In this project, network analysis and machine learning techniques will be used for the first time to study new databases of Cayley graphs for finite groups we will generate; as well as truncated versions for infinite groups. Looking for correlations between network properties (such as cycle bases and centrality) and group properties (such as abelian and simple). Then extending to using machine learning architectures to predict these properties for groups, with the aim of distilling how these group properties are embedded in the graph combinatoric data through the architecture interpretability. This will involve using Neural Networks to classify the groups in the databases according to whether each group expresses that particular property, taking input as the adjacency matrix of a group’s Cayley graph. For properties which learn well we can take small neural network size limits to determine how many degrees of freedom are required to learn these properties, and if time can experiment with gradient saliency methods to determine which parts of the graphs are most important for determining each group property."
  },
  {
    "objectID": "projects/Estefania-Loayza-Romero.html",
    "href": "projects/Estefania-Loayza-Romero.html",
    "title": "Riemannian deep reinforcement learning for PDE-constrained shape optimisation",
    "section": "",
    "text": "Estefania Loayza Romero is a Lecturer in Mathematics and Statistics at the University of Strathclyde in Glasgow. Before this, she was a Chapman Fellow in the Department of Mathematics at Imperial College London and, prior to that, a postdoctoral researcher at the Cluster of Excellence Mathematics Münster, working in the Mathematical Optimisation group led by Prof. Benedikt Wirth. She completed her PhD at the University of Heidelberg under the supervision of Prof. Roland Herzog and earned her undergraduate and MSc degrees at Escuela Politécnica Nacional del Ecuador, where she was part of the Research Center on Mathematical Modelling (MODEMAT) under the guidance of Prof. Juan Carlos De los Reyes and Prof. Pedro Merino. Her research focuses on computational PDE-constrained optimisation, with particular interest in non-smooth optimisation algorithms and their applications, data assimilation, shape optimisation, and optimisation on Riemannian manifolds."
  },
  {
    "objectID": "projects/Estefania-Loayza-Romero.html#estefania-loayza-romero",
    "href": "projects/Estefania-Loayza-Romero.html#estefania-loayza-romero",
    "title": "Riemannian deep reinforcement learning for PDE-constrained shape optimisation",
    "section": "",
    "text": "Estefania Loayza Romero is a Lecturer in Mathematics and Statistics at the University of Strathclyde in Glasgow. Before this, she was a Chapman Fellow in the Department of Mathematics at Imperial College London and, prior to that, a postdoctoral researcher at the Cluster of Excellence Mathematics Münster, working in the Mathematical Optimisation group led by Prof. Benedikt Wirth. She completed her PhD at the University of Heidelberg under the supervision of Prof. Roland Herzog and earned her undergraduate and MSc degrees at Escuela Politécnica Nacional del Ecuador, where she was part of the Research Center on Mathematical Modelling (MODEMAT) under the guidance of Prof. Juan Carlos De los Reyes and Prof. Pedro Merino. Her research focuses on computational PDE-constrained optimisation, with particular interest in non-smooth optimisation algorithms and their applications, data assimilation, shape optimisation, and optimisation on Riemannian manifolds."
  },
  {
    "objectID": "projects/Estefania-Loayza-Romero.html#project",
    "href": "projects/Estefania-Loayza-Romero.html#project",
    "title": "Riemannian deep reinforcement learning for PDE-constrained shape optimisation",
    "section": "Project",
    "text": "Project\nDeep learning has quickly become a key driver of progress in healthcare, finance, automation, and many other fields, enabling systems to learn from data and make intelligent decisions. However, traditional methods struggle with non-Euclidean structures such as curves and surfaces. This is where non-Euclidean approaches become crucial, allowing models to better capture the true structure of complex data. While this project focuses primarily on the geometric properties of data, it also opens the possibility of exploring its topological and algebraic characteristics [1].\nShape optimisation seeks to identify geometries that enhance performance. In this project, we focus on applications involving partial differential equations (PDEs), which have a broad range of uses. For example, we might search for the most rigid structure within a given volume, optimise the distribution of material and voids to improve an electric motor’s performance, or use impedance tomography to detect inclusions in a material. These problems are typically solved using gradient-based optimisation methods, but they often converge slowly, making them computationally expensive. Our goal is to explore whether deep reinforcement learning can provide a more efficient solution, as proposed in [2]. However, their work relies on parameterised representations of shapes, such as splines or Bézier curves. Instead, we aim to introduce a Riemannian perspective, treating shape data as elements of a shape manifold.\nTo develop the necessary tools, we will first study the fundamentals of differential geometry and extend optimisation descent methods to work on Riemannian manifolds. This project will leverage the newly developed shape module from the Python package Geomstats [3], which provides tools for statistical learning on shape manifolds.\n[1] Sanborn, S., Mathe, J., Papillon, M., et al (2024). Beyond euclid: An illustrated guide to modern machine learning with geometric, topological, and algebraic structures. arXiv preprint arXiv:2407.09468.\n[2] Viquerat, J., Rabault, J., Kuhnle, A., Ghraieb, H., Larcher, A., & Hachem, E. (2021). Direct shape optimization through deep reinforcement learning. Journal of Computational Physics, 428, 110080.\n[3] Pereira, L. F., Brigant, A. L., Myers, A., Hartman, E., et al (2024). Learning from landmarks, curves, surfaces, and shapes in Geomstats. arXiv preprint arXiv:2406.10437."
  },
  {
    "objectID": "projects/Jiayi-Li.html",
    "href": "projects/Jiayi-Li.html",
    "title": "Symmetry, degeneracy and effective dimensions of neural networks",
    "section": "",
    "text": "Jiayi is a postdoctoral fellow at the Max Planck Institute and a member of the Harrington group. Her research lies at the intersection of algebraic geometry and theoretical machine learning, where she develops numerical algebro-geometric methods to analyze and characterize the optimization landscape of neural networks and creates machine-assisted tools for discovering equations and uncovering hidden structures of interest to the theoretical study of algebraic geometry. Prior to moving to Germany, she did her PhD at UCLA as part of the mathematical machine learning group. Jiayi is passionate about mathematics and machine learning, particularly in devising advanced mathematical techniques that illuminate the training dynamics and generalization behaviors of modern neural networks."
  },
  {
    "objectID": "projects/Jiayi-Li.html#jiayi-li",
    "href": "projects/Jiayi-Li.html#jiayi-li",
    "title": "Symmetry, degeneracy and effective dimensions of neural networks",
    "section": "",
    "text": "Jiayi is a postdoctoral fellow at the Max Planck Institute and a member of the Harrington group. Her research lies at the intersection of algebraic geometry and theoretical machine learning, where she develops numerical algebro-geometric methods to analyze and characterize the optimization landscape of neural networks and creates machine-assisted tools for discovering equations and uncovering hidden structures of interest to the theoretical study of algebraic geometry. Prior to moving to Germany, she did her PhD at UCLA as part of the mathematical machine learning group. Jiayi is passionate about mathematics and machine learning, particularly in devising advanced mathematical techniques that illuminate the training dynamics and generalization behaviors of modern neural networks."
  },
  {
    "objectID": "projects/Jiayi-Li.html#project",
    "href": "projects/Jiayi-Li.html#project",
    "title": "Symmetry, degeneracy and effective dimensions of neural networks",
    "section": "Project",
    "text": "Project\n“Local learning coefficient” (LLC), a concept introduced in singular learning theory have recently gained attention among the modern machine learning community. In neural network setting, the LLC roughly corresponds to how much a single weight affects the overall loss, and thus can be used to track the weights associated to a specific component of the architecture (e.g. attention head, MLP blocks) throughout training, to obtain an LLC curve for each component. Recently a work by Wang et al. looks into how attention heads specialize throughout the training of a 2-layer Transformer, and how this specialization is correlated with the “local learning coefficient” (LLC) around that attention head. Through theoretical and empirical investigations, the authors show that the LLC clusters and the attention specialization clusters are highly correlated, allowing LLC metrics to be used as more automated metrics for checking attention specialization. The work was spotlighted at ICLR 2025 for their innovative theoretical contribution. Since the field is relative new and open, many interesting open question invites investigation.\nFor this project, we would like to look into LLCs and Emergent Modularity - Deep neural networks, in particular Transformer architectures, often exhibit emergent modularity where certain neurons, layers, or subnetworks specialize in handling different subproblems (e.g., detecting shapes, translating certain token patterns, or handling different frequency signals). However, identifying and quantifying this modularity—especially in large-scale models—remains challenging. We propose to apply Local Learning Coefficient (LLC) as an interpretability measure that quantifies how strongly individual parameters influence the global loss. By aggregating and analyzing the LLC across neurons or sub-networks, we can potentially uncover self-organizing clusters of parameters that correspond to distinct functional modules, providing a quantitative lens to study when and how deep networks split into specialized substructures. We aim to achieve the following goals throughout the project:\n\nQuantify Emergent Modularity with LLC: Develop metrics or frameworks that aggregate LLC values to detect the formation of specialized modules (e.g., specialized neurons or heads).\nTrack Modularity Throughout Training: Investigate how modules emerge or dissolve over the course of training, and relate these changes to the network’s learning phases: early-phase rapid error reduction, mid-phase feature refinement, late-phase fine-tuning.\nCompare to Existing Approaches: Benchmark LLC-based modularity measures against other known methods (e.g., mutual information clustering, connectivity-based analyses, or hierarchical clustering on weights/activations).\nInvestigate Influence of Architecture & Hyperparameters: Determine how architecture choices and hyperparameters (e.g., depth, width, regularization strategies) affect emergent modularity patterns as measured by LLC.\nExplore Implications for Interpretability, Robustness, and Transfer.\n\nBy participating in this project, students not only gain hands-on empirical research experience with modern Transformer architectures but also explore the deeper mathematical underpinnings of the Local Learning Coefficient, which has roots in algebraic geometry. Even without a background in this area, they have the opportunity to engage with the elegance and rigor of advanced mathematics, seeing firsthand how a theoretical framework can guide and enrich practical machine learning experimentation.\n[1] Carroll, L., Hoogland, J., Farrugia-Roberts, M., & Murfet, D. (2025). Dynamics of Transient Structure in In-Context Linear Regression Transformers.\n[2] Wang, G., Hoogland, J., van Wingerden, S., Furman, Z., & Murfet, D. Differentiation and specialization of attention heads via the refined local learning coefficient, 2024.\n[3] Watanabe, S. (2018). Mathematical theory of Bayesian statistics."
  },
  {
    "objectID": "projects/Anthea-Monod-and-Omer-Bobrowski.html",
    "href": "projects/Anthea-Monod-and-Omer-Bobrowski.html",
    "title": "Cycle Matching for High-Dimensional Neural Activation Patterns",
    "section": "",
    "text": "Anthea Monod is a Senior Lecturer at the Department of Mathematics at Imperial College London. Her research focuses on problems intersecting algebraic topology and algebraic geometry with computation, statistics, machine learning, and data analysis.\nShe currently leads a group of over 20 researchers (postdocs, PhD students, MSc students and undergraduate research assistants). She is a Co-Director of the “Erlangen Programme” £10M EPSRC-funded Hub for Mathematical and Computational Foundations of Artificial Intelligence. She earned her PhD from the Swiss Federal Institute of Technology in Lausanne (EPFL). Prior to joining Imperial, she held postdoctoral and visiting faculty positions at the Technion–Israel Institute of Technology, Duke University, Columbia University in the City of New York, and Tel Aviv University."
  },
  {
    "objectID": "projects/Anthea-Monod-and-Omer-Bobrowski.html#anthea-monod",
    "href": "projects/Anthea-Monod-and-Omer-Bobrowski.html#anthea-monod",
    "title": "Cycle Matching for High-Dimensional Neural Activation Patterns",
    "section": "",
    "text": "Anthea Monod is a Senior Lecturer at the Department of Mathematics at Imperial College London. Her research focuses on problems intersecting algebraic topology and algebraic geometry with computation, statistics, machine learning, and data analysis.\nShe currently leads a group of over 20 researchers (postdocs, PhD students, MSc students and undergraduate research assistants). She is a Co-Director of the “Erlangen Programme” £10M EPSRC-funded Hub for Mathematical and Computational Foundations of Artificial Intelligence. She earned her PhD from the Swiss Federal Institute of Technology in Lausanne (EPFL). Prior to joining Imperial, she held postdoctoral and visiting faculty positions at the Technion–Israel Institute of Technology, Duke University, Columbia University in the City of New York, and Tel Aviv University."
  },
  {
    "objectID": "projects/Anthea-Monod-and-Omer-Bobrowski.html#omer-bobrowski",
    "href": "projects/Anthea-Monod-and-Omer-Bobrowski.html#omer-bobrowski",
    "title": "Cycle Matching for High-Dimensional Neural Activation Patterns",
    "section": "Omer Bobrowski",
    "text": "Omer Bobrowski\nOmer Bobrowski is a Professor in Mathematical Data Science at Queen Mary University of London. His research explores the intersection of probability theory and applied topology, with applications in statistics, data analysis, and machine learning.\nHe’s currently leading a research group of five members, including postdoctoral researchers and PhD students. He earned his PhD from the Technion – Israel Institute of Technology, and did his postdoctoral research at Duke University. Prior to joining QMUL, he was an Associate Professor at the Technion, where he is currently on leave."
  },
  {
    "objectID": "projects/Anthea-Monod-and-Omer-Bobrowski.html#project",
    "href": "projects/Anthea-Monod-and-Omer-Bobrowski.html#project",
    "title": "Cycle Matching for High-Dimensional Neural Activation Patterns",
    "section": "Project",
    "text": "Project\nTopological data analysis (TDA) is a field within data science that adapts algebraic topology to computation, data analysis, statistics, and machine learning, with a purpose to reveal intrinsic geometric features within complex datasets. This project will study the task of tracking such features in comparative settings, referred to as cycle matching (or cycle registration)—identifying topological structures that two datasets have in common—in machine learning.\nRecently, statistically and computationally amenable approaches to cycle matching have been proposed [1,2], which work well for low-dimensional, naturally aligned data (such as 2D slices of 3D images or time series of images). However, neural activation patterns challenge this framework: they are extremely high-dimensional, lack natural alignment, and often exhibit varying intrinsic dimensionalities.\nThis project will revisit the problem of cycle matching for neural network activations both in terms of its mathematical definition and its computational approach. The goal will be to develop new definitions and algorithms that are robust to the challenges posed by high-dimensional, unaligned point clouds geared towards a tractable tool to study the intricate structure of neural activations. We will proceed by concurrently tackling theoretical questions of redefining cycle matching as well as computational approaches on both synthetic data and real-world, publicly available activation data on state-of-the-art LLMs (Phi 3 3.8, Mistral 7B, LLaMA 8B) under adversarial influences [3].\nSuch a methodology would result in a twofold impact: (i) we will be able to track the flow of information across different layers of neural networks, providing insights into how models process and transform data and interpreting model latent space, which will have concrete utility in better understanding adversarial influences, for instance; and (ii) the increase in subsampling efficiency will lead to proved topological-statistical estimates while reducing computational overhead, which will be adaptable in full generality of applications beyond understanding how neural networks train and learn.\n[1] Y. Reani, O. Bobrowski. Cycle Registration in Persistent Homology with Applications to Topological Bootstrap. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(5): 5579–5593 (2023).\n[2] I. Garcia-Redondo, A. Monod, A. Song. Fast Topological Signal Identification and Persistent Cohomological Cycle Matching. Journal of Applied and Computational Topology, 8: 695–726 (2024).\n[3] A. Abdelnabi, A. Fay, G. Cherubin, A. Salem, M. Fritz, M. Paverd. Are you still on track!? Catching LLM Task Drift with Activations. arXiv:2406.00799."
  },
  {
    "objectID": "projects/Veronica-Lachi.html",
    "href": "projects/Veronica-Lachi.html",
    "title": "Towards a More Rigorous Evaluation of Hyperbolic Graph Representation Learning",
    "section": "",
    "text": "Veronica Lachi is a researcher in the Mobile and Social Computing Lab at Bruno Kessler Foundation in Trento. Her research interests focus on the theoretical properties of Graph Neural Networks, particularly their expressiveness, hierarchical pooling, and temporal GNNs. She obtained a PhD in Artificial Intelligence from the University of Siena under the supervision of Prof. Monica Bianchini. During her PhD, she was a Visiting Researcher at UiT The Arctic University of Tromsø."
  },
  {
    "objectID": "projects/Veronica-Lachi.html#veronica-lachi",
    "href": "projects/Veronica-Lachi.html#veronica-lachi",
    "title": "Towards a More Rigorous Evaluation of Hyperbolic Graph Representation Learning",
    "section": "",
    "text": "Veronica Lachi is a researcher in the Mobile and Social Computing Lab at Bruno Kessler Foundation in Trento. Her research interests focus on the theoretical properties of Graph Neural Networks, particularly their expressiveness, hierarchical pooling, and temporal GNNs. She obtained a PhD in Artificial Intelligence from the University of Siena under the supervision of Prof. Monica Bianchini. During her PhD, she was a Visiting Researcher at UiT The Arctic University of Tromsø."
  },
  {
    "objectID": "projects/Veronica-Lachi.html#project",
    "href": "projects/Veronica-Lachi.html#project",
    "title": "Towards a More Rigorous Evaluation of Hyperbolic Graph Representation Learning",
    "section": "Project",
    "text": "Project\nRecent advances in graph representation learning have introduced models that leverage hyperbolic space instead of traditional Euclidean spaces [1,2,3,4]. The motivation behind this shift stems from theoretical results which demonstrated that tree-like structures can be embedded in hyperbolic space with arbitrarily low distortion—an advantage that Euclidean space does not provide [5]. However, in [6] it has been shown that Euclidean models with comparable number of parameters can match or even surpass the performance of hyperbolic graph models. This suggests that the benchmark tasks used to justify hyperbolic methods may not be well-suited for distinguishing their benefits from those of standard Euclidean models. In other words, the choice of datasets and evaluation protocols may not accurately reflect the claimed advantages of hyperbolic embeddings. A key issue is the metric used to quantify the hyperbolicity of graph datasets. The widely used δ-hyperbolicity measure characterizes the entire graph structure but does not account for the interplay between node features and connectivity. Moreover, this measure is relatively coarse.\nThe goal of this project is to refine the evaluation of hyperbolic graph learning models by identifying a more informative metric for assessing dataset hyperbolicity. Specifically, we will (1) identify other metrics which provide a finer geometric characterization of graph structures by incorporating both topology and feature information (an example is Ollivier-Ricci curvature [7]); (2) computing and analyzing this curvature across multiple real-world graph datasets of varying sizes, identifying cases where hyperbolic embeddings are genuinely beneficial; (3) repeat the experiments presented in [5] on the selected datasets.\nBeyond this immediate objective, the project has the possibility of extension in future works. A long-term goal is to develop models capable of learning the most appropriate underlying manifold for a given dataset and task, rather than assuming a fixed geometric prior.\n[1] Ganea, Octavian, Gary Bécigneul, and Thomas Hofmann. “Hyperbolic neural networks.” Advances in neural information processing systems 31 (2018).\n[2] Chami, Ines, et al. “Hyperbolic graph convolutional neural networks.” Advances in neural information processing systems 32 (2019).\n[3] Shimizu, Ryohei, Yusuke Mukuta, and Tatsuya Harada. “Hyperbolic neural networks++.” arXiv preprint arXiv:2006.08210 (2020).\n[4] Zhang, Yiding, et al. “Lorentzian graph convolutional networks.” Proceedings of the web conference 2021. 2021.\n[5] Sarkar, Rik. “Low distortion Delaunay embedding of trees in hyperbolic plane.” International symposium on graph drawing. Berlin, Heidelberg: Springer Berlin Heidelberg, 2011.\n[6] Katsman, Isay, and Anna Gilbert. “Shedding Light on Problems with Hyperbolic Graph Learning.” Transactions on Machine Learning Research, 2024.\n[7] Lin, Yong, Linyuan Lu, and Shing-Tung Yau. “Ricci curvature of graphs.” Tohoku Mathematical Journal, Second Series 63.4 (2011): 605-627."
  },
  {
    "objectID": "projects/Michael-Murray.html",
    "href": "projects/Michael-Murray.html",
    "title": "Investigating Emergent Invariance and Sampling Thresholds in Hopfield Networks on Graph Orbit Datasets",
    "section": "",
    "text": "Michael Murrray is a Lecturer in Mathematics at the University of Bath. Prior to this he completed a postdoc at UCLA and received a DPhil from Oxford. His research is focused on developing theory for machine learning: in particular, understanding optimization, generalization and feature learning in the context of neural networks."
  },
  {
    "objectID": "projects/Michael-Murray.html#michael-murray",
    "href": "projects/Michael-Murray.html#michael-murray",
    "title": "Investigating Emergent Invariance and Sampling Thresholds in Hopfield Networks on Graph Orbit Datasets",
    "section": "",
    "text": "Michael Murrray is a Lecturer in Mathematics at the University of Bath. Prior to this he completed a postdoc at UCLA and received a DPhil from Oxford. His research is focused on developing theory for machine learning: in particular, understanding optimization, generalization and feature learning in the context of neural networks."
  },
  {
    "objectID": "projects/Michael-Murray.html#project",
    "href": "projects/Michael-Murray.html#project",
    "title": "Investigating Emergent Invariance and Sampling Thresholds in Hopfield Networks on Graph Orbit Datasets",
    "section": "Project",
    "text": "Project\nRecent experiments have shown classical Hopfield networks, trained via energy flow minimisation, can store the isomorphism class of a graph given vanishingly few examples. This observation suggests, at least in the case of certain graph or group structured datasets, that classical Hopfield networks possess a far greater capacity compared with their ability to store random data. Moreover, empirical findings indicate that the learned Hopfield parameters exhibit at least approximate graph isomorphism invariance and that only a vanishingly small sampling ratio is necessary for strict memorization of the full orbit. This raises a number of important questions: how does invariance emerge when minimizing the energy flow given only a few examples from the orbit? Which types of graph can be strictly memorized by these invariant Hopfield networks? What is the critical sampling ratio required for reliably storing the entire orbit and how does this ratio depend on the properties of the graph in question? In addition, the potential algorithmic significance of these results is underexplored, particularly in the context of solving challenging combinatorial, group and graph theoretic problems."
  },
  {
    "objectID": "projects/Kelly-Maggs.html",
    "href": "projects/Kelly-Maggs.html",
    "title": "On Depth in Geometric Deep Learning: Scaling Up Biomolecular Analysis Using Deep Neural k-Forms",
    "section": "",
    "text": "Kelly Maggs trained as a mathematician specialising in algebraic topology. After receiving his PhD in mathematics at EPFL under the supervision of Kathryn Hess, he is now working with Heather Harrington at the Max Planck Institute for Cell Biology and Genetics. His research is at the interface of pure mathematics, machine learning and computational biology.\nHis research interests in order of descending mathematical purity are:\n\nRational homotopy theory and model categories\nPersistence, TDA, discrete Morse theory, discrete exterior calculus, combinatorial Laplacians.\nGeometric DEEP LEARNING (the bold part should be yelled)\nSingle cell RNA sequencing."
  },
  {
    "objectID": "projects/Kelly-Maggs.html#kelly-maggs",
    "href": "projects/Kelly-Maggs.html#kelly-maggs",
    "title": "On Depth in Geometric Deep Learning: Scaling Up Biomolecular Analysis Using Deep Neural k-Forms",
    "section": "",
    "text": "Kelly Maggs trained as a mathematician specialising in algebraic topology. After receiving his PhD in mathematics at EPFL under the supervision of Kathryn Hess, he is now working with Heather Harrington at the Max Planck Institute for Cell Biology and Genetics. His research is at the interface of pure mathematics, machine learning and computational biology.\nHis research interests in order of descending mathematical purity are:\n\nRational homotopy theory and model categories\nPersistence, TDA, discrete Morse theory, discrete exterior calculus, combinatorial Laplacians.\nGeometric DEEP LEARNING (the bold part should be yelled)\nSingle cell RNA sequencing."
  },
  {
    "objectID": "projects/Kelly-Maggs.html#project",
    "href": "projects/Kelly-Maggs.html#project",
    "title": "On Depth in Geometric Deep Learning: Scaling Up Biomolecular Analysis Using Deep Neural k-Forms",
    "section": "Project",
    "text": "Project\nRecent breakthroughs in artificial intelligence have been largely driven by the surprising robustness of scaling laws, where increasing model size and depth yields significant performance gains. However, in geometric deep learning, traditional message-passing architectures fall short of this potential due to inherent issues like over-smoothing and over-squashing. In this project, will be designing and implementing neural k-form architectures—novel models that leverage differential geometry and algebraic topology to process higher-dimensional features–as a replacement of the underlying message-passing engine. Our goal is to design scalable geometric deep learning architectures via structures from geometry and topology, and to apply these models to real-world challenges in molecular property prediction on large datasets. This somewhat bold interdisciplinary initiative offers a unique opportunity to bridge the gap between pure mathematics and software engineering, pushing the boundaries of geometric deep learning with the goal of tackling some of the most difficult problems in computational biology.\nThis project is interdisciplinary and open to anyone, with any background material to be covered in the workshop. However, on the mathematical side, any experience with differential geometry, algebraic topology, algebraic geometry and/or homological algebra would be considered useful for generating new ideas and approaches. On the ML and engineering side, useful experience includes working with learning on geometric objects such as point clouds, graphs, manifolds, and simplicial complexes, particularly in applications related to computational biology.\n[1] Maggs, K., Hacker, C. and Rieck, B., 2023. Simplicial representation learning with neural $ k $-forms. arXiv preprint arXiv:2312.08515."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "\nLOGML Summer School 2025\n",
    "section": "",
    "text": "LOGML Summer School 2025\n\n\nLondon, 7-11 July 2025\n\n\nLOGML (London Geometry and Machine Learning) aims to bring together mathematicians and computer scientists to collaborate on a variety of problems at the intersection of geometry and machine learning. There will be a selection of group projects, each overseen by an experienced mentor, talks by leading figures in the field, a variety of social events and a company networking night.\n\n\n\n\n\n\n\nParticipant Applications (Open)\n\n\n\nApplications for participants are now open! If you are interested, please apply using this form.\nThe summer school is designed for early-career researchers, primarily PhD students, but Master’s students, postdoctoral researchers, and industry professionals are also welcome to apply. While Bachelor’s students may be considered, preference will be given to those at the Master’s level and above.\nFor any questions, please contact logml.committee@gmail.com.\nApplications close on April 6th, AoE (Anywhere on Earth).\n\n\n\n\n\n\n\n\nMentor Applications (Closed)\n\n\n\nMentors guide a small group of passionate early career researchers on a week-long project of their choosing at the intersection of geometry and machine learning.\nLOGML is not merely a summer school; it’s an incubator for innovation at the intersection of geometry and machine learning. Several working groups that started at LOGML went on to form longer-term collaborations leading to publications in notable conferences and journals, including:\n\nConnecting Neural Models Latent Geometries with Relative Geodesic Representations (Workshop on Symmetry and Geometry in Neural Representations, NeurIPS 2024)\nGroup-invariant machine learning on the Kreuzer-Skarke dataset (Physics Letters B, 2024)\nImplicit Convolutional Kernels for Steerable CNNs (NeurIPS, 2023 )\nAccelerating Molecular Graph Neural Networks via Knowledge Distillation (Synergy of Scientific and Machine Learning Modeling workshop, ICML, 2023)\nSurfing on the Neural Sheaf (Workshop on Symmetry and Geometry in Neural Representations, NeurIPS 2022)\nGeneralized Laplacian Positional Encoding for Graph Representation Learning  (Workshop on Symmetry and Geometry in Neural Representations, NeurIPS 2022)\nEquivariant Mesh Attention Networks (TMLR, 2022)\nUnsupervised Network Embedding Beyond Homophily (TMLR, 2022)\nTowards Training GNNs Using Explanation Directed Message Passing (LOG conference, 2022)\nEqivariant Subgraph Aggregation Networks (ICLR 2022, Spotlight)\n\nHere’s what you can anticipate:\n\nDeep dive into collaborative research: You’ll steer a group of typically five early-career researchers, working closely on a well-defined project. While 15 hours of the week are earmarked for project working time, the energy and enthusiasm often see groups dedicating more.\nDrive tangible outcomes: The LOGML experience is intensive, yet the time frame is concise. With only one week available, it’s essential to zero in on achievable milestones. In the past, mentors have found success in adapting existing algorithms to fresh datasets, implementing a pilot for a theoretical idea, or laying the theoretical foundations for a longer-term project.\nEngage & enlighten: Apart from the project work, the week will be filled with lectures by leading figures in the field, as well as a company and networking night.\n\nApplications close on February 16th, AoE (Anywhere on Earth).\n\n\n\nYou can find a list of previous years’ projects and speakers under “Archives” above."
  },
  {
    "objectID": "logml2022.html",
    "href": "logml2022.html",
    "title": "Readings, lectures, and videos",
    "section": "",
    "text": "Each class session has a set of required readings that you should complete before watching the lecture."
  },
  {
    "objectID": "program.html",
    "href": "program.html",
    "title": "Schedule",
    "section": "",
    "text": "The provisional schedule of the events is available here. The timezone is London time. The talks are held in LT 308 (third floor) in the Huxley building.\nYou can add the events to your calendar via iCal.\n\n\nJoan Bruna: Quantitative Benefits of Rank in Attention Layers (Slides)\nAbstract: Attention-based mechanisms are widely used in machine learning, most prominently in transformers and graph neural networks. However, hyperparameters such as the rank of the attention matrices and the number of heads are kept nearly the same in all realizations of this architecture, without any theoretical justification. While the total number of parameters ( a proxy for `capacity’ in the language of modern scaling laws) only depends on the product of rank and number of heads, in this talk we will show dramatic tradeoffs between these two parameters. Namely, we present a simple and natural target function that can be represented using a single full-rank attention head for any context length, but that provably cannot be approximated by low-rank attention unless the number of heads is exponential in the embedding dimension, even for small context lengths. Moreover, we prove that, for short context lengths, adding depth allows the target to be approximated by low-rank attention. For long contexts, we conjecture that full-rank attention is necessary. Joint work with Noah Amsel and Gilad Yehudai.\nKathryn Hess Bellwald: Cochains are all you need (Slides)\nAbstract: I will present results from the thesis of my recently graduated student Kelly Maggs, a diverse collection of fruitful applications of the classical algebra-topological notion of “cochains” in signal processing, machine learning, and gene expression analysis. The first of these applications concerns the interplay between discrete Morse theory and combinatorial Hodge theory. The second involves the use of differential k-forms in Euclidean space to represent simplices in a simplicial complex and thus facilitate interpretability and geometric consistency in geometric deep learning, without message passing. The last application leads to a pipeline for detecting closed biological processes of various types (e.g., the cell cycle) from single-cell RNA seq data, based on persistent cohomology and lead-lag theory for embedded simplicial complexes. Before sketching these applications, I will introduce the notion of cochains and explain how they encode the interplay between geometry and topology.\nVishnu Jejjala: Deep Learning Topology (Slides)\nPietro Lio: Diffusion and geometric models for medicine\nPeter Lu: Tutorial on Optimal Transport (Slides)\nAnthea Monod: A Tropical Geometric Perspective on Deep Learning\nIslem Rekik: The landscape of generative GNNs in network neuroscience\nMichalis Vazirgiannis: Multimodal Graph Generative AI and applications to biomedical domain\nGraph generative models are recently gaining significant interest in current application domains. They are commonly used to model social networks, knowledge graphs, molecules and proteins. In this talk we will present the potential of graph generative models and our recent relevant efforts in the biomedical domain. More specifically we present a novel architecture that generates medical records as graphs with privacy guarantees. We capitalize and modify the graph Variational autoencoders (VAEs) architecture. We train the generative model with the well known MIMIC medical database and achieve generated data that are very similar to the real ones yet provide privacy guarantees. We achieve there as well promising results with potential for future application in broader biomedical related tasks. Finally we present ongoing research directions for multi modal generative models involving graphs and applications to protein function text generation – the prot2text model."
  },
  {
    "objectID": "people/advisors/jure.html",
    "href": "people/advisors/jure.html",
    "title": "Jure Leskovec",
    "section": "",
    "text": "Home\n    People\n    Jure Leskovec\n\n  \n\n\n  \n \n \n  \n   \n  \n    \n     website\n  \n\n\n\nJure Leskovec is Professor of Computer Science at Stanford University. He is affiliated with the Stanford AI Lab, Machine Learning Group and the Center for Research on Foundation Models.\nIn the past, he served as a Chief Scientist at Pinterest and was an investigator at Chan Zuckerberg Biohub. Most recently, he co-founded machine learning startup Kumo.AI. Leskovec pioneered the field of Graph Neural Networks and co-authored PyG, the most widely-used graph neural network library. Research from his group has been used by many countries to fight COVID-19 pandemic, and has been incorporated into products at Facebook, Pinterest, Uber, YouTube, Amazon, and more.\nHis research received several awards including Microsoft Research Faculty Fellowship in 2011, Okawa Research award in 2012, Alfred P. Sloan Fellowship in 2012, Lagrange Prize in 2015, and ICDM Research Contributions Award in 2019. His research contributions have spanned social networks, data mining and machine learning, and computational biomedicine with the focus on drug discovery. His work has won 12 best paper awards and 5 10-year test of time awards at premier venues in these research areas. Leskovec received his bachelor’s degree in computer science from University of Ljubljana, Slovenia, PhD in machine learning from Carnegie Mellon University and postdoctoral training at Cornell University."
  },
  {
    "objectID": "people/advisors/heather.html",
    "href": "people/advisors/heather.html",
    "title": "Heather Harrington",
    "section": "",
    "text": "Home\n    People\n    Heather Harrington\n\n  \n\n\n  \n \n \n  \n   \n  \n    \n     website\n  \n\n\n\nHeather Harrington is a director at the Max Planck Institute of Molecular Cell Biology and Genetics, where she also leads the interinstitutional Center for Systems Biology Dresden (CSBD) in collaboration with partners from the Technical University Dresden and the Max Planck Institute for the Physics of Complex Systems. She is also an Honorary Professor at Technical University of Dresden and Professor of Mathematics at Oxford.\nShe is a mathematician with expertise in the mathematical foundations of Topological Data Analysis and machine learning. She investigates topological methods suitable for analysis of high-dimensional biological data, as well as the mathematical foundations of AI. Her research focuses on the problem of reconciling models and data by extracting information about the structure of models and the shape of data. She combines techniques from a variety of disciplines such as computational algebraic geometry and computational topology, statistics, optimization, network theory, and systems biology.\nShe has received several honors for her contributions to mathematics, including the Whitehead Prize from the London Mathematical Society and the 2019 Adams Prize from the University of Cambridge (co-awarded). In 2020, she was recognized with the Philip Leverhulme Prize for advances in analysis of noisy data."
  },
  {
    "objectID": "people/advisors/bronstein.html",
    "href": "people/advisors/bronstein.html",
    "title": "Michael Bronstein",
    "section": "",
    "text": "Home\n    People\n    Michael Bronstein \n\n  \n\n\n  \n \n \n  \n   \n  \n    \n     website\n  \n\n\n\nMichael Bronstein is the DeepMind Professor of AI at the University of Oxford. He was previously Head of Graph Learning Research at Twitter, a professor at Imperial College London and held visiting appointments at Stanford, MIT, and Harvard. He has been affiliated with three Institutes for Advanced Study (at TUM as a Rudolf Diesel Fellow (2017-2019), at Harvard as a Radcliffe fellow (2017-2018), and at Princeton as a short-time scholar (2020)). Michael received his PhD from the Technion in 2007. He is the recipient of the EPSRC Turing AI World Leading Research Fellowship, Royal Society Wolfson Research Merit Award, Royal Academy of Engineering Silver Medal, five ERC grants, two Google Faculty Research Awards, and two Amazon AWS ML Research Awards. He is a Member of the Academia Europaea, Fellow of IEEE, IAPR, BCS, and ELLIS, ACM Distinguished Speaker, and World Economic Forum Young Scientist. In addition to his academic career, Michael is a serial entrepreneur and founder of multiple startup companies, including Novafora, Invision (acquired by Intel in 2012), Videocites, and Fabula AI (acquired by Twitter in 2019)."
  },
  {
    "objectID": "people/team/rahul.html",
    "href": "people/team/rahul.html",
    "title": "Rahul Singh",
    "section": "",
    "text": "Rahul is a Wu Tsai postdoctoral fellow at Yale University. He received his PhD in Machine Learning under the mentorship of Yongxin Chen from Georgia Institute of Technology. His research interests are in the areas of signal processing, graph neural networks, and machine learning applications in bioinformatics and neuroscience. He enjoys hiking, camping, and exploring nature and is also an AC Milan fan."
  },
  {
    "objectID": "people/team/pragya.html",
    "href": "people/team/pragya.html",
    "title": "Pragya Singh",
    "section": "",
    "text": "Pragya recently finished her masters in Artificial Intelligence at Imperial College London. Prior to that, she finished her undergraduate in Engineering Physics from IIT Roorkee and worked as a quantitative researcher at Goldman Sachs. Her research interests include geometric and topological machine learning, especially their applications to natural sciences."
  },
  {
    "objectID": "people/team/zhengang.html",
    "href": "people/team/zhengang.html",
    "title": "Zhengang Zhong",
    "section": "",
    "text": "Zhengang Zhong is a postdoctoral researcher at the University of Warwick. Prior to joining Warwick, he completed his PhD at Imperial College London and received the Diplom degree from the Technical University of Dresden. His research is about learning and decision-making under uncertainty, focusing on optimal control and variational problems on graphs."
  },
  {
    "objectID": "people/team/vincenzo.html",
    "href": "people/team/vincenzo.html",
    "title": "Vincenzo Marco De Luca",
    "section": "",
    "text": "Marco is a PhD student in the Department of Information Engineering and Computer Science at the University of Trento, supervised by Prof. Andrea Passerini, Prof. Giovanna Varni, and Prof. Marco Zenato. His research is dedicated to advancing Human-Centered AI, focusing on applications that model Social Interaction and Human-Machine Teaming. His research focuses on Graph Neural Networks to explore three areas: the design of trustworthy AI systems, the ability to learn effectively from limited data, and the modeling of complex relational dynamics."
  },
  {
    "objectID": "people/team/max.html",
    "href": "people/team/max.html",
    "title": "Massimiliano Esposito",
    "section": "",
    "text": "Max is a research scientist at IBM. He holds a PhD from Imperial College, with a thesis in harmonic analysis on the non-linear quantisation of pseudo-differential operators. His research at IBM focuses on AI for science, with a particular emphasis on AI for quantum computing, AI for nuclear fusion, and AI for materials discovery. In his spare time, he enjoys football, swimming, and considering how AI can improve maths discoveries and thinking."
  },
  {
    "objectID": "logml2022/speakers2022/other/petar-velickovic.html",
    "href": "logml2022/speakers2022/other/petar-velickovic.html",
    "title": "Petar Veličković",
    "section": "",
    "text": "website\n  \n\n\n\nPetar Veličković is a Senior Research Scientist at DeepMind. He holds a PhD in Computer Science from the University of Cambridge (Trinity College), obtained under the supervision of Pietro Liò. His research interests involve devising neural network architectures that operate on nontrivially structured data (such as graphs), and their applications in algorithmic reasoning and computational biology. He has published relevant research in these areas at both machine learning venues (NeurIPS, ICLR, ICML-W) and biomedical venues and journals (Bioinformatics, PLOS One, JCB, PervasiveHealth). In particular, he is the first author of Graph Attention Networks—a popular convolutional layer for graphs—and Deep Graph Infomax—a scalable local/global unsupervised learning pipeline for graphs (featured in ZDNet). Further, his research has been used in substantially improving the travel-time predictions in Google Maps (covered by outlets including the CNBC, Endgadget, VentureBeat, CNET, the Verge and ZDNet)."
  },
  {
    "objectID": "logml2022/speakers2022/other/taco-cohen.html",
    "href": "logml2022/speakers2022/other/taco-cohen.html",
    "title": "Taco Cohen",
    "section": "",
    "text": "website\n  \n\n\n\nTaco Cohen is a machine learning research scientist at Qualcomm AI Research in Amsterdam. He obtained his PhD at the University of Amsterdam, supervised by prof. Max Welling. He was a co-founder of Scyfer, a company focussed on active deep learning, acquired by Qualcomm in 2017. He holds a BSc in theoretical computer science from Utrecht University and a MSc in artificial intelligence from the University of Amsterdam (both cum laude). His research is focussed on understanding and improving deep representation learning, in particular learning of equivariant and disentangled representations, data-efficient deep learning, learning on non-Euclidean domains, and applications of group representation theory and non-commutative harmonic analysis, as well as deep learning based source compression. He has done internships at Google Deepmind (working with Geoff Hinton) and OpenAI. He received the 2014 University of Amsterdam thesis prize, a Google PhD Fellowship, ICLR 2018 best paper award for “Spherical CNNs”, and was named one of 35 innovators under 35 in Europe by MIT in 2018."
  },
  {
    "objectID": "logml2022/speakers2022/other/yang-hui-he.html",
    "href": "logml2022/speakers2022/other/yang-hui-he.html",
    "title": "Yang-Hui He",
    "section": "",
    "text": "website\n  \n\n\n\nYang-Hui He is a Fellow at the London Institute for Mathematical Sciences,, Professor of mathematics at City, University of London, Lecturer at Merton College, University of Oxford and Chang-Jiang Chair of physics at NanKai University. He studied at Princeton, Cambridge and MIT and works at the interface between string theory, algebraic geometry and machine learning."
  },
  {
    "objectID": "logml2022/speakers2022/keynote/heather-harrington.html",
    "href": "logml2022/speakers2022/keynote/heather-harrington.html",
    "title": "Heather Harrington",
    "section": "",
    "text": "website\n  \n\n\n\nHeather Harrington obtained her PhD in 2010 from the Department of Mathematics at Imperial College London. She joined the Mathematical Institute at the University of Oxford in 2013 as a Hooke Research Fellow and EPSRC Postdoctoral Fellow and was affiliated with St Cross College and Keble College. Heather Harrington was promoted to Professor of Mathematics in 2020. She now has affiliations with St John’s College as a Research Fellow in Mathematics and the Sciences and the Wellcome Centre for Human Genetics an Associate Group Leader. Her research focuses on the problem of reconciling models and data by extracting information about the structure of models and the shape of data. She develops methods relying on techniques from computational algebraic geometry and topology to study complex biological systems. She is the Co-Director of the Centre for Topological Data Analysis. She has been recognised for her research with a Royal Society University Research Fellowship, London Mathematical Society Whitehead Prize, University of Cambridge Adams Prize, and a Philip Leverhulme Prize in Mathematics and Statistics."
  },
  {
    "objectID": "logml2022/projects.html",
    "href": "logml2022/projects.html",
    "title": "Projects",
    "section": "",
    "text": "Adaptive frame averaging for invariant and equivariant representations\n\n\n\n\n\n\nGraphs\n\n\nML\n\n\nGDL\n\n\n\n\n\n\nProf Bruno Ribeiro\n\n\n\n\n\n\n\n\n\n\n\n\nCharacterizing generalization and adversarial robustness for set networks\n\n\n\n\n\n\nGraphs\n\n\nML\n\n\nGDL\n\n\n\n\n\n\nProf Tolga Birdal\n\n\n\n\n\n\n\n\n\n\n\n\nContrastive learning\n\n\n\n\n\n\nGraphs\n\n\nML\n\n\nGDL\n\n\n\n\n\n\nDr Melanie Weber\n\n\n\n\n\n\n\n\n\n\n\n\nDImplicit neural filters for steerable CNNs\n\n\n\n\n\n\nGraphs\n\n\nML\n\n\nGDL\n\n\n\n\n\n\nGabriele Cesa\n\n\n\n\n\n\n\n\n\n\n\n\nData reductions for graph attention variants\n\n\n\n\n\n\nGraphs\n\n\nML\n\n\nGDL\n\n\n\n\n\n\nKaustubh Dholé\n\n\n\n\n\n\n\n\n\n\n\n\nDeep functional map\n\n\n\n\n\n\nGraphs\n\n\nML\n\n\nGDL\n\n\n\n\n\n\nDr Abhishek Sharma\n\n\n\n\n\n\n\n\n\n\n\n\nDifferential geometry for representation learning\n\n\n\n\n\n\nGraphs\n\n\nML\n\n\nGDL\n\n\n\n\n\n\nProf Georgios Arvanitidis\n\n\n\n\n\n\n\n\n\n\n\n\nDistilling large GNNs for molecules\n\n\n\n\n\n\nGraphs\n\n\nML\n\n\nGDL\n\n\n\n\n\n\nJohannes Gasteiger\n\n\n\n\n\n\n\n\n\n\n\n\nEquivariant machine learning for vegetation dynamics\n\n\n\n\n\n\nGraphs\n\n\nML\n\n\nGDL\n\n\n\n\n\n\nProf Soledad Villar\n\n\n\n\n\n\n\n\n\n\n\n\nExploiting domain structure for music ML tasks\n\n\n\n\n\n\nGraphs\n\n\nML\n\n\nGDL\n\n\n\n\n\n\nDr Cătălina Cangea\n\n\n\n\n\n\n\n\n\n\n\n\nExploring network medicine principles encoded by graph neural networks\n\n\n\n\n\n\nGraphs\n\n\nML\n\n\nGDL\n\n\n\n\n\n\nKexin Huang\n\n\n\n\n\n\n\n\n\n\n\n\nGeneralized Laplacian positional encoding for graph learning\n\n\n\n\n\n\nGraphs\n\n\nML\n\n\nGDL\n\n\n\n\n\n\nDr Haggai Maron\n\n\n\n\n\n\n\n\n\n\n\n\nGeometric tools for investigating loss landscapes of deep neural networks\n\n\n\n\n\n\nGraphs\n\n\nML\n\n\nGDL\n\n\n\n\n\n\nDr James Lucas\n\n\n\n\n\n\n\n\n\n\n\n\nGraph-rewiring for GNNs from a geometric perspective\n\n\n\n\n\n\nGraphs\n\n\nML\n\n\nGDL\n\n\n\n\n\n\nDr Francesco di Giovanni\n\n\n\n\n\n\n\n\n\n\n\n\nHelmhotlz-Hodge Laplacians: edge flows and simplicial learning\n\n\n\n\n\n\nGraphs\n\n\nML\n\n\nGDL\n\n\n\n\n\n\nProf Stefan Schonsheck\n\n\n\n\n\n\n\n\n\n\n\n\nLatent graph learning for multivariate time series\n\n\n\n\n\n\nGraphs\n\n\nML\n\n\nGDL\n\n\n\n\n\n\nDr Xiang Zhang\n\n\n\n\n\n\n\n\n\n\n\n\nLearning graph rewiring using RL\n\n\n\n\n\n\nGraphs\n\n\nML\n\n\nGDL\n\n\n\n\n\n\nDr Eli Meirom\n\n\n\n\n\n\n\n\n\n\n\n\nLearning non-geodesic submanifolds\n\n\n\n\n\n\nGraphs\n\n\nML\n\n\nGDL\n\n\n\n\n\n\nProf Nina Miolane\n\n\n\n\n\n\n\n\n\n\n\n\nLine bundle cohomology formulae on Calabi-Yau threefolds\n\n\n\n\n\n\nGraphs\n\n\nML\n\n\nGDL\n\n\n\n\n\n\nDr Andrei Constantin\n\n\n\n\n\n\n\n\n\n\n\n\nMachine learning the fine interior\n\n\n\n\n\n\nGraphs\n\n\nML\n\n\nGDL\n\n\n\n\n\n\nProf Alexander Kasprzyk\n\n\n\n\n\n\n\n\n\n\n\n\nPDE-inspired sheaf neural networks\n\n\n\n\n\n\nGraphs\n\n\nML\n\n\nGDL\n\n\n\n\n\n\nCristian Bodnar\n\n\n\n\n\n\n\n\n\n\n\n\nTowards training GNNs using explanation feedbacks\n\n\n\n\n\n\nGraphs\n\n\nML\n\n\nGDL\n\n\n\n\n\n\nDr Chirag Agarwal\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Archives",
      "LOGML 2022",
      "Projects"
    ]
  },
  {
    "objectID": "logml2022/projects2022/project13/index.html",
    "href": "logml2022/projects2022/project13/index.html",
    "title": "Differential geometry for representation learning",
    "section": "",
    "text": "Georgios is a tenure-track assistant professor in the Section for Cognitive Systems at the Technical University of Denmark (DTU), from where he received his PhD in 2019 under the supervision of Søren Hauberg and Lars Kai Hansen. In between, he was a postdoctoral researcher at the Empirical Inference department at the Max Planck Institute for Intelligent Systems at Tübingen working with Bernhard Schölkopf. His research is mainly focused on geometrical methods in machine learning. In particular, he develops methods that learn the geometric structure of the data based on latent variable models, as well as the associated techniques that enable the analysis in these nonlinear spaces."
  },
  {
    "objectID": "logml2022/projects2022/project13/index.html#prof-georgios-arvanitidis",
    "href": "logml2022/projects2022/project13/index.html#prof-georgios-arvanitidis",
    "title": "Differential geometry for representation learning",
    "section": "",
    "text": "Georgios is a tenure-track assistant professor in the Section for Cognitive Systems at the Technical University of Denmark (DTU), from where he received his PhD in 2019 under the supervision of Søren Hauberg and Lars Kai Hansen. In between, he was a postdoctoral researcher at the Empirical Inference department at the Max Planck Institute for Intelligent Systems at Tübingen working with Bernhard Schölkopf. His research is mainly focused on geometrical methods in machine learning. In particular, he develops methods that learn the geometric structure of the data based on latent variable models, as well as the associated techniques that enable the analysis in these nonlinear spaces."
  },
  {
    "objectID": "logml2022/projects2022/project13/index.html#project",
    "href": "logml2022/projects2022/project13/index.html#project",
    "title": "Differential geometry for representation learning",
    "section": "Project",
    "text": "Project\nA common hypothesis in machine learning is that the data lie near a low dimensional manifold which is embedded in a high dimensional ambient space. This implies that shortest paths between points should respect the underlying geometric structure. In practice, we can capture the geometry of a data manifold through a Riemannian metric in the latent space of a stochastic generative model, relying on meaningful uncertainty estimation for the generative process. This enables us to compute identifiable distances, since the length of the shortest path remains invariant under re-parametrizations of the latent space. Consequently, we are able to study the learned latent representations beyond the classic Euclidean perspective.\nIn this project you will develop methods to learn Riemannian metrics in the latent space of deep generative models. You will then use the learned metrics for computing shortest paths in the representation space and for fitting a statistical model that respects the learned nonlinear geometry of the data.\nReferences () Latent Space Oddity: on the Curvature of Deep Generative Models, Georgios Arvanitidis, Lars Kai Hansen, Søren Hauberg, International Conference on Learning Representations (ICLR), 2018. () A prior-based approximate latent Riemannian metric, Georgios Arvanitidis, Bogdan Georgiev, Bernhard Schölkopf, International Conference on Artificial Intelligence and Statistics (AISTATS), 2022. () Fast and robust shortest paths on manifolds learned from data, Georgios Arvanitidis, Søren Hauberg, Philipp Hennig, Michael Tiemann, International Conference on Artificial Intelligence and Statistics (AISTATS), 2019. () A locally adaptive normal distribution, Georgios Arvanitidis, Lars Kai Hansen, Søren Hauberg, Advances in Neural Information Processing Systems (NeurIPS), 2016."
  },
  {
    "objectID": "logml2022/projects2022/project15/index.html",
    "href": "logml2022/projects2022/project15/index.html",
    "title": "Learning non-geodesic submanifolds",
    "section": "",
    "text": "Nina Miolane is an Assistant Professor at the University of California, Santa Barbara, and an Affiliate Researcher with the SLAC National Laboratory, Stanford. She received her M.S. in Mathematics from Ecole Polytechnique (France) & Theoretical Physics from Imperial College (UK), and her Ph.D. in Computer Science from INRIA (France) in collaboration with Stanford University (US). She was a postdoc and lecturer in Statistics at Stanford University, and worked as a deep learning software engineer in the Silicon Valley. At UCSB, Nina directs the BioShape Lab, whose research investigates how the shapes of proteins, cells, and organs correlate with physiological conditions and pathologies. Her team co-develops Geomstats, a software for machine learning on geometric data such as biological shape data. The BioShape Lab was awarded a NIH R01 grant on Molecular Shape Reconstruction and the NSF SCALE MoDL grant on Mathematical Foundations of Deep Learning. Nina was the recipient of the L’Oréal-Unesco for Women in Science Award, of the Regents’ Junior Faculty Fellowship Award. In her free time, Nina pilots single-engine airplanes in the Californian skies."
  },
  {
    "objectID": "logml2022/projects2022/project15/index.html#prof-nina-miolane",
    "href": "logml2022/projects2022/project15/index.html#prof-nina-miolane",
    "title": "Learning non-geodesic submanifolds",
    "section": "",
    "text": "Nina Miolane is an Assistant Professor at the University of California, Santa Barbara, and an Affiliate Researcher with the SLAC National Laboratory, Stanford. She received her M.S. in Mathematics from Ecole Polytechnique (France) & Theoretical Physics from Imperial College (UK), and her Ph.D. in Computer Science from INRIA (France) in collaboration with Stanford University (US). She was a postdoc and lecturer in Statistics at Stanford University, and worked as a deep learning software engineer in the Silicon Valley. At UCSB, Nina directs the BioShape Lab, whose research investigates how the shapes of proteins, cells, and organs correlate with physiological conditions and pathologies. Her team co-develops Geomstats, a software for machine learning on geometric data such as biological shape data. The BioShape Lab was awarded a NIH R01 grant on Molecular Shape Reconstruction and the NSF SCALE MoDL grant on Mathematical Foundations of Deep Learning. Nina was the recipient of the L’Oréal-Unesco for Women in Science Award, of the Regents’ Junior Faculty Fellowship Award. In her free time, Nina pilots single-engine airplanes in the Californian skies."
  },
  {
    "objectID": "logml2022/projects2022/project15/index.html#project",
    "href": "logml2022/projects2022/project15/index.html#project",
    "title": "Learning non-geodesic submanifolds",
    "section": "Project",
    "text": "Project\nRepresentation learning aims to transform data x into a lower-dimensional variable z designed to be more efficient for any downstream machine learning task, such as classification. In this project, you will tackle representation learning for manifold-valued data x, and specifically delve into non-geodesic submanifold learning with algorithms of curve fitting and variational autoencoders (rVAE) on manifolds. You will contribute to the open-source package Geomstats by implementing a representation learning module which will unify and contrast the aforementioned methods.\nReferences () Miolane et al. Geomstats: A Python Package for Riemannian Geometry in Machine Learning (JMLR 2020). () Miolane, Holmes. Learning Weighted Submanifolds with Variational Autoencoders and Riemannian Variational Autoencoders (CVPR 2020)."
  },
  {
    "objectID": "logml2022/projects2022/project7/index.html",
    "href": "logml2022/projects2022/project7/index.html",
    "title": "Contrastive learning",
    "section": "",
    "text": "Melanie is a Hooke Research Fellow at the University of Oxford. Her research focuses on the mathematical foundations of Machine Learning and Data Science, with a special interest in understanding the geometric features of data and in developing machine learning methods that utilize such geometric knowledge. She received her PhD from Princeton University under the supervision of Charles Fefferman. She held visiting positions at MIT’s Laboratory for Information and Decision Systems and the Simons Institute in Berkeley and interned in the research labs of Facebook, Google and Microsoft. In addition to her academic work, she is the Chief Scientist of the Legal AI start up Claudius Legal Intelligence, where she leads a team of researchers in developing Trustworthy Machine Learning tools for legal analytics. Her awards include Princeton’s C.V. Starr Fellowship, a Simons-Berkeley Fellowship, a selection as Rising Star in EECS and an Alan Turing Post-Doctoral Enrichment Award."
  },
  {
    "objectID": "logml2022/projects2022/project7/index.html#dr-melanie-weber",
    "href": "logml2022/projects2022/project7/index.html#dr-melanie-weber",
    "title": "Contrastive learning",
    "section": "",
    "text": "Melanie is a Hooke Research Fellow at the University of Oxford. Her research focuses on the mathematical foundations of Machine Learning and Data Science, with a special interest in understanding the geometric features of data and in developing machine learning methods that utilize such geometric knowledge. She received her PhD from Princeton University under the supervision of Charles Fefferman. She held visiting positions at MIT’s Laboratory for Information and Decision Systems and the Simons Institute in Berkeley and interned in the research labs of Facebook, Google and Microsoft. In addition to her academic work, she is the Chief Scientist of the Legal AI start up Claudius Legal Intelligence, where she leads a team of researchers in developing Trustworthy Machine Learning tools for legal analytics. Her awards include Princeton’s C.V. Starr Fellowship, a Simons-Berkeley Fellowship, a selection as Rising Star in EECS and an Alan Turing Post-Doctoral Enrichment Award."
  },
  {
    "objectID": "logml2022/projects2022/project7/index.html#project",
    "href": "logml2022/projects2022/project7/index.html#project",
    "title": "Contrastive learning",
    "section": "Project",
    "text": "Project\nContrastive learning seeks to train a representation function that encodes the similarity structure in a data set based on pairs of positive samples (similar data points) and negative samples (dissimilar data points). This project will investigate ways of incorporating geometric information, such as equivariances or symmetries, into Contrastive Learning approaches. Depending on the interests and expertise of the group, both computational and theoretical avenues of investigation may be pursued."
  },
  {
    "objectID": "logml2022/projects2022/project8/index.html",
    "href": "logml2022/projects2022/project8/index.html",
    "title": "Graph-rewiring for GNNs from a geometric perspective",
    "section": "",
    "text": "Francesco is an ML Researcher currently working at Twitter with Michael Bronstein on geometry-inspired ideas applied in the context of graph machine learning. He completed a PhD in Riemannian geometry focussing on singularity formation of symmetric (cohomogeneity-1) Ricci flows at UCL under the supervision of Jason Lotay."
  },
  {
    "objectID": "logml2022/projects2022/project8/index.html#dr-francesco-di-giovanni",
    "href": "logml2022/projects2022/project8/index.html#dr-francesco-di-giovanni",
    "title": "Graph-rewiring for GNNs from a geometric perspective",
    "section": "",
    "text": "Francesco is an ML Researcher currently working at Twitter with Michael Bronstein on geometry-inspired ideas applied in the context of graph machine learning. He completed a PhD in Riemannian geometry focussing on singularity formation of symmetric (cohomogeneity-1) Ricci flows at UCL under the supervision of Jason Lotay."
  },
  {
    "objectID": "logml2022/projects2022/project8/index.html#project",
    "href": "logml2022/projects2022/project8/index.html#project",
    "title": "Graph-rewiring for GNNs from a geometric perspective",
    "section": "Project",
    "text": "Project\nIn this project we will investigate possible ways to formalize and understand the notion of graph-rewiring in the context of Graph Neural Networks from a geometric perspective. In certain regimes - existence of long-range dependencies that are crucial for the classification task or heterophily of the label (feature) distribution - the graph structure is known not to be beneficial and, in some cases, even harmful. However, a clear understanding of how classical GNNs behave with respect to specific topological transformations is still missing. Providing a clearer picture in this regard is intimately connected with the problem of stability of GNNs with respect to topological perturbations and might also shed some light on tackling expressivity from a different angle. The main goal of the project consists in studying notions of distances among graphs and associated classes of transformations that would allow us to better tackle the problem of graph-rewiring and analyse theoretically its effects on GNNs."
  },
  {
    "objectID": "logml2022/projects2022/project6/index.html",
    "href": "logml2022/projects2022/project6/index.html",
    "title": "Learning graph rewiring using RL",
    "section": "",
    "text": "Eli Meirom is a research scientist at NVIDIA Research. His research interests include machine learning on graphs and reinforcement learning. He completed his Ph.D at the Technion, Israel. Before joining NVIDIA Research, he co-founded HearWize and Amooka-AI, the latter was acquired by Ford in 2018, where he worked as a senior research scientist, developing Ford’s autonomous vehicles driving policy."
  },
  {
    "objectID": "logml2022/projects2022/project6/index.html#dr-eli-meirom",
    "href": "logml2022/projects2022/project6/index.html#dr-eli-meirom",
    "title": "Learning graph rewiring using RL",
    "section": "",
    "text": "Eli Meirom is a research scientist at NVIDIA Research. His research interests include machine learning on graphs and reinforcement learning. He completed his Ph.D at the Technion, Israel. Before joining NVIDIA Research, he co-founded HearWize and Amooka-AI, the latter was acquired by Ford in 2018, where he worked as a senior research scientist, developing Ford’s autonomous vehicles driving policy."
  },
  {
    "objectID": "logml2022/projects2022/project6/index.html#project",
    "href": "logml2022/projects2022/project6/index.html#project",
    "title": "Learning graph rewiring using RL",
    "section": "Project",
    "text": "Project\nMost GNNs are based on the concept of message passing, which is by itself based on information diffusion. In diffusion dynamics, key information lies in closer objects, and distant nodes’ effect is decimated [1]. However, it is not clear that the topological graph structure must dictate the information transfer on the graph. In fact, in many cases, such as combinatorial optimization problems, nodes and edges that are distant from a node may have a major impact on the node’s value or class. To that end, graph rewiring allows adding edges, nodes, or other structures in order to assist information transfer. In practice, it decouples the information graph from the topological (input) graph. In this project, we will investigate how we can (meta) learn to build better information graphs using RL. Specifically, our agent will learn how to modify (add/remove) edges, i.e., perform graph rewiring, to improve learning. Our goal is to publish the results of this project in a top ML conference.\nReferences [1] Understanding over-squashing and bottlenecks on graphs via curvature, Topping et. al., 2022. [2] On the bottleneck of graph neural networks and its practical implications, Alon et. al., 2020"
  },
  {
    "objectID": "logml2022/projects2022/project21/index.html",
    "href": "logml2022/projects2022/project21/index.html",
    "title": "PDE-inspired sheaf neural networks",
    "section": "",
    "text": "Cristian is a third-year PhD student in the Department of Computer Science, University of Cambridge, supervised by Prof Pietro Liò. His research uses applied Topology and Differential Geometry for understanding and developing Geometric Deep Learning models suitable for problems presenting an underlying combinatorial structure. He is also a Microsoft Research PhD Fellow (2021) and a former research intern at Twitter Cortex, Google Brain and Google X."
  },
  {
    "objectID": "logml2022/projects2022/project21/index.html#cristian-bodnar",
    "href": "logml2022/projects2022/project21/index.html#cristian-bodnar",
    "title": "PDE-inspired sheaf neural networks",
    "section": "",
    "text": "Cristian is a third-year PhD student in the Department of Computer Science, University of Cambridge, supervised by Prof Pietro Liò. His research uses applied Topology and Differential Geometry for understanding and developing Geometric Deep Learning models suitable for problems presenting an underlying combinatorial structure. He is also a Microsoft Research PhD Fellow (2021) and a former research intern at Twitter Cortex, Google Brain and Google X."
  },
  {
    "objectID": "logml2022/projects2022/project21/index.html#project",
    "href": "logml2022/projects2022/project21/index.html#project",
    "title": "PDE-inspired sheaf neural networks",
    "section": "Project",
    "text": "Project\nCellular sheaves [1, 2] equip graphs with a “geometrical” structure by attaching vector spaces and linear maps to their nodes and edges. It turns out that this additional structure can help mitigate some of the well-known problems of Graph Neural Networks. In a recent paper [3], we studied neural diffusion PDEs on sheaves and proved they have many desirable properties, such as better performance in heterophilic settings than GNNs and robust behaviour in the infinite-time limit (i.e. infinitely many layers).\nIn this project, we aim to extend this approach to other sheaf-based PDEs that behave differently from diffusion, leading to novel Sheaf Neural Network models. We will be studying the theoretical properties of these neural PDEs (e.g. Do they minimise some energy? Are the dynamics stable?) with the ultimate goal of building practical models for node classification and regression tasks.\nReferences [1] Jakob Hansen, Robert Ghrist, Toward a Spectral Theory of Cellular Sheaves, Journal of Applied and Computational Topology volume 3, pages 315–358 (2019) [2] Jakob Hansen, Thomas Gebhart, Sheaf Neural Networks, NeurIPS 2020 Workshop on TDA and Beyond [3] Bodnar et al., Neural Sheaf Diffusion: A Topological Perspective on Heterophily and Oversmoothing in GNNs, Preprint 2022"
  },
  {
    "objectID": "logml2022/projects2022/project10/index.html",
    "href": "logml2022/projects2022/project10/index.html",
    "title": "Deep functional map",
    "section": "",
    "text": "Abhishek is an incoming DL Research Scientist at Illumina AI lab Cambridge and before that, did his Ph.D. in the Shape Analysis Group at Ecole Polytechnique. His Ph.D. thesis is about Deep Functional Maps for 3D shape analysis. He is very interested in alignment problems in general and their applications in molecular biology."
  },
  {
    "objectID": "logml2022/projects2022/project10/index.html#dr-abhishek-sharma",
    "href": "logml2022/projects2022/project10/index.html#dr-abhishek-sharma",
    "title": "Deep functional map",
    "section": "",
    "text": "Abhishek is an incoming DL Research Scientist at Illumina AI lab Cambridge and before that, did his Ph.D. in the Shape Analysis Group at Ecole Polytechnique. His Ph.D. thesis is about Deep Functional Maps for 3D shape analysis. He is very interested in alignment problems in general and their applications in molecular biology."
  },
  {
    "objectID": "logml2022/projects2022/project10/index.html#project",
    "href": "logml2022/projects2022/project10/index.html#project",
    "title": "Deep functional map",
    "section": "Project",
    "text": "Project\n3D Shape matching is a fundamental problem in computer vision and graphics with significant applications on biological data. In this project, we will take a closer look at Deep Functional Map (DFM) [1,2] paradigm for 3D shape matching. You will become familiar with unsupervised DFM frameworks [2] and investigate a couple of directions less explored in the DFM literature. First, most DFM literature strongly relies on precomputed Laplacian Beltrami (LB) eigenbasis. There have been some recent attempts to learn an embedding [3,4] instead. However, it is not entirely clear in which circumstances learned embedding is more robust and useful than LB eigenbasis. Secondly, we will investigate cycle consistency constraints in DFM that provide a strong regularization by jointly optimizing the maps over a collection of shapes. By the end of the project, you should better understand these topics from both theoretical and practical perspectives.\nReferences [1] Litany et al., Deep Functional Map: Structure prediction for dense shape correspondence, ICCV 2017 [2] Roufosse et al., Unsupervised Deep Learning for 3D shape Matching, ICCV 2019 [3] Marin et al., Correspondence Learning via Linearly Invariant Embedding, Neurips 2020 [4] Sharma & Ovsjanikov, Joint Symmetry Detection and Shape Matching for Non-rigid Point Cloud, arXiv"
  },
  {
    "objectID": "logml2022/projects2022/project16/index.html",
    "href": "logml2022/projects2022/project16/index.html",
    "title": "Exploring network medicine principles encoded by graph neural networks",
    "section": "",
    "text": "Kexin Huang is a CS PhD student at Stanford with Prof. Jure Leskovec. His research interest lies in algorithmic challenges arising from real-world biomedicine, with a focus on graph learning and therapeutic discovery."
  },
  {
    "objectID": "logml2022/projects2022/project16/index.html#kexin-huang",
    "href": "logml2022/projects2022/project16/index.html#kexin-huang",
    "title": "Exploring network medicine principles encoded by graph neural networks",
    "section": "",
    "text": "Kexin Huang is a CS PhD student at Stanford with Prof. Jure Leskovec. His research interest lies in algorithmic challenges arising from real-world biomedicine, with a focus on graph learning and therapeutic discovery."
  },
  {
    "objectID": "logml2022/projects2022/project16/index.html#project",
    "href": "logml2022/projects2022/project16/index.html#project",
    "title": "Exploring network medicine principles encoded by graph neural networks",
    "section": "Project",
    "text": "Project\nGraph neural networks (GNNs) have enabled many successful biomedical applications when applying to biological networks - from prioritization of treatments, detection of side effects, to identification of protein function. Because of the impressive performance, it is hypothesized that GNN must capture fundamental biological principles [1] but it is unclear what they are and how GNNs encode these principles. There are decades of research in the field of network medicine [2] where researchers study the organizing principles of biological networks and described hypotheses of governing laws. In this project, we will explore what are these important network medicine principles, how they are manifested in the GNN embedding space. If GNN fails to encode some network medicine principles, then we can motivate a new method to tackle them.\nReferences [1] Li, Michelle M., Kexin Huang, and Marinka Zitnik. Graph Representation Learning in Biomedicine (2021). arXiv: 2104.04883 [2] Barabási, Albert-László, Natali Gulbahce, and Joseph Loscalzo. Network medicine: a network-based approach to human disease. Nature reviews genetics 12.1 (2011): 56-68."
  },
  {
    "objectID": "logml2022/projects2022/project18/index.html",
    "href": "logml2022/projects2022/project18/index.html",
    "title": "Characterizing generalization and adversarial robustness for set networks",
    "section": "",
    "text": "Tolga Birdal is a prospective assistant professor in the Department of Computing of Imperial College London. Previously, he was a senior Postdoctoral Research Fellow at Stanford University within the Geometric Computing Group of Prof. Leonidas Guibas. Tolga has defended his masters and Ph.D. theses at the Computer Vision Group, Chair for Computer Aided Medical Procedures, Technical University of Munich led by Prof. Nassir Navab. He was also a Doktorand at Siemens AG under supervision of Dr. Slobodan Ilic working on “Geometric Methods for 3D Reconstruction from Large Point Clouds”. His current foci of interest involve geometric machine learning and 3D computer vision. More theoretical work is aimed at investigating and interrogating limits in geometric computing and non-Euclidean inference as well as principles of deep learning and artificial consciousness. Tolga has several publications at the well-respected venues such as NeurIPS, CVPR, ICCV, ECCV, T-PAMI, ICRA, IROS, ICASSP and 3DV. Aside from his academic life, Tolga is an entrepreneur. He has co-founded multiple companies including Befunky, a widely used web-based image editing platform."
  },
  {
    "objectID": "logml2022/projects2022/project18/index.html#prof-tolga-birdal",
    "href": "logml2022/projects2022/project18/index.html#prof-tolga-birdal",
    "title": "Characterizing generalization and adversarial robustness for set networks",
    "section": "",
    "text": "Tolga Birdal is a prospective assistant professor in the Department of Computing of Imperial College London. Previously, he was a senior Postdoctoral Research Fellow at Stanford University within the Geometric Computing Group of Prof. Leonidas Guibas. Tolga has defended his masters and Ph.D. theses at the Computer Vision Group, Chair for Computer Aided Medical Procedures, Technical University of Munich led by Prof. Nassir Navab. He was also a Doktorand at Siemens AG under supervision of Dr. Slobodan Ilic working on “Geometric Methods for 3D Reconstruction from Large Point Clouds”. His current foci of interest involve geometric machine learning and 3D computer vision. More theoretical work is aimed at investigating and interrogating limits in geometric computing and non-Euclidean inference as well as principles of deep learning and artificial consciousness. Tolga has several publications at the well-respected venues such as NeurIPS, CVPR, ICCV, ECCV, T-PAMI, ICRA, IROS, ICASSP and 3DV. Aside from his academic life, Tolga is an entrepreneur. He has co-founded multiple companies including Befunky, a widely used web-based image editing platform."
  },
  {
    "objectID": "logml2022/projects2022/project18/index.html#project",
    "href": "logml2022/projects2022/project18/index.html#project",
    "title": "Characterizing generalization and adversarial robustness for set networks",
    "section": "Project",
    "text": "Project\nDisobeying the classical wisdom of statistical learning theory, modern deep neural networks generalize well even though they typically contain millions of parameters. Hence, a new generation of learning theory has emerged to explain the characteristics of deep neural networks, such as generalization, overfitting or robustness. Empirical or theoretical, most of the works which prosper in bringing insights to the learning phenomenon, focus on convolutional networks, operating on the image domain. However, a vast majority of the computer vision problems involve either graphs or point clouds which live in unstructured domains. The goal of this project is to first empirically understand the generalization character of point cloud networks. To this end, we will deploy a series of state of the art measures. Guided by this empirical study, we aim to theorize how and why deep sets generalize. In particular, we will focus on topological data analysis as a unifying framework."
  },
  {
    "objectID": "logml2022/projects2022/project4/index.html",
    "href": "logml2022/projects2022/project4/index.html",
    "title": "Equivariant machine learning for vegetation dynamics",
    "section": "",
    "text": "Soledad Villar is an Assistant Professor in Applied Mathematics and Statistics at Johns Hopkins University. Prior to that she worked with Joan Bruna and Afonso Bandeira at New York University and was affiliated with the Simons Foundation in New York City and UC Berkeley. She received her PhD from UT Austin supervised by Rachel Ward. Her research interests include equivariant machine learning, graph neural networks and mathematical foundations of deep learning. She is also interested in applications to computational biology."
  },
  {
    "objectID": "logml2022/projects2022/project4/index.html#prof-soledad-villar",
    "href": "logml2022/projects2022/project4/index.html#prof-soledad-villar",
    "title": "Equivariant machine learning for vegetation dynamics",
    "section": "",
    "text": "Soledad Villar is an Assistant Professor in Applied Mathematics and Statistics at Johns Hopkins University. Prior to that she worked with Joan Bruna and Afonso Bandeira at New York University and was affiliated with the Simons Foundation in New York City and UC Berkeley. She received her PhD from UT Austin supervised by Rachel Ward. Her research interests include equivariant machine learning, graph neural networks and mathematical foundations of deep learning. She is also interested in applications to computational biology."
  },
  {
    "objectID": "logml2022/projects2022/project4/index.html#project",
    "href": "logml2022/projects2022/project4/index.html#project",
    "title": "Equivariant machine learning for vegetation dynamics",
    "section": "Project",
    "text": "Project\nPredicting vegetation dynamics is a fundamental problem in ecology, especially in the context of climate change. In this project we aim to learn the equations that rule the vegetation dynamics from data with machine learning.\nThe input features to the learning problem include observables such as rainfall, vegetation density, waterabsorbed into the soil, etc. Each of these observables have precise units (liters per day per square meter, grams per square meter, and liters per square meter, respectively for the examples above); therefore the learning should be units-equivariant. This means that if – for instance – we do a transformation where all the input features with units of meters are rescaled to inches, the predictions should transform accordingly. This symmetry is known as unit equivariance and it corresponds to group equivariance by an action by the (non-compact) group of scalings (see Section 3 of [3]).\nA few methods, based on classical dimensional analysis, have been recently proposed to model units-equivariant machine learning problems (see [3, 1]). In this project we propose to combine these ideas with symbolic regression and PDE integrators to learn the equations that produce the dynamics from data. For more information about the prediction problem and how the data is generated refer to Section 7 of [3] or contact Prof. Villar at soledad.villar@jhu.edu.\nReferences [1] Joseph Bakarji, Jared Callaham, Steven L Brunton, and J Nathan Kutz. Dimensionally consistent learning with buckingham Pi.arXiv:2202.04643, 2022. [2] Max Rietkerk, Maarten C Boerlijst, Frank van Langevelde, Reinier HilleRisLambers, Johan van de Kop-pel, Lalit Kumar, Herbert HT Prins, and Andr ́e M de Roos. Self-organization of vegetation in aridecosystems. The American Naturalist, 160(4):524–530, 2002. [3] Soledad Villar, Weichi Yao, David W Hogg, Ben Blum-Smith, and Bianca Dumitrascu. Dimensionless machine learning: Imposing exact units equivariance. arXiv preprint arXiv:2204.00887, 2022."
  },
  {
    "objectID": "logml2022/projects2022/project2/project2.html",
    "href": "logml2022/projects2022/project2/project2.html",
    "title": "Machine learning the fine interior",
    "section": "",
    "text": "Alexander Kasprzyk is  Associate Professor of Geometry and Head of Pure Mathematics at the  University of Nottingham. His research focuses on the intersection of  algebraic geometry, combinatorics, and Big Data. Through his work he has  pioneered the use of massively-parallel computational algebra and large databases to address fundamental  questions in geometry, applying tens of centuries of computing time to  make substantial and important mathematical advances in the  classification of Fano varieties, including the largest collections of Fano 3-fold and 4-fold varieties known. He maintains the online  Graded Ring Database, and is an editor for the new interdisciplinary  journal “Data Science in the Mathematical Sciences”."
  },
  {
    "objectID": "logml2022/projects2022/project2/project2.html#prof-alexander-kasprzyk",
    "href": "logml2022/projects2022/project2/project2.html#prof-alexander-kasprzyk",
    "title": "Machine learning the fine interior",
    "section": "",
    "text": "Alexander Kasprzyk is  Associate Professor of Geometry and Head of Pure Mathematics at the  University of Nottingham. His research focuses on the intersection of  algebraic geometry, combinatorics, and Big Data. Through his work he has  pioneered the use of massively-parallel computational algebra and large databases to address fundamental  questions in geometry, applying tens of centuries of computing time to  make substantial and important mathematical advances in the  classification of Fano varieties, including the largest collections of Fano 3-fold and 4-fold varieties known. He maintains the online  Graded Ring Database, and is an editor for the new interdisciplinary  journal “Data Science in the Mathematical Sciences”."
  },
  {
    "objectID": "logml2022/projects2022/project2/project2.html#project",
    "href": "logml2022/projects2022/project2/project2.html#project",
    "title": "Machine learning the fine interior",
    "section": "Project",
    "text": "Project\nFirst described in 1983, the Fine interior is a key combinatorial tool in Mirror Symmetry. Broadly speaking, a convex lattice polytope P corresponds to a hypersurface Z in a toric variety. Associate to P is the Fine interior F(P): the rational polytope given by moving all supporting hyperplanes of P in by lattice distance 1. Many geometric properties of Z can be deduced from combinatorial properties of F(P). For example, there exists a unique canonical model of Z if F(P) is non-empty, and the Kodaira dimension is determined by the dimension of F(P). Computing the Fine interior F(P) is computationally challenging and, despite being so important, almost nothing is known about how the combinatorics of P determines the dimension of F(P). This is an area perfect for investigation via Machine Learning.\nIn this project we will explore the classification of certain four-dimension lattice simplices – those containing a single interior lattice point. Each of these 338,752 simplices can be described uniquely by an integer-valued vector (a_0,…,a_4), and in nearly every case we know the Fine interior as a result of brute-force computations totalling many decades of CPU time. We will ask whether Machine Learning can predict the dimension of F(P) directly from the vector (a_0,…,a_4) and, if successful, attempt to understand how the machine is performing this calculation. This should present us with unique insights into the combinatorics of the Fine interior, which in turn will generate a richer understanding of the underlying geometry."
  },
  {
    "objectID": "logml2022/people2022/advisors/ron.html",
    "href": "logml2022/people2022/advisors/ron.html",
    "title": "Ron Kimmel",
    "section": "",
    "text": "website\n  \n\n\n\nRon Kimmel is a Professor of Computer Science (and Electrical and Computer Engineering by courtesy) at the Technion where he holds the Montreal Chair in Sciences. He held a post-doctoral position at UC Berkeley and a visiting professorship at Stanford University. He has worked in various areas of shape reconstruction and analysis in computer vision, image processing, deep learning of big geometric data, and computer graphics. Kimmel’s interest in recent years has been understanding of machine learning, medical imaging and specifically computational oncology/pathology, precision medicine, optimization of solvers to problems with a geometric flavor, and applications of metric, spectral, Riemannian, and differential geometries. Kimmel is an IEEE Fellow and SIAM Fellow for his contributions to image processing, shape reconstruction and geometric analysis. He is the founder of the Geometric Image Processing Lab. and a founder and advisor of several successful image processing and analysis companies."
  },
  {
    "objectID": "logml2022/people2022/advisors/bronstein.html",
    "href": "logml2022/people2022/advisors/bronstein.html",
    "title": "Michael Bronstein",
    "section": "",
    "text": "website\n  \n\n\n\nMichael Bronstein is the DeepMind Professor of AI at the University of Oxford. He was previously Head of Graph Learning Research at Twitter, a professor at Imperial College London and held visiting appointments at Stanford, MIT, and Harvard. He has been affiliated with three Institutes for Advanced Study (at TUM as a Rudolf Diesel Fellow (2017-2019), at Harvard as a Radcliffe fellow (2017-2018), and at Princeton as a short-time scholar (2020)). Michael received his PhD from the Technion in 2007. He is the recipient of the EPSRC Turing AI World Leading Research Fellowship, Royal Society Wolfson Research Merit Award, Royal Academy of Engineering Silver Medal, five ERC grants, two Google Faculty Research Awards, and two Amazon AWS ML Research Awards. He is a Member of the Academia Europaea, Fellow of IEEE, IAPR, BCS, and ELLIS, ACM Distinguished Speaker, and World Economic Forum Young Scientist. In addition to his academic career, Michael is a serial entrepreneur and founder of multiple startup companies, including Novafora, Invision (acquired by Intel in 2012), Videocites, and Fabula AI (acquired by Twitter in 2019)."
  },
  {
    "objectID": "logml2022/people2022/team/michelle.html",
    "href": "logml2022/people2022/team/michelle.html",
    "title": "Michelle Li",
    "section": "",
    "text": "Michelle is a PhD candidate in the Department of Biomedical Informatics  at Harvard Medical School. Advised by Prof. Marinka Zitnik, Michelle is  developing deep graph representation learning algorithms to characterize  drugs at a single cell resolution and to better leverage rich  biomedical knowledge graphs for diagnosing rare diseases."
  },
  {
    "objectID": "logml2024/speakers2024/other/michalis.html",
    "href": "logml2024/speakers2024/other/michalis.html",
    "title": "Michalis Vazirgiannis",
    "section": "",
    "text": "website\n  \n\n\n\nM. Vazirgiannis is a Distinguished Professor at Ecole Polytechnique in France. He has been intensively involved in data science and AI related research. His broad research area is in methods for data mining and machine/deep learning methods for diverse data types and applications (including graphs, text, time series). Recently he is working of i. GNNs and aspects including expressiveness, efficiency, generation ii. Pretrained models and resources for multilingual NLP and Biomedical applications. His research and industrial impact is spanning different domains such as web advertising, social networks, online gambling, insurance, legal text applications, aviation and maritime industry and the bio/medical domain. He also pioneered at the teaching/training level having introduced new machine/deep learning and AI courses for academic and executive training studies. Pr. Vazirgiannis has published more than 250 papers in international journals and proceedings of international conferences and his work is highly cited. On the side of supervision he has supervised 30 completed PhD theses. Finally he has been able to attract significant funding for research from national and international sources, from research agencies and industrial partners (including Google, Airbus, Huawei, Deezer, BNP, LVMH). He lead(s) academic research chairs (DIGITEO 2013-15, ANR/HELAS 2020-26) and an industrial one (AXA, 2015-2018). He has received several awards and distinctions including i. Marie Curie Intra European Fellowship (2006-8) ii. “Rhino-Bird International Academic Expert Award” in recognition of his academic/professional work @ Tencent (2017), iii. best paper awards in international conferences (such as IJCAI 2018, CIKM2013). He has been invited to media interviews in France, USA and China and published popularized articles in French and Greek magazines/newspapers on Artificial Intelligence topics."
  },
  {
    "objectID": "logml2024/speakers2024/other/islem.html",
    "href": "logml2024/speakers2024/other/islem.html",
    "title": "Islem Rekik",
    "section": "",
    "text": "website\n  \n\n\n\nIslem Rekik is the Director of the Brain And SIgnal Research and Analysis (BASIRA) laboratory (http://basira-lab.com/) and an Associate Professor at Imperial College London (Innovation Hub I-X).\nTogether with BASIRA members, she conducted more than 90 cutting-edge research projects cross-pollinating AI and healthcare —with a sharp focus on brain imaging and neuroscience. She is also a co/chair/organizer of more than 20 international first-class conferences/workshops/competitions (e.g., Affordable AI 2021-22, Predictive AI 2018-2023, Machine Learning in Medical Imaging 2021-23, WILL competition 2021-22).\nDr Rekik has been awarded prestigious international research fellowships including the EU Marie-Curie Fellowship in 2019 and the TUBITAK 2232 for Outstanding Experienced Researchers during 2020-2022. In addition to her 130+ high-impact publications, she is a strong advocate of equity, inclusiveness and diversity in research.\nShe is the former president of the Women in MICCAI (WiM), the co-founder of the international RISE Network to Reinforce Inclusiveness & diverSity and Empower minority researchers in Low-Middle Income Countries (LMIC) and a committee member of the AFRICAI network."
  },
  {
    "objectID": "logml2024/speakers2024/other/vishnu.html",
    "href": "logml2024/speakers2024/other/vishnu.html",
    "title": "Vishnu Jejjala",
    "section": "",
    "text": "website\n  \n\n\n\nEarly in his scientific career, Vishnu Jejjala switched from vertebrate paleontology to astronomy. He was seven years old at the time. His interest in explicating the origin of the Universe and playing with cool mathematics led him to string theory. He completed a Ph.D. in Physics at the University of Illinois at Urbana-Champaign in 2002 and subsequently held postdoctoral research appointments at Virginia Tech (2002–2004), Durham University (2004–2007), the Institut des Hautes Études Scientifiques (2007–2009), and Queen Mary, University of London (2009–2011). Since October 2011, Vishnu is the South African Research Chair in Theoretical Particle Cosmology at the University of the Witwatersrand in Johannesburg. He is also a Professor in the School of Physics.\nVishnu’s research interests are broad, but focus on exploring quantum gravity and the structure of quantum field theories with the goal of bringing string theory into contact with the real world. Black holes are one theoretical laboratory for investigating these issues. Another is string compactification on Calabi–Yau spaces. Vishnu has recently been applying techniques from machine learning to study large data sets in string theory and mathematics."
  },
  {
    "objectID": "logml2024/speakers2024/keynote/pietro.html",
    "href": "logml2024/speakers2024/keynote/pietro.html",
    "title": "Pietro Liò",
    "section": "",
    "text": "website\n  \n\n\n\nI am Full Professor at the department of Computer Science and Technology of the University of Cambridge and I am a member of the Artificial Intelligence group. I am a member of the Cambridge Centre for AI in Medicine. My research interest focuses on developing Artificial Intelligence and Computational Biology models to understand diseases complexity and address personalised and precision medicine. Current focus is on Graph Neural Network modeling.\nI have a MA from Cambridge, a PhD in Complex Systems and Non Linear Dynamics (School of Informatics, dept of Engineering of the University of Firenze, Italy) and a PhD in (Theoretical) Genetics (University of Pavia, Italy). Other Affliations: I am member of CAMBRIDGE CENTRE FOR AI IN MEDICINE - the Integrate Cancer Medicine Institute, the committee of MPhil in Computational Biology (Stakeholder Group for the CCBI) , steering committee of Cambridge BIG data, VPH-UK (Virtual Physiological Human), Fellow and member of the Council of Clare Hall College , I am member of Ellis, the European Lab for Learning & Intelligent Systems, I am member of the Academia Europaea; I am listed in www.topitalianscientists.org/Top_italian_scientists_VIA-Academy.aspx"
  },
  {
    "objectID": "logml2024/projects2024/project13/index.html",
    "href": "logml2024/projects2024/project13/index.html",
    "title": "Learning to predict optimal solution value for NP-Hard Combinatorial problems",
    "section": "",
    "text": "Sahil Manchanda is a PhD scholar at the Department of Computer Science at the Indian Institute of Technology Delhi, working under the supervision of Prof. Sayan Ranu. Sahil works in the area of Learning to solve graph optimization problems with focus on Combinatorial Optimization, Graph Neural Networks, Lifelong Learning and Generative modeling. He is also interested in applications of Computer Science concepts in other fields such as Material Science and Hardware. He has interned at prestigious institutes such as the University of Tokyo, NAVER Labs- France and Qualcomm AI Research Amsterdam. His research works have been published in top conferences such as NeurIPS, AAAI, ICML, ECML-PKDD and LoG. Additionally he has one US patent granted to his name. Sahil has been the recipient of the Qualcomm Innovation Fellowship for the year 2022."
  },
  {
    "objectID": "logml2024/projects2024/project13/index.html#sahil-manchanda",
    "href": "logml2024/projects2024/project13/index.html#sahil-manchanda",
    "title": "Learning to predict optimal solution value for NP-Hard Combinatorial problems",
    "section": "",
    "text": "Sahil Manchanda is a PhD scholar at the Department of Computer Science at the Indian Institute of Technology Delhi, working under the supervision of Prof. Sayan Ranu. Sahil works in the area of Learning to solve graph optimization problems with focus on Combinatorial Optimization, Graph Neural Networks, Lifelong Learning and Generative modeling. He is also interested in applications of Computer Science concepts in other fields such as Material Science and Hardware. He has interned at prestigious institutes such as the University of Tokyo, NAVER Labs- France and Qualcomm AI Research Amsterdam. His research works have been published in top conferences such as NeurIPS, AAAI, ICML, ECML-PKDD and LoG. Additionally he has one US patent granted to his name. Sahil has been the recipient of the Qualcomm Innovation Fellowship for the year 2022."
  },
  {
    "objectID": "logml2024/projects2024/project13/index.html#project",
    "href": "logml2024/projects2024/project13/index.html#project",
    "title": "Learning to predict optimal solution value for NP-Hard Combinatorial problems",
    "section": "Project",
    "text": "Project\nRecently, a lot of interest has been shown in the ML community to learn to solve NP-Hard Combinatorial Optimization(CO) problems. The prime reasons being:\n\nDeep Learning models offer the advantage of speed-up(with good quality) due to GPU based acceleration which is expected to further enhance as GPU hardware improves.\nHeuristics can be learned/distilled directly from the data distribution, thus minimizing or eliminating the need for manually crafted designs.\n\nIn this project we take a different approach and aim to learn to predict the optimal values of NP-Hard Combinatorial problems. Combinatorial optimization solvers can be augmented with machine learning-based optimal solution value predictors to reduce the search space during the quest for precise and high-quality solutions[1]. Further, in some problems such as Graph Edit Distance, directly estimating the optimal value between 2 graphs is also very useful[2].\nRecently, one work[1] attempts to learn to predict optimal values for TSP and job-shop scheduling problems using a Graph Transformer. The paper has interesting results giving hope that with further enhancements and better modelling it might be possible to learn to predict the optimal value more accurately. However, to understand its applicability in practical scenarios, results on crucial aspects such as inference time, percentage errors against optimal solutions and scalability to large CO problem instances are not discussed in the paper. Further, analysis is also required on how does the cost-accuracy tradeoff vary as the model capacity changes especially w.r.t to Graph Transformer layers etc.\nGoal of project:\nThe goal of this project will be initially implement the paper[1] and find out the cost(running time) and error(%) trade off with different model capacities on a set of CO problems. Further, if time permits analyze how does the method scale for larger instances of CO problems such as TSP 100 and TSP 200.\nExecution Plan:\nThe paper mentions that it uses GraphGPS[3] Transformer from PyTorch Geometric. It is easy to use.\nI will provide instances for CO problems(Eg:- TSP for different sizes, Job-Shop Scheduling Problem etc.) and their optimal values to the students. The students will write code to train the parameters of the Graph Transformer. Analyze results w.r.t cost vs prediction accuracy(error) trade off and explore the Pareto frontier etc. If time permits then understand the generalization w.r.t problem size.\nFinally, based upon the results we get, we together hope to formulate and investigate an interesting problem :).\nReferences:\n[1] Wang, Tianze, Amir H. Payberah, and Vladimir Vlassov. “Graph Representation Learning with Graph Transformers in Neural Combinatorial Optimization.”\n[2] Ranjan, Rishabh, et al. “Greed: A neural framework for learning graph distance functions.” Advances in Neural Information Processing Systems 35 (2022): 22518-22530.\n[3] Rampášek, Ladislav, et al. “Recipe for a general, powerful, scalable graph transformer.” Advances in Neural Information Processing Systems 35 (2022): 14501-14515."
  },
  {
    "objectID": "logml2024/projects2024/project15/index.html",
    "href": "logml2024/projects2024/project15/index.html",
    "title": "Calabi-Yau Metrics with U(1)-invariant Neural Networks",
    "section": "",
    "text": "Yidi Qi is a PhD student in the Department of Physics at Northeastern University, under the supervision of Fabian Ruehle. His research focuses on applying machine learning techniques to string theory and mathematics. He is also a junior investigator at the NSF AI Institute for Artificial Intelligence and Fundamental Interactions (IAIFI)."
  },
  {
    "objectID": "logml2024/projects2024/project15/index.html#yidi-qi",
    "href": "logml2024/projects2024/project15/index.html#yidi-qi",
    "title": "Calabi-Yau Metrics with U(1)-invariant Neural Networks",
    "section": "",
    "text": "Yidi Qi is a PhD student in the Department of Physics at Northeastern University, under the supervision of Fabian Ruehle. His research focuses on applying machine learning techniques to string theory and mathematics. He is also a junior investigator at the NSF AI Institute for Artificial Intelligence and Fundamental Interactions (IAIFI)."
  },
  {
    "objectID": "logml2024/projects2024/project15/index.html#project",
    "href": "logml2024/projects2024/project15/index.html#project",
    "title": "Calabi-Yau Metrics with U(1)-invariant Neural Networks",
    "section": "Project",
    "text": "Project\nCalabi-Yau manifolds are compact Kähler manifolds which admit a Ricci-flat metric. They play important roles in modern physics, notably as the extra dimensions in string theory. Knowing the explicit Ricci-flat metrics on Calabi-Yau manifolds is essential for constructing realistic models that describe our four-dimensional universe. However, finding such metrics requires solving the complex Monge-Ampère equation and it is generally believed that an analytical solution does not exist. Even solving it numerically has been proved to be challenging. Recent progress shows that specially designed physics-informed neural networks can help obtain numerical Calabi-Yau metrics much more efficiently. These networks must be invariant under a certain U(1)-action (which is the same as an SO(2)-action), and so far only one special architecture of invariant networks has been implemented. In this project, we will implement other architectures which are invariant under this U(1) group action. Because this gives much more flexibility, there is a chance that this can improve the performance of the physics-informed neural networks, therefore leading to better approximations for Calabi-Yau metrics."
  },
  {
    "objectID": "logml2024/projects2024/project7/index.html",
    "href": "logml2024/projects2024/project7/index.html",
    "title": "Powerful Graph Neural Networks for Relational Databases",
    "section": "",
    "text": "Joshua is a postdoctoral scholar at Stanford working with Jure Leskovec. He obtained his PhD from MIT CSAIL in 2023, where we was advised by Stefanie Jegelka and Suvrit Sra. His interests include 1) designing algorithms for learning over structured domains, such as graphs, eigenvectors, and relational databases, and 2) self-supervised representation learning. He was also a co-organizer of the first LoG conference."
  },
  {
    "objectID": "logml2024/projects2024/project7/index.html#joshua-robinson",
    "href": "logml2024/projects2024/project7/index.html#joshua-robinson",
    "title": "Powerful Graph Neural Networks for Relational Databases",
    "section": "",
    "text": "Joshua is a postdoctoral scholar at Stanford working with Jure Leskovec. He obtained his PhD from MIT CSAIL in 2023, where we was advised by Stefanie Jegelka and Suvrit Sra. His interests include 1) designing algorithms for learning over structured domains, such as graphs, eigenvectors, and relational databases, and 2) self-supervised representation learning. He was also a co-organizer of the first LoG conference."
  },
  {
    "objectID": "logml2024/projects2024/project7/index.html#project",
    "href": "logml2024/projects2024/project7/index.html#project",
    "title": "Powerful Graph Neural Networks for Relational Databases",
    "section": "Project",
    "text": "Project\nMuch of the world’s data is stored in relational databases, which contain multiple tables whose rows are connected via primary-foreign key relations [1]. Consequently, many interesting forecasting problems can be thought of as predictions on relational data (Who will win next weekend’s F1? Will patient A respond to treatment X?). Crucially, relational databases can be viewed as “relational graphs”, with one node per table row, and edges given by primary-foreign key relations. However, relational graphs are not arbitrary graphs. They are k-partite, and some of the node partitions have a fixed number of in- and out-edges per node. This suggests a need for specialized GNN architectures suited to this graph data. The aim of this project will be an initial scoping of the (in)suitability of existing GNNs for processing relational graphs. Depending on student interest, this project’s scope is both a) empirical: evaluating the performance of existing GNN models and components to determine best modeling practices (for testing we have recently released a benchmark suite: https://relbench.stanford.edu/), or b) theoretical: finding examples of relational data structures that existing GNNs cannot distinguish. In both cases, this lays the groundwork for designing more powerful graph networks for relational data.\n[1] Relational Deep Learning: Graph Representation Learning on Relational Databases, Fey et al. 2023 arXiv:2312.04615."
  },
  {
    "objectID": "logml2024/projects2024/project8/index.html",
    "href": "logml2024/projects2024/project8/index.html",
    "title": "Exploiting Graph Neural Networks for Prescriptive maintenance of CERN’s technical infrastructure",
    "section": "",
    "text": "Lorenzo Giusti is currently a senior research scientist at CERN working on geometric and topological deep learning for particle physics in the cryogenics group. Lorenzo holds a PhD in Data Science at La Sapienza, University of Rome, specialized in topological neural networks. His research includes a period of visiting at the University of Cambridge and as a research scientist intern at NASA’s Jet Propulsion Laboratory, where he led a project on Martian terrain modeling using spacecraft imagery and Neural Radiance Fields."
  },
  {
    "objectID": "logml2024/projects2024/project8/index.html#lorenzo-giusti",
    "href": "logml2024/projects2024/project8/index.html#lorenzo-giusti",
    "title": "Exploiting Graph Neural Networks for Prescriptive maintenance of CERN’s technical infrastructure",
    "section": "",
    "text": "Lorenzo Giusti is currently a senior research scientist at CERN working on geometric and topological deep learning for particle physics in the cryogenics group. Lorenzo holds a PhD in Data Science at La Sapienza, University of Rome, specialized in topological neural networks. His research includes a period of visiting at the University of Cambridge and as a research scientist intern at NASA’s Jet Propulsion Laboratory, where he led a project on Martian terrain modeling using spacecraft imagery and Neural Radiance Fields."
  },
  {
    "objectID": "logml2024/projects2024/project8/index.html#project",
    "href": "logml2024/projects2024/project8/index.html#project",
    "title": "Exploiting Graph Neural Networks for Prescriptive maintenance of CERN’s technical infrastructure",
    "section": "Project",
    "text": "Project\nCERN, the European Organization for Nuclear Research, is the largest centre for scientific research in particle physics, and it is known for its complex system of systems comprising advanced particle accelerators and detectors. To fulfill the physics program and deliver the required luminosity for the experiments, advanced tools are required to operate, maintain, and guide device consolidation. It is, therefore, critical to monitor the activities objectively and guide the implementation of strategies to improve performance, optimize costs and highlight key areas needing prioritization. Moreover, the availability of reliable, cost-effective, and energy-efficient sensors entails growing data that captures the underlying phenomena happening within the CERN’s technical infrastructure. To identify failures early and perform prescriptive maintenance of such a complex system of systems, this project aims to reveal hidden dependencies and approach the maintenance operations within the technical infrastructure of the largest particle accelerator complex of the world using graph neural networks."
  },
  {
    "objectID": "logml2024/projects2024/project6/index.html",
    "href": "logml2024/projects2024/project6/index.html",
    "title": "Graph Learning for Uplift Modeling",
    "section": "",
    "text": "George Panagopoulos is a postdoctoral scientist at the University of Luxembourg, researching graph neural networks and causal inference for biomedical applications. Before that, he was working on machine learning for operations research as an applied scientist in the Algorithms and Optimization lab of Amazon Transportation Services in Luxembourg. He received his PhD in machine learning from the Data Science and Mining Team of the École Polytechnique, specializing in graph learning for forecasting and combinatorial optimization. Previously, he was a research assistant and obtained his M.Sc. in computer science with a focus on digital signal processing for neural/physiological data from the University of Houston."
  },
  {
    "objectID": "logml2024/projects2024/project6/index.html#george-panagopoulos",
    "href": "logml2024/projects2024/project6/index.html#george-panagopoulos",
    "title": "Graph Learning for Uplift Modeling",
    "section": "",
    "text": "George Panagopoulos is a postdoctoral scientist at the University of Luxembourg, researching graph neural networks and causal inference for biomedical applications. Before that, he was working on machine learning for operations research as an applied scientist in the Algorithms and Optimization lab of Amazon Transportation Services in Luxembourg. He received his PhD in machine learning from the Data Science and Mining Team of the École Polytechnique, specializing in graph learning for forecasting and combinatorial optimization. Previously, he was a research assistant and obtained his M.Sc. in computer science with a focus on digital signal processing for neural/physiological data from the University of Houston."
  },
  {
    "objectID": "logml2024/projects2024/project6/index.html#project",
    "href": "logml2024/projects2024/project6/index.html#project",
    "title": "Graph Learning for Uplift Modeling",
    "section": "Project",
    "text": "Project\nFrom precision medicine and drug discovery to recommendation systems and online marketing, causal inference using randomized experiments is the gold standard for decision-making.\nHowever, these experiments require interventions that might be too costly, time-consuming, or simply impossible. Machine learning has provided promising solutions in predicting the conditional average treatment effect of an intervention, without actually making it [1]. In this project, we will examine the use of machine learning to facilitate a large-scale marketing campaign.\nDuring such a campaign, promotional codes are distributed to users to increase their activity. Given a specific promotional budget, the campaign should minimize the outreach to users who will not respond, or even worse respond negatively. Hence the aim is to build a model that can predict which users will respond positively to an intervention, commonly called uplift modelling. In uplift modeling, we run an experiment on a representative subset of the users and train a model to predict the average treatment effect on the rest of the user base.\nThe plan is to focus on the network of interactions between samples, which can hide potential confounders that introduce bias in the experiment [2]. We will tackle uplift modeling using graph learning on a heterogeneous graph of an e-commerce system with ground truth interventions and outcomes from an actual marketing campaign. We will then compare the performance with state-of-the-art methods using one of the libraries for ML-based causal inference [3].\n[1] Künzel, Sören R., et al. “Metalearners for estimating heterogeneous treatment effects using machine learning.” Proceedings of the national academy of sciences 116.10 (2019): 4156-4165.\n[2] Guo, Ruocheng, Jundong Li, and Huan Liu. “Learning individual causal effects from networked observational data.” Proceedings of the 13th international conference on web search and data mining. 2020.\n[3] Syrgkanis, Vasilis, et al. “Causal inference and machine learning in practice with econml and causalml: Industrial use cases at microsoft, tripadvisor, uber.” Proceedings of the 27th ACM SIGKDD conference on knowledge discovery & data mining. 2021."
  },
  {
    "objectID": "logml2024/projects2024/project11/index.html",
    "href": "logml2024/projects2024/project11/index.html",
    "title": "Spectral Signed GNNs for fMRI Connectomes",
    "section": "",
    "text": "Rahul is a Wu Tsai postdoctoral fellow at Yale University where he is mentored by Smita Krishnaswamy and Joy Hirsch. He received his PhD in Machine Learning in under the mentorship of Yongxin Chen from the Georgia Institute of Technology. His research interests are in the areas of signal processing, graph neural networks, and machine learning applications in bioinformatics and neuroscience. One of his current research focuses is to explore the neural complexes of brain-to-brain communication when two humans interact."
  },
  {
    "objectID": "logml2024/projects2024/project11/index.html#rahul-singh",
    "href": "logml2024/projects2024/project11/index.html#rahul-singh",
    "title": "Spectral Signed GNNs for fMRI Connectomes",
    "section": "",
    "text": "Rahul is a Wu Tsai postdoctoral fellow at Yale University where he is mentored by Smita Krishnaswamy and Joy Hirsch. He received his PhD in Machine Learning in under the mentorship of Yongxin Chen from the Georgia Institute of Technology. His research interests are in the areas of signal processing, graph neural networks, and machine learning applications in bioinformatics and neuroscience. One of his current research focuses is to explore the neural complexes of brain-to-brain communication when two humans interact."
  },
  {
    "objectID": "logml2024/projects2024/project11/index.html#project",
    "href": "logml2024/projects2024/project11/index.html#project",
    "title": "Spectral Signed GNNs for fMRI Connectomes",
    "section": "Project",
    "text": "Project\nThe existing GNN architectures have focused almost exclusively on graphs with nonnegative edges, which encode some kind of similarity relation between the incident nodes. In contrast, negative edges are often useful to model dissimilarity relations: for instance, in social networks, users may have common/opposite political views or like/dislike each other. Such negative correlations also arise in functional magnetic resonance imaging (fMRI)-derived brain networks or connectomes [1]. These dissimilarity relations are modeled using signed graphs allowing the edges to take both positive or negative values.\nWe will build upon the recently proposed spectral signed graph neural network (GNN) architechtures [2] that can handle (directed) signed graphs and provide interpretable models backed by frequency analysis of signed graphs. Existing representation learning based methods such as BrainGNN [3] for fMRI data do not take these negative correlations into account. The focus of the project will be to utilize interpretable spectral signed GNNs to understand the neurobiological information of negative edges in fMRI connectomes. We will use open-source fMRI datasets human connectome project (HCP) and autism brain imaging data exchange (ABIDE) for our analysis. The brain will be modeled as a signed graph with nodes representing brain regions of interest (ROIs) and edges representing the functional connectivity between those ROIs computed as the pairwise (positive as well as negative) correlations of fMRI time series. The goal will be to identify and explain the effect of negative correlations between different brain regions on learned representations over fMRI connectomes.\n[1] Liang Zhan et al. The significance of negative correlations in brain connectivity, Journal of Comparative Neurology 2017.\n[2] Rahul Singh and Yongxin Chen, Signed graph neural networks: A frequency perspective, Transactions on Machine Learning Research 2023.\n[3] Xiaoxiao Li et al. BrainGNN: Interpretable brain graph neural network for fMRI analysis, Medical Image Analysis 2021."
  },
  {
    "objectID": "logml2024/projects2024/project3/index.html",
    "href": "logml2024/projects2024/project3/index.html",
    "title": "Self-supervised learning for Topological Neural Networks",
    "section": "",
    "text": "Claudio Battiloro is a PhD candidate in ICT at Sapienza University of Rome, a Visiting Scholar at Harvard SPH, and a former Visiting Associate at the SEAS of University of Pennsylvania. He is a prospective co-appointed postdoctoral fellow at Harvard University and University of Pennsylvania. Claudio’s research interests include theory and methods for topological and algebraic signal processing, topological deep learning, and distributed optimization. He has several publications in top-tier journals and conferences. Claudio received different awards such as the IEEE SPS Italian Chapter Best M.Sc. Thesis Award (2020). In 2020, he graduated with distinction in the M.Sc. in Data Science with a (university-overall) Top 400 Students award at Sapienza University."
  },
  {
    "objectID": "logml2024/projects2024/project3/index.html#claudio-battiloro",
    "href": "logml2024/projects2024/project3/index.html#claudio-battiloro",
    "title": "Self-supervised learning for Topological Neural Networks",
    "section": "",
    "text": "Claudio Battiloro is a PhD candidate in ICT at Sapienza University of Rome, a Visiting Scholar at Harvard SPH, and a former Visiting Associate at the SEAS of University of Pennsylvania. He is a prospective co-appointed postdoctoral fellow at Harvard University and University of Pennsylvania. Claudio’s research interests include theory and methods for topological and algebraic signal processing, topological deep learning, and distributed optimization. He has several publications in top-tier journals and conferences. Claudio received different awards such as the IEEE SPS Italian Chapter Best M.Sc. Thesis Award (2020). In 2020, he graduated with distinction in the M.Sc. in Data Science with a (university-overall) Top 400 Students award at Sapienza University."
  },
  {
    "objectID": "logml2024/projects2024/project3/index.html#project",
    "href": "logml2024/projects2024/project3/index.html#project",
    "title": "Self-supervised learning for Topological Neural Networks",
    "section": "Project",
    "text": "Project\nSelf-supervised methods can be broadly categorized into contrastive approaches and predictive approaches. In the context of GNNs, self-supervision is particularly useful for learning graph encoders by utilizing information from the distribution of unlabeled graphs and minimizing a self-supervised loss. However, literature about self-supervised techniques for Topological Deep Learning is extremely sparse, and only a couple of works for simplicial complexes have been presented. For this reason, I propose to design novel self-predictive and contrastive objectives for training TDL architectures.\nIn particular, I will mentor the attendants with the aim of investigating generative objectives based on the predictive approach for architectures defined over regular cell complexes. We will explore cell complex-based reconstruction tasks that can properly generalize the well-studied node property prediction and attribute denoising tasks in the space of attributed graphs.\nWe will go through the actual SoA to design novel techniques able to combine my personal signal processing-grounded approach with cutting edge ML research on Topological and Geometric DL."
  },
  {
    "objectID": "logml2024/projects2024/project5/index.html",
    "href": "logml2024/projects2024/project5/index.html",
    "title": "Generating Calabi-Yau Manifolds with Machine Learning",
    "section": "",
    "text": "I am final year PhD student at City, University of London and soon to be postdoctoral researcher at Imperial College London. Broadly speaking my research involves using techniques from data science and machine learning to study compactification spaces in string theory, namely Calabi-Yau manifolds and G2 manifolds. I attended LOGML just before I started my PhD and learnt a lot, so I look forward to being part of LOGML once again this time as a mentor."
  },
  {
    "objectID": "logml2024/projects2024/project5/index.html#elli-heyes",
    "href": "logml2024/projects2024/project5/index.html#elli-heyes",
    "title": "Generating Calabi-Yau Manifolds with Machine Learning",
    "section": "",
    "text": "I am final year PhD student at City, University of London and soon to be postdoctoral researcher at Imperial College London. Broadly speaking my research involves using techniques from data science and machine learning to study compactification spaces in string theory, namely Calabi-Yau manifolds and G2 manifolds. I attended LOGML just before I started my PhD and learnt a lot, so I look forward to being part of LOGML once again this time as a mentor."
  },
  {
    "objectID": "logml2024/projects2024/project5/index.html#project",
    "href": "logml2024/projects2024/project5/index.html#project",
    "title": "Generating Calabi-Yau Manifolds with Machine Learning",
    "section": "Project",
    "text": "Project\nCalabi-Yau manifolds are of interest in string theory as they describe the shape of the extra dimensions that arise in the theory. It is an active area of research to construct new Calabi-Yau manifolds, and in this project we will use genetic algorithms to generate Calabi-Yau manifolds arising from the construction of hypersurfaces in toric varieties. This was pioneered in the preprint “New Calabi-Yau Manifolds from Genetic Algorithms” (https://arxiv.org/abs/2306.06159). Using our algorithm we will search for Calabi-Yau manifolds with the set of properties that are required in order to give rise to the physics of the Standard Model. Time permitting, we may experiment with more complicated algorithms, for example by combining genetic algorithms with neural networks to optimise the evolutionary process."
  },
  {
    "objectID": "logml2024/projects.html",
    "href": "logml2024/projects.html",
    "title": "Projects",
    "section": "",
    "text": "Calabi-Yau Metrics with U(1)-invariant Neural Networks\n\n\n\n\n\n\n\n\n\n\n\nYidi Qi\n\n\n\n\n\n\n\n\n\n\n\n\nExploiting Graph Neural Networks for Prescriptive maintenance of CERN’s technical infrastructure\n\n\n\n\n\n\n\n\n\n\n\nLorenzo Giusti\n\n\n\n\n\n\n\n\n\n\n\n\nGenerating Calabi-Yau Manifolds with Machine Learning\n\n\n\n\n\n\n\n\n\n\n\nElli Heyes\n\n\n\n\n\n\n\n\n\n\n\n\nGeometric GNNs for particle level reconstruction\n\n\n\n\n\n\n\n\n\n\n\nDolores Garcia\n\n\n\n\n\n\n\n\n\n\n\n\nGeometry for Distribution Learning\n\n\n\n\n\n\n\n\n\n\n\nZhengang Zhong\n\n\n\n\n\n\n\n\n\n\n\n\nGraph Learning for Uplift Modeling\n\n\n\n\n\n\n\n\n\n\n\nGeorge Panagopoulos\n\n\n\n\n\n\n\n\n\n\n\n\nInvariantly learning terminal singularities\n\n\n\n\n\n\n\n\n\n\n\nSara Veneziale\n\n\n\n\n\n\n\n\n\n\n\n\nLearning to predict optimal solution value for NP-Hard Combinatorial problems\n\n\n\n\n\n\n\n\n\n\n\nSahil Manchanda\n\n\n\n\n\n\n\n\n\n\n\n\nMatching graphs with spatial constrains\n\n\n\n\n\n\n\n\n\n\n\nAnna Calissano\n\n\n\n\n\n\n\n\n\n\n\n\nMixed Curvature Graph Neural Networks\n\n\n\n\n\n\n\n\n\n\n\nRishi Sonthalia\n\n\n\n\n\n\n\n\n\n\n\n\nMultimodal Protein Representation Learning\n\n\n\n\n\n\n\n\n\n\n\nMichail Chatzianastasis\n\n\n\n\n\n\n\n\n\n\n\n\nOn the Geometry of Relative Representations\n\n\n\n\n\n\n\n\n\n\n\nMarco Fumero\n\n\n\n\n\n\n\n\n\n\n\n\nPowerful Graph Neural Networks for Relational Databases\n\n\n\n\n\n\n\n\n\n\n\nJoshua Robinson\n\n\n\n\n\n\n\n\n\n\n\n\nPredicting the pathogenicity of a (missense) mutation\n\n\n\n\n\n\n\n\n\n\n\nAbhishek Sharma\n\n\n\n\n\n\n\n\n\n\n\n\nSelf-supervised learning for Topological Neural Networks\n\n\n\n\n\n\n\n\n\n\n\nClaudio Battiloro\n\n\n\n\n\n\n\n\n\n\n\n\nSpectral Signed GNNs for fMRI Connectomes\n\n\n\n\n\n\n\n\n\n\n\nRahul Singh\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Archives",
      "LOGML 2024",
      "Projects"
    ]
  },
  {
    "objectID": "logml2024/speaker.html",
    "href": "logml2024/speaker.html",
    "title": "Speakers",
    "section": "",
    "text": "Keynote speakers\n\n\n\n\n\n\n\n\n\n\nKathryn Hess\n\n\nEPFL\n\n\n\n\n\n\n\n\n\n\n\n\n\nPietro Liò\n\n\nUniversity of Cambridge\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\nSpeakers\n\n\n\n\n\n\n\n\n\n\nJoan Bruna\n\n\nNew York University\n\n\n\n\n\n\n\n\n\n\n\n\n\nVishnu Jejjala\n\n\nUniversity of the Witwatersrand: Johannesburg\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnthea Monod\n\n\nImperial College London\n\n\n\n\n\n\n\n\n\n\n\n\n\nIslem Rekik\n\n\nImperial College London\n\n\n\n\n\n\n\n\n\n\n\n\n\nMichalis Vazirgiannis\n\n\nEcole Polytechnique\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Archives",
      "LOGML 2024",
      "Speakers"
    ]
  },
  {
    "objectID": "logml2024/people2024/advisors/heather.html",
    "href": "logml2024/people2024/advisors/heather.html",
    "title": "Heather Harrington",
    "section": "",
    "text": "website"
  },
  {
    "objectID": "logml2024/people2024/advisors/bronstein.html",
    "href": "logml2024/people2024/advisors/bronstein.html",
    "title": "Michael Bronstein",
    "section": "",
    "text": "website\n  \n\n\n\nMichael Bronstein is the DeepMind Professor of AI at the University of Oxford. He was previously Head of Graph Learning Research at Twitter, a professor at Imperial College London and held visiting appointments at Stanford, MIT, and Harvard. He has been affiliated with three Institutes for Advanced Study (at TUM as a Rudolf Diesel Fellow (2017-2019), at Harvard as a Radcliffe fellow (2017-2018), and at Princeton as a short-time scholar (2020)). Michael received his PhD from the Technion in 2007. He is the recipient of the EPSRC Turing AI World Leading Research Fellowship, Royal Society Wolfson Research Merit Award, Royal Academy of Engineering Silver Medal, five ERC grants, two Google Faculty Research Awards, and two Amazon AWS ML Research Awards. He is a Member of the Academia Europaea, Fellow of IEEE, IAPR, BCS, and ELLIS, ACM Distinguished Speaker, and World Economic Forum Young Scientist. In addition to his academic career, Michael is a serial entrepreneur and founder of multiple startup companies, including Novafora, Invision (acquired by Intel in 2012), Videocites, and Fabula AI (acquired by Twitter in 2019)."
  },
  {
    "objectID": "logml2024/people2024/advisors/ulrike.html",
    "href": "logml2024/people2024/advisors/ulrike.html",
    "title": "Ulrike Tillmann",
    "section": "",
    "text": "website"
  },
  {
    "objectID": "logml2024/people2024/advisors/xavier.html",
    "href": "logml2024/people2024/advisors/xavier.html",
    "title": "Xavier Bresson",
    "section": "",
    "text": "website\n  \n\n\n\nXavier Bresson (PhD 2005, EPFL, Switzerland) is Associate Professor in Computer Science at NUS, Singapore. He is a leading researcher in the field of Graph Deep Learning, a new framework that combines graph theory and deep learning techniques to tackle complex data domains in natural language processing, computer vision, combinatorial optimization, quantum chemistry, physics, neuroscience, genetics and social networks. In 2016, he received the highly competitive Singaporean NRF Fellowship of $2.5M to develop these deep learning techniques. He was also awarded several research grants in the U.S. and Hong Kong. As a leading researcher in the field, he has published more than 60 peer-reviewed papers in the leading journals and conference proceedings in machine learning, including articles in NeurIPS, ICML, ICLR, CVPR, JMLR. He has organized several international workshops and tutorials on AI and deep learning in collaboration with Facebook, NYU and Imperial such as the 2019 and 2018 UCLA workshops, the 2017 CVPR tutorial and the 2017 NeurIPS tutorial. He has been teaching undergraduate, graduate and industrial courses in AI and deep learning since 2014 at EPFL (Switzerland), NTU (Singapore) and UCLA (U.S.)."
  },
  {
    "objectID": "logml2024/people2024/team/george.html",
    "href": "logml2024/people2024/team/george.html",
    "title": "George Dasoulas",
    "section": "",
    "text": "George Dasoulas is a postdoc at Harvard University,  concentrating in learning graph representations for bioinformatics applications. His research focuses on building powerful graph learning models, that are able to extract knowledge from biomedical networks. He received a Ph.D. in Computer Science from Laboratoire d’informatique at École Polytechnique in Paris, France."
  },
  {
    "objectID": "logml2024/people2024/team/konstantinos.html",
    "href": "logml2024/people2024/team/konstantinos.html",
    "title": "Konstantinos Barmpas",
    "section": "",
    "text": "Konstantinos Barmpas is a PhD student at Imperial College London under the supervision of Professor Stefanos Zafeiriou. His research lies in the intersection of Machine Learning and Brain-Computer Interfaces (BCIs). Prior to his PhD studies, Konstantinos received his M.Eng. (integrated B.Eng) in Electrical and Electronic Engineering from Imperial College London in 2020. He undertook his Master’s Year as an exchange student at ETH Zürich, where he conducted his Master Thesis at the Data Analytics Lab under the supervision of Professor Thomas Hofmann."
  },
  {
    "objectID": "logml2024/people2024/team/daniel.html",
    "href": "logml2024/people2024/team/daniel.html",
    "title": "Daniel Platt",
    "section": "",
    "text": "Daniel is a Chapman-Schmidt fellow at Imperial College London working on differential geometry and machine learning. He obtained my PhD from Imperial College London for his thesis on gauge theory in dimension 7 on problems that are motivated by string theory. Because of this, he is interested in big datasets of Calabi-Yau manifolds. Apart from this, Lie groups and  group actions are important for the kind of geometry that he studies and he is also interested in applications of differential geometry to group invariant machine learning."
  },
  {
    "objectID": "logml2024/people2024/team/michelle.html",
    "href": "logml2024/people2024/team/michelle.html",
    "title": "Michelle Li",
    "section": "",
    "text": "Michelle is a PhD candidate in the Department of Biomedical Informatics  at Harvard Medical School. Advised by Prof. Marinka Zitnik, Michelle is  developing deep graph representation learning algorithms to characterize  drugs at a single cell resolution and to better leverage rich  biomedical knowledge graphs for diagnosing rare diseases."
  },
  {
    "objectID": "logml2024/people2024/team/valentina.html",
    "href": "logml2024/people2024/team/valentina.html",
    "title": "Valentina Giunchiglia",
    "section": "",
    "text": "Home\n    People\n    Organising Committee\n    Valentina Giunchiglia\n  \n\nValentina Giunchiglia is a first year PhD student funded by the Medical Research Council, specialising in Artificial Intelligence and Computational Neuroscience.\nHer research interests focus on building top-down and bottom-up computational models of the brain to better understand and treat different neurodegenerative diseases (e.g. Multiple Sclerosis, Parkinson’s or Alzheimer’s). She is interested in working on research projects that cross the boundaries between artificial intelligence (AI) and the clinics, with the aim of making AI actually applicable in clinical practice. She works on data-centric and explainable AI, with a special interest towards graph machine learning."
  }
]